<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 7 Multiple random variables | MATH1710 Probability and Statistics I</title>
  <meta name="description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 7 Multiple random variables | MATH1710 Probability and Statistics I" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://mpaldridge.github.io/math1710/" />
  
  <meta property="og:description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 7 Multiple random variables | MATH1710 Probability and Statistics I" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  

<meta name="author" content="Matthew Aldridge" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="S06-discrete-dist.html"/>
<link rel="next" href="P4.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH1710 notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i>About MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#organisation"><i class="fa fa-check"></i>Organisation of MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#problem-sheets"><i class="fa fa-check"></i>Problem sheets</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#tutorials"><i class="fa fa-check"></i>Tutorials</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#r-worksheets"><i class="fa fa-check"></i>R worksheets</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#dropin"><i class="fa fa-check"></i>Optional “office hours” drop-in sessions</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#time"><i class="fa fa-check"></i>Time management</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#exam"><i class="fa fa-check"></i>Exam</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#ask"><i class="fa fa-check"></i>Who should I ask about…?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#about-content"><i class="fa fa-check"></i>Content of MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prereqs"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#syllabus"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#books"><i class="fa fa-check"></i>Books</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#about-notes"><i class="fa fa-check"></i>About these notes</a></li>
</ul></li>
<li class="part"><span><b>Part I: EDA</b></span></li>
<li class="chapter" data-level="1" data-path="S01-eda.html"><a href="S01-eda.html"><i class="fa fa-check"></i><b>1</b> Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-eda.html"><a href="S01-eda.html#what-is-eda"><i class="fa fa-check"></i><b>1.1</b> What is EDA?</a></li>
<li class="chapter" data-level="1.2" data-path="S01-eda.html"><a href="S01-eda.html#what-is-R"><i class="fa fa-check"></i><b>1.2</b> What is R?</a></li>
<li class="chapter" data-level="1.3" data-path="S01-eda.html"><a href="S01-eda.html#summary-stat"><i class="fa fa-check"></i><b>1.3</b> Summary statistics and boxplots</a></li>
<li class="chapter" data-level="1.4" data-path="S01-eda.html"><a href="S01-eda.html#binned"><i class="fa fa-check"></i><b>1.4</b> Binned data and histograms</a></li>
<li class="chapter" data-level="1.5" data-path="S01-eda.html"><a href="S01-eda.html#multiple"><i class="fa fa-check"></i><b>1.5</b> Multiple variables and scatterplots</a></li>
<li class="chapter" data-level="" data-path="S01-eda.html"><a href="S01-eda.html#summary-01"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html"><i class="fa fa-check"></i>Problem Sheet 1</a>
<ul>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="part"><span><b>Part II: Probability</b></span></li>
<li class="chapter" data-level="2" data-path="S02-probability.html"><a href="S02-probability.html"><i class="fa fa-check"></i><b>2</b> Probability spaces</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-probability.html"><a href="S02-probability.html#what-is-prob"><i class="fa fa-check"></i><b>2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2" data-path="S02-probability.html"><a href="S02-probability.html#sample-events"><i class="fa fa-check"></i><b>2.2</b> Sample spaces and events</a></li>
<li class="chapter" data-level="2.3" data-path="S02-probability.html"><a href="S02-probability.html#set-theory"><i class="fa fa-check"></i><b>2.3</b> Basic set theory</a></li>
<li class="chapter" data-level="2.4" data-path="S02-probability.html"><a href="S02-probability.html#axioms"><i class="fa fa-check"></i><b>2.4</b> Probability axioms</a></li>
<li class="chapter" data-level="2.5" data-path="S02-probability.html"><a href="S02-probability.html#prob-properties"><i class="fa fa-check"></i><b>2.5</b> Properties of probability</a></li>
<li class="chapter" data-level="2.6" data-path="S02-probability.html"><a href="S02-probability.html#addition"><i class="fa fa-check"></i><b>2.6</b> Addition rules for unions</a></li>
<li class="chapter" data-level="" data-path="S02-probability.html"><a href="S02-probability.html#summary-02"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-classical.html"><a href="S03-classical.html"><i class="fa fa-check"></i><b>3</b> Classical probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-classical.html"><a href="S03-classical.html#classical-intro"><i class="fa fa-check"></i><b>3.1</b> Probability with equally likely outcomes</a></li>
<li class="chapter" data-level="3.2" data-path="S03-classical.html"><a href="S03-classical.html#multiplication"><i class="fa fa-check"></i><b>3.2</b> Multiplication principle</a></li>
<li class="chapter" data-level="3.3" data-path="S03-classical.html"><a href="S03-classical.html#sampling"><i class="fa fa-check"></i><b>3.3</b> Sampling with and without replacement</a></li>
<li class="chapter" data-level="3.4" data-path="S03-classical.html"><a href="S03-classical.html#ordering"><i class="fa fa-check"></i><b>3.4</b> Ordering</a></li>
<li class="chapter" data-level="3.5" data-path="S03-classical.html"><a href="S03-classical.html#combinations"><i class="fa fa-check"></i><b>3.5</b> Sampling without replacement in any order</a></li>
<li class="chapter" data-level="3.6" data-path="S03-classical.html"><a href="S03-classical.html#birthday"><i class="fa fa-check"></i><b>3.6</b> Birthday problem</a></li>
<li class="chapter" data-level="" data-path="S03-classical.html"><a href="S03-classical.html#summary-03"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html"><i class="fa fa-check"></i>Problem Sheet 2</a>
<ul>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-conditional.html"><a href="S04-conditional.html"><i class="fa fa-check"></i><b>4</b> Independence and conditional probability</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-conditional.html"><a href="S04-conditional.html#independent-events"><i class="fa fa-check"></i><b>4.1</b> Independent events</a></li>
<li class="chapter" data-level="4.2" data-path="S04-conditional.html"><a href="S04-conditional.html#conditional"><i class="fa fa-check"></i><b>4.2</b> Conditional probability</a></li>
<li class="chapter" data-level="4.3" data-path="S04-conditional.html"><a href="S04-conditional.html#chain-rule"><i class="fa fa-check"></i><b>4.3</b> Chain rule</a></li>
<li class="chapter" data-level="4.4" data-path="S04-conditional.html"><a href="S04-conditional.html#total-prob"><i class="fa fa-check"></i><b>4.4</b> Law of total probability</a></li>
<li class="chapter" data-level="4.5" data-path="S04-conditional.html"><a href="S04-conditional.html#bayes"><i class="fa fa-check"></i><b>4.5</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="4.6" data-path="S04-conditional.html"><a href="S04-conditional.html#screening"><i class="fa fa-check"></i><b>4.6</b> Diagnostic testing</a></li>
<li class="chapter" data-level="" data-path="S04-conditional.html"><a href="S04-conditional.html#summary-034"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html"><i class="fa fa-check"></i><b>5</b> Discrete random variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#rv"><i class="fa fa-check"></i><b>5.1</b> What is a random variable?</a></li>
<li class="chapter" data-level="5.2" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#pmf"><i class="fa fa-check"></i><b>5.2</b> Probability mass functions</a></li>
<li class="chapter" data-level="5.3" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#expectation"><i class="fa fa-check"></i><b>5.3</b> Expectation</a></li>
<li class="chapter" data-level="5.4" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#functions"><i class="fa fa-check"></i><b>5.4</b> Functions of random variables</a></li>
<li class="chapter" data-level="5.5" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#variance"><i class="fa fa-check"></i><b>5.5</b> Variance</a></li>
<li class="chapter" data-level="" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#summary-05"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html"><i class="fa fa-check"></i>Problem Sheet 3</a>
<ul>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html"><i class="fa fa-check"></i><b>6</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#binomial"><i class="fa fa-check"></i><b>6.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="6.2" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#geometric"><i class="fa fa-check"></i><b>6.2</b> Geometric distribution</a></li>
<li class="chapter" data-level="6.3" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#poisson"><i class="fa fa-check"></i><b>6.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="6.4" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#poisson-approx"><i class="fa fa-check"></i><b>6.4</b> Poisson approximation to the binomial</a></li>
<li class="chapter" data-level="6.5" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#models"><i class="fa fa-check"></i><b>6.5</b> Distributions as models for data</a></li>
<li class="chapter" data-level="" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#summary-06"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html"><i class="fa fa-check"></i><b>7</b> Multiple random variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#joint"><i class="fa fa-check"></i><b>7.1</b> Joint distributions</a></li>
<li class="chapter" data-level="7.2" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#independence-rv"><i class="fa fa-check"></i><b>7.2</b> Independence of random variables</a></li>
<li class="chapter" data-level="7.3" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#cond-rv"><i class="fa fa-check"></i><b>7.3</b> Conditional distributions</a></li>
<li class="chapter" data-level="7.4" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#sum-product"><i class="fa fa-check"></i><b>7.4</b> Expectation of sums and products</a></li>
<li class="chapter" data-level="7.5" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#covariance"><i class="fa fa-check"></i><b>7.5</b> Covariance</a></li>
<li class="chapter" data-level="7.6" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#lln"><i class="fa fa-check"></i><b>7.6</b> Law of large numbers</a></li>
<li class="chapter" data-level="" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#summary-07"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html"><i class="fa fa-check"></i>Problem Sheet 4</a>
<ul>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="S08-continuous.html"><a href="S08-continuous.html"><i class="fa fa-check"></i><b>8</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-continuous.html"><a href="S08-continuous.html#continuous-rv"><i class="fa fa-check"></i><b>8.1</b> What is a continuous random variable?</a></li>
<li class="chapter" data-level="8.2" data-path="S08-continuous.html"><a href="S08-continuous.html#pdf"><i class="fa fa-check"></i><b>8.2</b> Probability density functions</a></li>
<li class="chapter" data-level="8.3" data-path="S08-continuous.html"><a href="S08-continuous.html#prop-cont"><i class="fa fa-check"></i><b>8.3</b> Properties of continuous random variables</a></li>
<li class="chapter" data-level="8.4" data-path="S08-continuous.html"><a href="S08-continuous.html#exponential"><i class="fa fa-check"></i><b>8.4</b> Exponential distribution</a></li>
<li class="chapter" data-level="8.5" data-path="S08-continuous.html"><a href="S08-continuous.html#continuous-multiple"><i class="fa fa-check"></i><b>8.5</b> Multiple continuous random variables</a></li>
<li class="chapter" data-level="" data-path="S08-continuous.html"><a href="S08-continuous.html#summary-08"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-normal.html"><a href="S09-normal.html"><i class="fa fa-check"></i><b>9</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-normal.html"><a href="S09-normal.html#normal-definition"><i class="fa fa-check"></i><b>9.1</b> Definition of the normal distribution</a></li>
<li class="chapter" data-level="9.2" data-path="S09-normal.html"><a href="S09-normal.html#normal-properties"><i class="fa fa-check"></i><b>9.2</b> Properties of the normal distribution</a></li>
<li class="chapter" data-level="9.3" data-path="S09-normal.html"><a href="S09-normal.html#normal-r"><i class="fa fa-check"></i><b>9.3</b> Calculations using R</a></li>
<li class="chapter" data-level="9.4" data-path="S09-normal.html"><a href="S09-normal.html#normal-tables"><i class="fa fa-check"></i><b>9.4</b> Calculations using statistical tables</a></li>
<li class="chapter" data-level="9.5" data-path="S09-normal.html"><a href="S09-normal.html#clt"><i class="fa fa-check"></i><b>9.5</b> Central limit theorem</a></li>
<li class="chapter" data-level="9.6" data-path="S09-normal.html"><a href="S09-normal.html#normal-approx"><i class="fa fa-check"></i><b>9.6</b> Approximations with the normal distribution</a></li>
<li class="chapter" data-level="" data-path="S09-normal.html"><a href="S09-normal.html#summary-09"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html"><i class="fa fa-check"></i>Problem Sheet 5</a>
<ul>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="part"><span><b>Part III: Bayesian statistics</b></span></li>
<li class="chapter" data-level="10" data-path="S10-bayesian.html"><a href="S10-bayesian.html"><i class="fa fa-check"></i><b>10</b> Introduction to Bayesian statistics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-bayesian.html"><a href="S10-bayesian.html#fake-coin"><i class="fa fa-check"></i><b>10.1</b> Example: fake coin?</a></li>
<li class="chapter" data-level="10.2" data-path="S10-bayesian.html"><a href="S10-bayesian.html#bayesian-framework"><i class="fa fa-check"></i><b>10.2</b> Bayesian framework</a></li>
<li class="chapter" data-level="10.3" data-path="S10-bayesian.html"><a href="S10-bayesian.html#beta"><i class="fa fa-check"></i><b>10.3</b> Beta distribution</a></li>
<li class="chapter" data-level="10.4" data-path="S10-bayesian.html"><a href="S10-bayesian.html#beta-bern"><i class="fa fa-check"></i><b>10.4</b> Beta–Bernoulli model</a></li>
<li class="chapter" data-level="10.5" data-path="S10-bayesian.html"><a href="S10-bayesian.html#normal-normal"><i class="fa fa-check"></i><b>10.5</b> Normal–normal model</a></li>
<li class="chapter" data-level="10.6" data-path="S10-bayesian.html"><a href="S10-bayesian.html#modern-bayes"><i class="fa fa-check"></i><b>10.6</b> Modern Bayesian statistics</a></li>
<li class="chapter" data-level="" data-path="S10-bayesian.html"><a href="S10-bayesian.html#summary-10"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P6.html"><a href="P6.html"><i class="fa fa-check"></i>Problem Sheet 6</a></li>
<li class="part"><span><b>Other stuff</b></span></li>
<li class="chapter" data-level="11" data-path="S11-summary.html"><a href="S11-summary.html"><i class="fa fa-check"></i><b>11</b> The last section</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-summary.html"><a href="S11-summary.html#summary-of-the-module"><i class="fa fa-check"></i><b>11.1</b> Summary of the module</a></li>
<li class="chapter" data-level="11.2" data-path="S11-summary.html"><a href="S11-summary.html#about-the-exam"><i class="fa fa-check"></i><b>11.2</b> About the exam</a></li>
<li class="chapter" data-level="11.3" data-path="S11-summary.html"><a href="S11-summary.html#past-papers"><i class="fa fa-check"></i><b>11.3</b> Past papers</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i>R Worksheets</a>
<ul>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-work"><i class="fa fa-check"></i>R worksheets</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#about-r"><i class="fa fa-check"></i>About R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-access"><i class="fa fa-check"></i>How to access R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-install"><i class="fa fa-check"></i>Installing R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#troubleshooting"><i class="fa fa-check"></i>Troubleshooting drop-in sessions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i>Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P1-solutions"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P2-solutions"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P3-solutions"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P4-solutions"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P5-solutions"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P6-solutions"><i class="fa fa-check"></i>Problem Sheet 6</a></li>
</ul></li>
<li class="divider"></li>
<li></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH1710 Probability and Statistics I</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S07-multi-rv" class="section level1" number="7">
<h1><span class="header-section-number">Section 7</span> Multiple random variables</h1>
<div id="joint" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Joint distributions</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/5trzf2sZnLw">
</iframe>
</div>
</div>
<p>In previous sections, we have looked at single discrete random variables in isolation. In the section, we want to look at how multiple discrete random variables can interact.</p>
<p>Consider tossing a fair coin 3 times. Let <span class="math inline">\(X\)</span> be the number of Heads in the first two tosses, and let <span class="math inline">\(Y\)</span> be the number of Heads over all three tosses.</p>
<p>We know that <span class="math inline">\(X \sim \text{Bin}(2, \frac12)\)</span> and <span class="math inline">\(Y \sim \text{Bin}(3, \frac12)\)</span>, so we can easily write down their probability mass functions:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(x = 0\)</span></th>
<th align="center"><span class="math inline">\(x = 1\)</span></th>
<th align="center"><span class="math inline">\(x = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_X(x)\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(\frac12\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(y\)</span></th>
<th align="center"><span class="math inline">\(y = 0\)</span></th>
<th align="center"><span class="math inline">\(y = 1\)</span></th>
<th align="center"><span class="math inline">\(y = 2\)</span></th>
<th align="center"><span class="math inline">\(y = 3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_Y(y)\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
</tr>
</tbody>
</table>
<p>When we have multiple random variables and we want to emphasise that a PMF refers to only one of them, we often use the phrase <strong>marginal PMF</strong> or <strong>marginal distribution</strong>. So the PMFs above are the marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>However, we might also want to know how <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> interact. To do this, we will need the <strong>joint PMF</strong>, given by
<span class="math display">\[ p_{X,Y}(x,y) = \mathbb P(X = x \text{ and } Y = y) . \]</span></p>
<p>In our case of the coin tosses, we have</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(p_{X,Y}(x,y)\)</span></th>
<th align="center"><span class="math inline">\(y = 0\)</span></th>
<th align="center"><span class="math inline">\(y = 1\)</span></th>
<th align="center"><span class="math inline">\(y = 2\)</span></th>
<th align="center"><span class="math inline">\(y = 3\)</span></th>
<th align="center"><span class="math inline">\(\phantom{p_X(x)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x=0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x=1\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x=2\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vphantom{p_Y(y)}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>For probabilities of events, we had that if some <span class="math inline">\(A_i\)</span>s form a partition, then
<span class="math display">\[ \mathbb P(B) = \sum_i \mathbb P(B \cap A_i) . \]</span>
Note that the events <span class="math inline">\(\{X = x\}\)</span>, as <span class="math inline">\(x\)</span> varies over the range of <span class="math inline">\(X\)</span>, also make up a partition. Therefore, we have
<span class="math display">\[ \mathbb P(Y = y) = \sum_x \mathbb P(X = x \text{ and } Y = y) ; \]</span>
or, to phrase this in terms of joint and marginal PMFs,
<span class="math display">\[ p_Y(y) = \sum_x p_{X,Y}(x, y) . \]</span>
In other words, to get the marginal distribution of <span class="math inline">\(Y\)</span>, we need to sum down the columns in the table of the joint distribution.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(p_{X,Y}(x,y)\)</span></th>
<th align="center"><span class="math inline">\(y = 0\)</span></th>
<th align="center"><span class="math inline">\(y = 1\)</span></th>
<th align="center"><span class="math inline">\(y = 2\)</span></th>
<th align="center"><span class="math inline">\(y = 3\)</span></th>
<th align="center"><span class="math inline">\(\phantom{p_X(x)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x=0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x=1\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x=2\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(p_Y(y)\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>In exactly the same way, we have
<span class="math display">\[ p_X(x) = \sum_y p_{X,Y}(x, y) ; \]</span>
so to get the marginal distribution of <span class="math inline">\(X\)</span>, we need to sum across the rows in the table of the joint distribution.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(p_{X,Y}(x,y)\)</span></th>
<th align="center"><span class="math inline">\(y = 0\)</span></th>
<th align="center"><span class="math inline">\(y = 1\)</span></th>
<th align="center"><span class="math inline">\(y = 2\)</span></th>
<th align="center"><span class="math inline">\(y = 3\)</span></th>
<th align="center"><span class="math inline">\(p_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x=0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x=1\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac12\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x=2\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac14\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(p_Y(y)\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac38\)</span></td>
<td align="center"><span class="math inline">\(\frac18\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>We can check that these marginal PMFs match those we started with. The term “marginal” PMF or distribution is presumably because one ends up writing the values in the “margins” of the table.</p>
<p>Note that the joint PMF conforms to the same rules as a normal PMF:</p>
<ul>
<li>it is non-negative: <span class="math inline">\(p_{X,Y}(x,y) \geq 0\)</span>;</li>
<li>it sums to 1: <span class="math inline">\(\displaystyle\sum_{x,y} p_{X,Y}(x,y) = 1\)</span>.</li>
</ul>
<p>We may want to look at more than two random variables, <span class="math inline">\(\mathbf X = (X_1, X_2, \dots, X_n)\)</span>. In this case, the joint PMF is
<span class="math display">\[ p_{\mathbf X}(\mathbf x) = p_{X_1, \dots, X_n}(x_1, \dots, x_n) = \mathbb P(X_1 = x_1 \text{ and } \cdots \text{ and } X_n = x_n) .   \]</span>
In the same way, we can find the marginal distribution of one of the variables – say <span class="math inline">\(X_1\)</span> – by summing over all the other variables:
<span class="math display">\[ p_{X_1}(x_1) = \sum_{x_2, \dots, x_n} p_{X_1, X_2, \dots, X_n}(x_1, x_2, \dots, x_n) . \]</span></p>
</div>
<div id="independence-rv" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Independence of random variables</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/ZxoVHpnI298">
</iframe>
</div>
</div>
<p>We said that two events are independent if <span class="math inline">\(\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)\)</span>. We now can give a similar definition for what it means two random variables to be independent.</p>
<div class="definition">
<p><span id="def:unlabeled-div-75" class="definition"><strong>Definition 7.1  </strong></span>We say two discrete random variables are <strong>independent</strong> if, for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>,
<span class="math display">\[ \mathbb P(X = x \text{ and } Y = y) = \mathbb P(X = x) \, \mathbb P(Y = y) . \]</span>
In terms of the joint and marginal PMFs, this is the condition that
<span class="math display">\[ p_{X,Y}(x,y) = p_X(x) \, p_Y(y) . \]</span></p>
<p>More generally, a sequence of random variables <span class="math inline">\(\mathbf X = (X_1, X_2, \dots)\)</span> are independent if
<span class="math display">\[ p_{\mathbf X}(\mathbf x) = p_{X_1}(x_1) \times p_{X_2}(x_2) \times \cdots \times p_{X_n}(x_n). \]</span></p>
</div>
<p>Returning to our case of the dice from before, we see that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent, because, for just one counterexample, <span class="math inline">\(p_{X,Y}(0,0) = \frac18\)</span>, while <span class="math inline">\(p_X(0) = \frac14\)</span> and <span class="math inline">\(p_Y(0) = \frac18\)</span>, so <span class="math inline">\(p_{X,Y}(0,0) \neq p_X(0) \, p_Y(0)\)</span>.</p>
<p>An important scenario in probability theory and statistics is that of <strong>independent and identically distributed</strong> (or <strong>IID</strong>) random variables. IID random variables represent an experiment that is repeated many times, with each experiment independent of the others. So all the random variables have the same distribution, and they are all independent of each other. So <span class="math inline">\(\mathbf X = (X_1, X_2, \dots )\)</span> are IID random variables with a common PMF <span class="math inline">\(p_X\)</span>, say, if
<span class="math display">\[ p_{\mathbf X}(\mathbf x) = p_X(X_1) \times p_X(x_2) \times \cdots . \]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-76" class="example"><strong>Example 7.1  </strong></span><em>Let <span class="math inline">\(X_1, X_2, \dots, X_{20}\)</span> be IID random variables following a Poisson distribution with rate <span class="math inline">\(\lambda = 3\)</span>. What is the probability that all 20 of the the <span class="math inline">\(X_i\)</span> are nonzero?</em></p>
<p>Because the <span class="math inline">\(X_i\)</span> are identically distribution, the probability that any one of them is nonzero is
<span class="math display">\[ \mathbb P(X_1 &gt; 0) = 1 - \mathbb P(X_1 = 0) = 1 - \mathrm{e}^{-3} = 0.950 . \]</span>
Then, because the <span class="math inline">\(X_i\)</span> independent, the probability that they are all nonzero is
<span class="math display">\[ \mathbb P(X_1 &gt; 0)^{20} = 0.950^{20} = 0.360. \]</span></p>
</div>
</div>
<div id="cond-rv" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Conditional distributions</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/U0kNB45K81w">
</iframe>
</div>
</div>
<p>For probability with events, we had the conditional probability
<span class="math display">\[ \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} . \]</span>
In the same way, for random variables, we have
<span class="math display">\[ \mathbb P(Y = y \mid X = x) = \frac{\mathbb P(X = x \text{ and } Y = y)}{\mathbb P(X = x)} . \]</span>
It makes sense to call the distribution of this the <strong>conditional distribution</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-77" class="definition"><strong>Definition 7.2  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with joint PMF <span class="math inline">\(p_{X,Y}\)</span> and marginal PMFs <span class="math inline">\(p_X\)</span> and <span class="math inline">\(p_Y\)</span> respectively. Then the <strong>condition probability mass function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span></strong> <span class="math inline">\(p_{Y \mid X}\)</span> is given by
<span class="math display">\[ p_{Y \mid X}(y \mid x) = \frac{p_{X,Y}(x,y)}{p_X(x)} .   \]</span></p>
</div>
<p>Let’s think again about our coin tossing example. To get the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = 1\)</span>, say, we have
<span class="math display">\[ p_{Y \mid X}(y \mid 1) = \frac{p_{X,Y}(1,y)}{p_X(1)} ;   \]</span>
so we take the <span class="math inline">\(x = 1\)</span> row of the joint distribution table and “renormalise it” by dividing through by the total of the row, so it adds up to 1. That is,
<span class="math display">\[\begin{align*}
  p_{Y \mid X} (0 \mid 1) &amp;= \frac{p_{X,Y}(1, 0)}{p_X(1)} = \frac{0}{\frac12} = 0 , \\
  p_{Y \mid X} (1 \mid 1) &amp;= \frac{p_{X,Y}(1, 1)}{p_X(1)} = \frac{\frac14}{\frac12} = \tfrac12 , \\
  p_{Y \mid X} (2 \mid 1) &amp;= \frac{p_{X,Y}(1, 2)}{p_X(1)} = \frac{\frac14}{\frac12} = \tfrac12 , \\
  p_{Y \mid X} (3 \mid 1) &amp;= \frac{p_{X,Y}(1, 3)}{p_X(1)} = \frac{0}{\frac12} = 0 .
\end{align*}\]</span>
In just the same way, we could get the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = 2\)</span>, say, by taking the <span class="math inline">\(y = 2\)</span> column of the joint distribution table, and renormalising so that the column sums to 1, That is,
<span class="math display">\[\begin{align*}
  p_{X \mid Y} (0 \mid 2) &amp;= \frac{p_{X,Y}(0,2)}{p_Y(2)} = \frac{0}{\frac38} = 0 , \\
  p_{X \mid Y} (1 \mid 2) &amp;= \frac{p_{X,Y}(1,2)}{p_Y(2)} = \frac{\frac14}{\frac38} = \tfrac23 , \\
  p_{X \mid Y} (2 \mid 2) &amp;= \frac{p_{X,Y}(2,2)}{p_Y(2)} = \frac{\frac18}{\frac38} = \tfrac13 .
\end{align*}\]</span></p>
<p>Results that we used for conditional probability with events also carry over to random variables. For example, from <strong>Bayes’ theorem</strong> we know that
<span class="math display">\[ \mathbb P(A \mid B) = \frac{ \mathbb P(A) \, \mathbb P(B \mid A)}{\mathbb P(B)} . \]</span>
In the same way, we have
<span class="math display">\[ \mathbb P(X = x \mid Y = y) = \frac{ \mathbb P(X = x) \, \mathbb P(Y = y \mid X = x)}{\mathbb P(Y = y)} , \]</span>
which in terms of conditional and marginal PMFs is
<span class="math display">\[ p_{X \mid Y}(x \mid y) = \frac{p_X(x) \, p_{Y \mid X}(y \mid x)}{p_Y(y)} . \]</span>
This will be a particularly important formula when we study Bayesian statistics at the end of the module.</p>
<p>We can check Bayes’ theorem with <span class="math inline">\(x = 1\)</span> and <span class="math inline">\(y = 2\)</span>, for example.
The right-hand side of Bayes’ theorem is
<span class="math display">\[ \frac{p_X(1) \, p_{Y \mid X}(2 \mid 1)}{p_Y(2)} = \frac{\frac12 \times \frac12}{\frac38} = \tfrac{23} .   \]</span>
The left-hand side of Bayes’ theorem is
<span class="math display">\[ p_{X \mid Y}(1 \mid 2) = \tfrac23 , \]</span>
which is equal, as it should be, to the right-hand side.</p>
</div>
<div id="sum-product" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Expectation of sums and products</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/BaEbVJZkGGY">
</iframe>
</div>
</div>
<p>When we have multiple random variables, we might be interested in functions of those multiple random variables – for example their sum or their product. (In fact, it’s often possible to find out about the whole distribution of a sum, product, or function of the variables – see MATH2715 Statistical Methods for more on this – but will just look at their expectations and, later, variances.)</p>
<div class="theorem">
<p><span id="thm:linearity2" class="theorem"><strong>Theorem 7.1  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with joint probability mass function <span class="math inline">\(p_{X,Y}\)</span>. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb Eg(X,Y) = \displaystyle\sum_{x,y} g(x,y) p_{X,Y}(x,y)\)</span>.</li>
<li><strong>(Linearity of expectation, 2)</strong> <span class="math inline">\(\mathbb E(X + Y) = \mathbb EX + \mathbb EY\)</span>, regardless of whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent or not.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mathbb EXY = \mathbb EX \times \mathbb EY\)</span>.</li>
</ol>
</div>
<p>If we put the second point here together with the other result of linearity of expectation (Theorem <a href="S05-discrete-rv.html#thm:linearity1">5.3</a>) then we get the general rule
<span class="math display">\[ \mathbb E(aX + bY + c) = a\,\mathbb EX + b \,\mathbb EY + c , \]</span>
and this holds whether or not <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
<div class="proof">
<p><span id="unlabeled-div-78" class="proof"><em>Proof</em>. </span>Part 1 is just the law of the unconscious statistician for the random variable <span class="math inline">\((X,Y)\)</span>, and the same proof holds.</p>
<p>For part 2, we have
<span class="math display">\[\begin{align*}
\mathbb E(X + Y) &amp;= \sum_{x,y} (x + y)p_{X,Y}(x,y) \\
  &amp;= \sum_{x,y} x\,p_{X,Y}(x,y) + \sum_{x,y} y\,p_{X,Y}(x,y) \\
  &amp;= \sum_x x \sum_y p_{X,Y}(x,y) + \sum_y y \sum_x p_{X,Y}(x,y)
\end{align*}\]</span>
But summing a joint PMF over one of the variables gives the marginal PMF; so <span class="math inline">\(\sum_y p_{X,Y}(x,y) = p_X(x)\)</span> and <span class="math inline">\(\sum_x p_{X,Y}(x,y) = p_Y(y)\)</span>. So this gives
<span class="math display">\[\begin{align*}
\mathbb E(X + Y) &amp;= \sum_x x\, p_X(x) + \sum_y y\,p_Y(y) \\
&amp;= \mathbb EX + \mathbb EY .
\end{align*}\]</span></p>
<p>For part 3, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(p_{X,Y}(x,y) = p_X(x) \, p_Y(y)\)</span>. Therefore,
<span class="math display">\[\begin{align*}
\mathbb EXY &amp;= \sum_{x,y} xy p_{X,Y}(x,y) \\
  &amp;= \sum_x \sum_y xy p_X(x) p_Y(y) \\
  &amp;= \sum_x x p_X(x) \sum_y y p_Y(y) \\
  &amp;= \mathbb EX \times \mathbb EY,
\end{align*}\]</span>
as required.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-79" class="example"><strong>Example 7.2  </strong></span><em>A student is solving five questions on a problem sheet. The time taken for each question to the nearest minute is an identically distributed random variable with expectation <span class="math inline">\(\mathbb EX_i = \mu\)</span>. What is the total expected time to complete the problem sheet?</em></p>
<p>By linearity of expectation, this is
<span class="math display">\[ \mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = \mathbb EX_1 + \mathbb EX_2 + \mathbb EX_3 + \mathbb EX_4 + \mathbb EX_5 = 5\mu . \]</span></p>
<p><em>What if the lengths of time are not independent – for example, if the student is slower at answering all the questions when she is tired?</em></p>
<p>It’s still the case that <span class="math inline">\(\mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = 5\mu\)</span>, because this result is true whether or not the random variables are independent.</p>
</div>
</div>
<div id="covariance" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Covariance</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/eXsUrJzcRCE">
</iframe>
</div>
</div>
<p>If we are interested at how two random variables vary together, we need to look at the covariance.</p>
<div class="definition">
<p><span id="def:unlabeled-div-80" class="definition"><strong>Definition 7.3  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with expectations <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span> respectively. Then their <strong>covariance</strong> is
<span class="math display">\[ \operatorname{Cov}(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) . \]</span></p>
</div>
<p>In the least surprising result of this whole module, we also have a computational formula to go along with this definitional formula.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-81" class="theorem"><strong>Theorem 7.2  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with expectations <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span> respectively. Then their covariance can also be calculated as
<span class="math display">\[ \operatorname{Cov}(X,Y) = \mathbb EXY - \mu_X\, \mu_Y . \]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-82" class="proof"><em>Proof</em>. </span>Exactly as we’ve done many times before, we have
<span class="math display">\[\begin{align*}
\operatorname{Cov}(X,Y) &amp;= \mathbb E(X - \mu_X)(Y - \mu_Y) \\
&amp;= \mathbb E(XY - X\,\mu_Y - \mu_X\, Y + \mu_X\,\mu_Y) \\
&amp;= \mathbb EXY  - \mu_Y \,\mathbb EX - \mu_X \,\mathbb EY + \mu_X \, \mu_Y \\
&amp;= \mathbb EXY - \mu_X \, \mu_Y - \mu_X \, \mu_Y + \mu_X \, \mu_Y \\
&amp;= \mathbb EXY - \mu_X \, \mu_Y ,
\end{align*}\]</span>
and we’re done.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-83" class="example"><strong>Example 7.3  </strong></span>We continue with our coin-tossing example. We know that <span class="math inline">\(X \sim \text{Bin}(2, \frac12)\)</span> so <span class="math inline">\(\mu_X = 1\)</span> and <span class="math inline">\(Y \sim \text{Bin}(3, \frac12)\)</span> so <span class="math inline">\(\mu_Y = 1.5\)</span>. We then need <span class="math inline">\(\mathbb EXY\)</span>, which is
<span class="math display">\[\begin{align*}
\mathbb EXY &amp;= \sum_{x,y} xy\, p_{X,Y}(x,y) \\
  &amp;= 0\times 0\,p_{X,Y}(0,0) + 0 \times 1 \, p_{X,Y}(0,1) + \cdots + 2\times 3 \,p_{X,Y}(2,3) \\
  &amp;= 0 \times \tfrac18 + 0 \times \tfrac18 + \cdots + 6 \times \tfrac18 \\
  &amp;= 2.
\end{align*}\]</span>
Hence the covariance is
<span class="math display">\[ \operatorname{Cov}(X,Y) = \mathbb EXY - \mu_X\mu_Y = 2 - 1 \times 1.5 = 0.5 .\]</span></p>
</div>
<p>A very important fact is the following.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-84" class="theorem"><strong>Theorem 7.3  </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{Cov}(X,Y) = 0\)</span>.</p>
</div>
<p>To use the <a href="https://www.varsitytutors.com/hotmath/hotmath_help/topics/converse-inverse-contrapositive">“contrapositive”</a>, in our example, <span class="math inline">\(\operatorname{Cov}(X,Y) \neq 0\)</span>, which means that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent (confirming what we already knew).</p>
<div class="proof">
<p><span id="unlabeled-div-85" class="proof"><em>Proof</em>. </span>Recall from Theorem <a href="S07-multi-rv.html#thm:linearity2">7.1</a> that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we have <span class="math inline">\(\mathbb EXY = \mathbb EX \times \mathbb EY = \mu_X \, \mu_Y\)</span>. Then from the computational formula, we have
<span class="math display">\[ \operatorname{Cov}(X,Y) = \mathbb EXY - \mu_X\,\mu_Y = \mu_X\,\mu_Y - \mu_X\,\mu_Y = 0, \]</span>
and we are done.</p>
</div>
<p>Here are some more important properties of the covariance.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-86" class="theorem"><strong>Theorem 7.4  </strong></span>Let <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> be random variables. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X)\)</span>;</li>
<li><span class="math inline">\(\operatorname{Cov}(X,X) = \operatorname{Var}(X)\)</span>’</li>
<li><span class="math inline">\(\operatorname{Cov}(aX, Y) = a\,\operatorname{Cov}(X,Y)\)</span>;</li>
<li><span class="math inline">\(\operatorname{Cov}(X + b, Y) = \operatorname{Cov}(X,Y)\)</span>;</li>
<li><span class="math inline">\(\operatorname{Cov}(X + Y, Z) = \operatorname{Cov}(X, Z) + \operatorname{Cov}(Y,Z)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-87" class="proof"><em>Proof</em>. </span>Part 1 and 2 are immediate from the definition.</p>
<p>Parts 3, 4 and 5 are quite similar. We’ll do part 5 here, and you can do parts 3 and 4 on <a href="P4.html#P4">Problem Sheet 4</a>.</p>
<p>For part 5, note that <span class="math inline">\(\mathbb E(X + Y) = \mu_X + \mu_Y\)</span> by linearity of expectation. Hence
<span class="math display">\[\begin{align*}
\operatorname{Cov}(X + Y, Z)
  &amp;= \mathbb E (X + Y - \mu_X - \mu_Y)(Z - \mu_Z) \\
  &amp;= \mathbb E \big((X - \mu_X) + (Y - \mu_Y)\big)(Z - \mu_Z) \\
  &amp;= \mathbb E \big((X - \mu_X)(Z - \mu_Z) + (Y - \mu_Y) (Z - \mu_Z) \big) \\
  &amp;= \mathbb E (X - \mu_X)(Z - \mu_Z) + \mathbb E  (Y - \mu_Y) (Z - \mu_Z) \\
  &amp;= \operatorname{Cov}(X,Z) + \operatorname{Cov}(Y,Z) ,
\end{align*}\]</span>
as required.</p>
</div>
<p>We could calculate the covariance in our coin-tossing example a different way, by noting that <span class="math inline">\(Y = X + Z\)</span>, where <span class="math inline">\(Z \sim \text{Bern}(\frac12)\)</span> represents the third coin toss and is independent of <span class="math inline">\(X\)</span>. Then we have
<span class="math display">\[\begin{multline*}
\operatorname{Cov}(X,Y) = \operatorname{Cov}(X, X + Z) = \operatorname{Cov}(X, X) + \operatorname{Cov}(X, Z) \\
= \operatorname{Var}(X) + 0 = 2 \times \tfrac12 \times \big(1 - \tfrac12\big) = \tfrac12, \end{multline*}\]</span>
matching our previous calculation. In the above calculation, we used <span class="math inline">\(\operatorname{Cov}(X, Z) = 0\)</span> since <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are independent, and we knew the variance of <span class="math inline">\(X\)</span> because <span class="math inline">\(X \sim \text{Bin}(2, \frac12)\)</span>.</p>
<p>Now that we know some facts about the covariance, we can calculate the variance of a sum.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-88" class="theorem"><strong>Theorem 7.5  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables. Then
<span class="math display">\[ \operatorname{Var}(X + Y) = \operatorname{Var}(X) + 2\operatorname{Cov}(X,Y) + \operatorname{Var}(Y) . \]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then
<span class="math display">\[ \operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) . \]</span></p>
</div>
<p>It’s easy to forget the conditions for the following two facts:</p>
<ul>
<li><span class="math inline">\(\mathbb E(X + Y) = \mathbb EX + \mathbb EY\)</span> regardless of whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent or not.</li>
<li><span class="math inline">\(\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
</ul>
<div class="proof">
<p><span id="unlabeled-div-89" class="proof"><em>Proof</em>. </span>For the main part of the proof, we start with the definition of variance. By linearity of expectation, we have <span class="math inline">\(\mathbb E(X + Y) = \mu_X + \mu_Y\)</span>. So
<span class="math display">\[\begin{align*}
\operatorname{Var}(X + Y) &amp;= \mathbb E\big((X + Y) - (\mu_X + \mu_Y)\big)^2 \\
  &amp;= \mathbb E \big((X - \mu_X) + (Y - \mu_Y) \big)^2 \\
  &amp;= \mathbb E \big( (X - \mu_X)^2 + 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2\big) \\
  &amp;= \mathbb E(X - \mu_X)^2 + 2 \mathbb E(X - \mu_X)(Y - \mu_Y) + \mathbb E (Y - \mu_Y)^2 \\
  &amp;= \operatorname{Var}(X) + 2\operatorname{Cov}(X,Y) + \operatorname{Var}(Y) ,
\end{align*}\]</span>
where we used the linearity of expectation.</p>
<p>For the second part, recall that is <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{Cov}(X,Y) = 0\)</span>.</p>
</div>
<p>It can sometimes be useful to “normalise” the covariance, by dividing through by the individual standard deviations. This gives a measurement of the linear relationship between two random variables.</p>
<div class="definition">
<p><span id="def:unlabeled-div-90" class="definition"><strong>Definition 7.4  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables. Then the <strong>correlation</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is
<span class="math display">\[ \operatorname{Corr}(X,Y) = \frac{\operatorname{Cov}(X,Y)} {\sqrt{\operatorname{Var}(X)\,\operatorname{Var}(Y)}} . \]</span></p>
</div>
<p>As with the sample correlation <span class="math inline">\(r_{xy}\)</span> from Section 1, the correlation is a number between <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>, where values near <span class="math inline">\(+1\)</span> mean that large values of <span class="math inline">\(X\)</span> and large values of <span class="math inline">\(Y\)</span> are likely to occur together, while values near <span class="math inline">\(-1\)</span> mean that large values of <span class="math inline">\(X\)</span> and small values of <span class="math inline">\(Y\)</span> are likely to occur together.</p>
<p>Recall that, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{Cov}(X,Y) = 0\)</span>. Hence it follows that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{Corr}(X,Y) = 0\)</span> also.</p>
<div class="example">
<p><span id="exm:unlabeled-div-91" class="example"><strong>Example 7.4  </strong></span>For the coin-tossing again, we have
<span class="math display">\[ \operatorname{Corr}(X,Y) = \frac{\operatorname{Cov}(X,Y)} {\sqrt{\operatorname{Var}(X)\,\operatorname{Var}(Y)}} = \frac{\frac12}{\sqrt{\frac12 \times \frac34}} = \sqrt{\tfrac23} = 0.816 .    \]</span></p>
</div>
</div>
<div id="lln" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Law of large numbers</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/u4hJv2sDUoc">
</iframe>
</div>
</div>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a sequence of IID random variables. Let us write <span class="math inline">\(\mu = \mathbb EX_1\)</span> for the common expectation and <span class="math inline">\(\sigma^2 = \operatorname{Var}(X_1)\)</span> for the common variance.</p>
<p>At the beginning of the course, we saw the mean of some values was
<span class="math display">\[ \bar x = \frac{1}{n} (x_1 + x_2 + \cdots + x_n) = \frac{1}{n} \sum_{i=1}^n x_i ; \]</span>
that is, what we get if we add them up and divide by <span class="math inline">\(n\)</span>. In the same way, we could calculate the “mean” of some random variables by adding them up and dividing by <span class="math inline">\(n\)</span>; that is:
<span class="math display">\[ \overline X_n = \frac{1}{n} (X_1 + X_2 + \cdots + X_n) = \frac{1}{n} \sum_{i=1}^n X_i . \]</span>
(The subscript <span class="math inline">\(n\)</span> on “<span class="math inline">\(\overline X_n\)</span>” is just to remind us this is a mean of <span class="math inline">\(n\)</span> random variables.)</p>
<p>Here, each of the <span class="math inline">\(X_i\)</span>s is a random variable, so their mean <span class="math inline">\(\overline X_n\)</span> is another random variable as well. So we can ask questions about the random variable <span class="math inline">\(\overline X_n\)</span> just the same as we would ask about any other random variable. For example: What is its expectation and variance?</p>
<p>The expectation of <span class="math inline">\(\bar X_n\)</span> is
<span class="math display">\[\begin{align*}
\mathbb E \overline X_n &amp;= \mathbb E \left( \frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
&amp;=   \frac{1}{n} (\mathbb EX_1 + \mathbb EX_2 + \cdots + \mathbb EX_n)\\
&amp;= \frac{1}{n} (\mu + \mu + \cdots + \mu)\\
&amp;= \frac{1}{n} n \mu \\
&amp;= \mu .
\end{align*}\]</span>
Here we use linearity of expectation to take the <span class="math inline">\(1/n\)</span> out of the brackets, and then add up the individual expectations.</p>
<p>Since the <span class="math inline">\(X_i\)</span>s are independent, the variance of <span class="math inline">\(\overline X_n\)</span> is
<span class="math display">\[\begin{align*}
\operatorname{Var}( \overline X_n) &amp;= \operatorname{Var}\left( \frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
&amp;= \left(\frac{1}{n}\right)^2 \operatorname{Var}(X_1 + X_2 + \cdots + X_n) \\
&amp;=   \frac{1}{n^2} \big(\operatorname{Var}(X_1) + \operatorname{Var}(X_2) + \cdots + \operatorname{Var}(X_n)\big)\\
&amp;= \frac{1}{n^2} (\sigma^2 + \sigma^2+ \cdots + \sigma^2)\\
&amp;= \frac{1}{n^2} n \sigma^2 \\
&amp;= \frac{\sigma^2}{n} .
\end{align*}\]</span></p>
<p>In conclusion we have this:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-92" class="theorem"><strong>Theorem 7.6  </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a sequence of IID random variables. Write <span class="math inline">\(\mu = \mathbb EX_1\)</span> for the common expectation and <span class="math inline">\(\sigma^2 = \operatorname{Var}(X_1)\)</span> for the common variance, and
<span class="math inline">\(\overline X_n =\frac{1}{n} \sum_{i=1}^n X_i\)</span> for the mean. Then
<span class="math display">\[ \mathbb E \overline X_n = \mu \qquad \operatorname{Var}(\overline X_n) = \frac{\sigma^2}{n} . \]</span></p>
</div>
<p>Now think about what happens to this mean <span class="math inline">\(\overline X_n\)</span> when <span class="math inline">\(n\)</span> gets very large. We see that the expectation <span class="math inline">\(\mathbb E\overline X_n = \mu\)</span> stays the same, but the variance <span class="math inline">\(\operatorname{Var}(\overline X_n) = \sigma^2/n\)</span> gets smaller and smaller as <span class="math inline">\(n\)</span> gets bigger. Thus the range of probably values for <span class="math inline">\(\overline X_n\)</span> well be squeezing tighter and tighter around <span class="math inline">\(\mu\)</span>. Given that, it seems as if (and it can be proven that) we have the “law of large numbers”.</p>
<div class="theorem">
<p><span id="thm:thLLN" class="theorem"><strong>Theorem 7.7  (Law of large numbers) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots\)</span> be a sequence of IID random variables. Write <span class="math inline">\(\mu = \mathbb EX_1\)</span> for the common expectation and <span class="math inline">\(\overline X_n =\frac{1}{n} \sum_{i=1}^n X_i\)</span> for the mean of the first <span class="math inline">\(n\)</span> random variables. Then
<span class="math display">\[ \overline X_n \to \mu \quad \text{in probability as $n \to \infty$}; \]</span>
by which we mean that, for any <span class="math inline">\(\epsilon &gt; 0\)</span>,
<span class="math display">\[ \mathbb P\big(|\overline X_n - \mu| &gt; \epsilon\big) \to 0 \quad \text{as $n\to\infty$.} \]</span></p>
</div>
<p>The precise mathematical definition of the convergence is not important here. What is important is the general principle that the expectation <span class="math inline">\(\mathbb EX = \mu\)</span> represents the “long-run average” of independent experiments.</p>
<p>One special case is if we have repeated experiments the succeed with probability <span class="math inline">\(p\)</span>; that is, <span class="math inline">\(X_n \sim \text{Bern}(p)\)</span>. Then the law of large numbers says that the long-run proportion of successes is
<span class="math display">\[ \frac{1}{n} \sum_{i = 1}^n X_n = \overline X_n \to \mathbb EX_1 = p . \]</span>
So the long-run proportion of times an event happens converges to its probability. This goes back to what we said about “frequentist probability” <a href="S02-probability.html#what-is-prob">right at the beginning of Section 2</a>: that one way to understand the probability of an event is as the long-run frequency of its occurrence.</p>
</div>
<div id="summary-07" class="section level2 unnumbered">
<h2>Summary</h2>
<div class="mysummary">
<ul>
<li>For two random variables, the joint PMF <span class="math inline">\(p_{X,Y}\)</span>, marginal PMF <span class="math inline">\(p_X\)</span>, and conditional PMF <span class="math inline">\(p_{Y \mid X}\)</span> are
<span class="math display">\[\begin{align*}
p_{X,Y}(x,y) &amp;= \mathbb P(X =x \text{ and } Y = y) \\
p_X(x) &amp;= \mathbb P(X = x) = \sum_y p_{X,Y}(x,y) \\
p_{Y \mid X}(y \mid x) &amp;= \mathbb P(Y = y \mid X = x) = \frac{p_{X,Y}(x,y)}{p_X(x)} 
\end{align*}\]</span></li>
<li>Two random variables are independent if <span class="math inline">\(p_{X,Y}(x,y) = p_X(x) \, p_Y(y)\)</span>.</li>
<li><span class="math inline">\(\mathbb E(X + Y) = \mathbb EX + \mathbb EY\)</span></li>
<li>The covariance is <span class="math inline">\(\operatorname{Cov}(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) = \mathbb EXY - \mu_X \,\mu_Y\)</span>.</li>
<li><span class="math inline">\(\operatorname{Var}(X + Y) = \operatorname{Var}(X) + 2\operatorname{Cov}(X,Y) + \operatorname{Var}(Y)\)</span>; or if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)\)</span>.</li>
<li>The law of large numbers says that the mean <span class="math inline">\(\overline X_n\)</span> of IID random variables tends to the expectation <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n \to \infty\)</span>.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="S06-discrete-dist.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="P4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["math1710.pdf", "math1710.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
