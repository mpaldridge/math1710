<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 10 Introduction to Bayesian statistics | MATH1710 Probability and Statistics I</title>
  <meta name="description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 10 Introduction to Bayesian statistics | MATH1710 Probability and Statistics I" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://mpaldridge.github.io/math1710/" />
  
  <meta property="og:description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 10 Introduction to Bayesian statistics | MATH1710 Probability and Statistics I" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH1710 Probability and Statistics I at the University of Leeds, 2021–2022" />
  

<meta name="author" content="Matthew Aldridge" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="P5.html"/>
<link rel="next" href="P6.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH1710 notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i>About MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#organisation"><i class="fa fa-check"></i>Organisation of MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#problem-sheets"><i class="fa fa-check"></i>Problem sheets</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#tutorials"><i class="fa fa-check"></i>Tutorials</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#r-worksheets"><i class="fa fa-check"></i>R worksheets</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#dropin"><i class="fa fa-check"></i>Optional “office hours” drop-in sessions</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#time"><i class="fa fa-check"></i>Time management</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#exam"><i class="fa fa-check"></i>Exam</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#ask"><i class="fa fa-check"></i>Who should I ask about…?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#about-content"><i class="fa fa-check"></i>Content of MATH1710</a>
<ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prereqs"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#syllabus"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#books"><i class="fa fa-check"></i>Books</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#about-notes"><i class="fa fa-check"></i>About these notes</a></li>
</ul></li>
<li class="part"><span><b>Part I: EDA</b></span></li>
<li class="chapter" data-level="1" data-path="S01-eda.html"><a href="S01-eda.html"><i class="fa fa-check"></i><b>1</b> Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-eda.html"><a href="S01-eda.html#what-is-eda"><i class="fa fa-check"></i><b>1.1</b> What is EDA?</a></li>
<li class="chapter" data-level="1.2" data-path="S01-eda.html"><a href="S01-eda.html#what-is-R"><i class="fa fa-check"></i><b>1.2</b> What is R?</a></li>
<li class="chapter" data-level="1.3" data-path="S01-eda.html"><a href="S01-eda.html#summary-stat"><i class="fa fa-check"></i><b>1.3</b> Summary statistics and boxplots</a></li>
<li class="chapter" data-level="1.4" data-path="S01-eda.html"><a href="S01-eda.html#binned"><i class="fa fa-check"></i><b>1.4</b> Binned data and histograms</a></li>
<li class="chapter" data-level="1.5" data-path="S01-eda.html"><a href="S01-eda.html#multiple"><i class="fa fa-check"></i><b>1.5</b> Multiple variables and scatterplots</a></li>
<li class="chapter" data-level="" data-path="S01-eda.html"><a href="S01-eda.html#summary-01"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html"><i class="fa fa-check"></i>Problem Sheet 1</a>
<ul>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P1.html"><a href="P1.html#P1-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="part"><span><b>Part II: Probability</b></span></li>
<li class="chapter" data-level="2" data-path="S02-probability.html"><a href="S02-probability.html"><i class="fa fa-check"></i><b>2</b> Probability spaces</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-probability.html"><a href="S02-probability.html#what-is-prob"><i class="fa fa-check"></i><b>2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2" data-path="S02-probability.html"><a href="S02-probability.html#sample-events"><i class="fa fa-check"></i><b>2.2</b> Sample spaces and events</a></li>
<li class="chapter" data-level="2.3" data-path="S02-probability.html"><a href="S02-probability.html#set-theory"><i class="fa fa-check"></i><b>2.3</b> Basic set theory</a></li>
<li class="chapter" data-level="2.4" data-path="S02-probability.html"><a href="S02-probability.html#axioms"><i class="fa fa-check"></i><b>2.4</b> Probability axioms</a></li>
<li class="chapter" data-level="2.5" data-path="S02-probability.html"><a href="S02-probability.html#prob-properties"><i class="fa fa-check"></i><b>2.5</b> Properties of probability</a></li>
<li class="chapter" data-level="2.6" data-path="S02-probability.html"><a href="S02-probability.html#addition"><i class="fa fa-check"></i><b>2.6</b> Addition rules for unions</a></li>
<li class="chapter" data-level="" data-path="S02-probability.html"><a href="S02-probability.html#summary-02"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-classical.html"><a href="S03-classical.html"><i class="fa fa-check"></i><b>3</b> Classical probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-classical.html"><a href="S03-classical.html#classical-intro"><i class="fa fa-check"></i><b>3.1</b> Probability with equally likely outcomes</a></li>
<li class="chapter" data-level="3.2" data-path="S03-classical.html"><a href="S03-classical.html#multiplication"><i class="fa fa-check"></i><b>3.2</b> Multiplication principle</a></li>
<li class="chapter" data-level="3.3" data-path="S03-classical.html"><a href="S03-classical.html#sampling"><i class="fa fa-check"></i><b>3.3</b> Sampling with and without replacement</a></li>
<li class="chapter" data-level="3.4" data-path="S03-classical.html"><a href="S03-classical.html#ordering"><i class="fa fa-check"></i><b>3.4</b> Ordering</a></li>
<li class="chapter" data-level="3.5" data-path="S03-classical.html"><a href="S03-classical.html#combinations"><i class="fa fa-check"></i><b>3.5</b> Sampling without replacement in any order</a></li>
<li class="chapter" data-level="3.6" data-path="S03-classical.html"><a href="S03-classical.html#birthday"><i class="fa fa-check"></i><b>3.6</b> Birthday problem</a></li>
<li class="chapter" data-level="" data-path="S03-classical.html"><a href="S03-classical.html#summary-03"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html"><i class="fa fa-check"></i>Problem Sheet 2</a>
<ul>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P2.html"><a href="P2.html#P2-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-conditional.html"><a href="S04-conditional.html"><i class="fa fa-check"></i><b>4</b> Independence and conditional probability</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-conditional.html"><a href="S04-conditional.html#independent-events"><i class="fa fa-check"></i><b>4.1</b> Independent events</a></li>
<li class="chapter" data-level="4.2" data-path="S04-conditional.html"><a href="S04-conditional.html#conditional"><i class="fa fa-check"></i><b>4.2</b> Conditional probability</a></li>
<li class="chapter" data-level="4.3" data-path="S04-conditional.html"><a href="S04-conditional.html#chain-rule"><i class="fa fa-check"></i><b>4.3</b> Chain rule</a></li>
<li class="chapter" data-level="4.4" data-path="S04-conditional.html"><a href="S04-conditional.html#total-prob"><i class="fa fa-check"></i><b>4.4</b> Law of total probability</a></li>
<li class="chapter" data-level="4.5" data-path="S04-conditional.html"><a href="S04-conditional.html#bayes"><i class="fa fa-check"></i><b>4.5</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="4.6" data-path="S04-conditional.html"><a href="S04-conditional.html#screening"><i class="fa fa-check"></i><b>4.6</b> Diagnostic testing</a></li>
<li class="chapter" data-level="" data-path="S04-conditional.html"><a href="S04-conditional.html#summary-034"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html"><i class="fa fa-check"></i><b>5</b> Discrete random variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#rv"><i class="fa fa-check"></i><b>5.1</b> What is a random variable?</a></li>
<li class="chapter" data-level="5.2" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#pmf"><i class="fa fa-check"></i><b>5.2</b> Probability mass functions</a></li>
<li class="chapter" data-level="5.3" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#expectation"><i class="fa fa-check"></i><b>5.3</b> Expectation</a></li>
<li class="chapter" data-level="5.4" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#functions"><i class="fa fa-check"></i><b>5.4</b> Functions of random variables</a></li>
<li class="chapter" data-level="5.5" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#variance"><i class="fa fa-check"></i><b>5.5</b> Variance</a></li>
<li class="chapter" data-level="" data-path="S05-discrete-rv.html"><a href="S05-discrete-rv.html#summary-05"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html"><i class="fa fa-check"></i>Problem Sheet 3</a>
<ul>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P3.html"><a href="P3.html#P3-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html"><i class="fa fa-check"></i><b>6</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#binomial"><i class="fa fa-check"></i><b>6.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="6.2" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#geometric"><i class="fa fa-check"></i><b>6.2</b> Geometric distribution</a></li>
<li class="chapter" data-level="6.3" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#poisson"><i class="fa fa-check"></i><b>6.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="6.4" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#poisson-approx"><i class="fa fa-check"></i><b>6.4</b> Poisson approximation to the binomial</a></li>
<li class="chapter" data-level="6.5" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#models"><i class="fa fa-check"></i><b>6.5</b> Distributions as models for data</a></li>
<li class="chapter" data-level="" data-path="S06-discrete-dist.html"><a href="S06-discrete-dist.html#summary-06"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html"><i class="fa fa-check"></i><b>7</b> Multiple random variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#joint"><i class="fa fa-check"></i><b>7.1</b> Joint distributions</a></li>
<li class="chapter" data-level="7.2" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#independence-rv"><i class="fa fa-check"></i><b>7.2</b> Independence of random variables</a></li>
<li class="chapter" data-level="7.3" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#cond-rv"><i class="fa fa-check"></i><b>7.3</b> Conditional distributions</a></li>
<li class="chapter" data-level="7.4" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#sum-product"><i class="fa fa-check"></i><b>7.4</b> Expectation of sums and products</a></li>
<li class="chapter" data-level="7.5" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#covariance"><i class="fa fa-check"></i><b>7.5</b> Covariance</a></li>
<li class="chapter" data-level="7.6" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#lln"><i class="fa fa-check"></i><b>7.6</b> Law of large numbers</a></li>
<li class="chapter" data-level="" data-path="S07-multi-rv.html"><a href="S07-multi-rv.html#summary-07"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html"><i class="fa fa-check"></i>Problem Sheet 4</a>
<ul>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P4.html"><a href="P4.html#P4-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="S08-continuous.html"><a href="S08-continuous.html"><i class="fa fa-check"></i><b>8</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-continuous.html"><a href="S08-continuous.html#continuous-rv"><i class="fa fa-check"></i><b>8.1</b> What is a continuous random variable?</a></li>
<li class="chapter" data-level="8.2" data-path="S08-continuous.html"><a href="S08-continuous.html#pdf"><i class="fa fa-check"></i><b>8.2</b> Probability density functions</a></li>
<li class="chapter" data-level="8.3" data-path="S08-continuous.html"><a href="S08-continuous.html#prop-cont"><i class="fa fa-check"></i><b>8.3</b> Properties of continuous random variables</a></li>
<li class="chapter" data-level="8.4" data-path="S08-continuous.html"><a href="S08-continuous.html#exponential"><i class="fa fa-check"></i><b>8.4</b> Exponential distribution</a></li>
<li class="chapter" data-level="8.5" data-path="S08-continuous.html"><a href="S08-continuous.html#continuous-multiple"><i class="fa fa-check"></i><b>8.5</b> Multiple continuous random variables</a></li>
<li class="chapter" data-level="" data-path="S08-continuous.html"><a href="S08-continuous.html#summary-08"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-normal.html"><a href="S09-normal.html"><i class="fa fa-check"></i><b>9</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-normal.html"><a href="S09-normal.html#normal-definition"><i class="fa fa-check"></i><b>9.1</b> Definition of the normal distribution</a></li>
<li class="chapter" data-level="9.2" data-path="S09-normal.html"><a href="S09-normal.html#normal-properties"><i class="fa fa-check"></i><b>9.2</b> Properties of the normal distribution</a></li>
<li class="chapter" data-level="9.3" data-path="S09-normal.html"><a href="S09-normal.html#normal-r"><i class="fa fa-check"></i><b>9.3</b> Calculations using R</a></li>
<li class="chapter" data-level="9.4" data-path="S09-normal.html"><a href="S09-normal.html#normal-tables"><i class="fa fa-check"></i><b>9.4</b> Calculations using statistical tables</a></li>
<li class="chapter" data-level="9.5" data-path="S09-normal.html"><a href="S09-normal.html#clt"><i class="fa fa-check"></i><b>9.5</b> Central limit theorem</a></li>
<li class="chapter" data-level="9.6" data-path="S09-normal.html"><a href="S09-normal.html#normal-approx"><i class="fa fa-check"></i><b>9.6</b> Approximations with the normal distribution</a></li>
<li class="chapter" data-level="" data-path="S09-normal.html"><a href="S09-normal.html#summary-09"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html"><i class="fa fa-check"></i>Problem Sheet 5</a>
<ul>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-short"><i class="fa fa-check"></i>A: Short questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-long"><i class="fa fa-check"></i>B: Long questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-assessed"><i class="fa fa-check"></i>C: Assessed questions</a></li>
<li class="chapter" data-level="" data-path="P5.html"><a href="P5.html#P5-short-sols"><i class="fa fa-check"></i>Solutions to short questions</a></li>
</ul></li>
<li class="part"><span><b>Part III: Bayesian statistics</b></span></li>
<li class="chapter" data-level="10" data-path="S10-bayesian.html"><a href="S10-bayesian.html"><i class="fa fa-check"></i><b>10</b> Introduction to Bayesian statistics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-bayesian.html"><a href="S10-bayesian.html#fake-coin"><i class="fa fa-check"></i><b>10.1</b> Example: fake coin?</a></li>
<li class="chapter" data-level="10.2" data-path="S10-bayesian.html"><a href="S10-bayesian.html#bayesian-framework"><i class="fa fa-check"></i><b>10.2</b> Bayesian framework</a></li>
<li class="chapter" data-level="10.3" data-path="S10-bayesian.html"><a href="S10-bayesian.html#beta"><i class="fa fa-check"></i><b>10.3</b> Beta distribution</a></li>
<li class="chapter" data-level="10.4" data-path="S10-bayesian.html"><a href="S10-bayesian.html#beta-bern"><i class="fa fa-check"></i><b>10.4</b> Beta–Bernoulli model</a></li>
<li class="chapter" data-level="10.5" data-path="S10-bayesian.html"><a href="S10-bayesian.html#normal-normal"><i class="fa fa-check"></i><b>10.5</b> Normal–normal model</a></li>
<li class="chapter" data-level="10.6" data-path="S10-bayesian.html"><a href="S10-bayesian.html#modern-bayes"><i class="fa fa-check"></i><b>10.6</b> Modern Bayesian statistics</a></li>
<li class="chapter" data-level="" data-path="S10-bayesian.html"><a href="S10-bayesian.html#summary-10"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P6.html"><a href="P6.html"><i class="fa fa-check"></i>Problem Sheet 6</a></li>
<li class="part"><span><b>Other stuff</b></span></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i>R Worksheets</a>
<ul>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-work"><i class="fa fa-check"></i>R worksheets</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#about-r"><i class="fa fa-check"></i>About R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-access"><i class="fa fa-check"></i>How to access R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#r-install"><i class="fa fa-check"></i>Installing R and RStudio</a></li>
<li class="chapter" data-level="" data-path="R.html"><a href="R.html#troubleshooting"><i class="fa fa-check"></i>Troubleshooting drop-in sessions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i>Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P1-solutions"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P2-solutions"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P3-solutions"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P4-solutions"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#P5-solutions"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
</ul></li>
<li class="divider"></li>
<li></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH1710 Probability and Statistics I</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S10-bayesian" class="section level1" number="10">
<h1><span class="header-section-number">Section 10</span> Introduction to Bayesian statistics</h1>
<div id="fake-coin" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Example: fake coin?</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/BUZ_4DqHjIM">
</iframe>
</div>
</div>
<p><strong>Statistics</strong> concerns how to draw conclusions from data; and <strong>Bayesian statistics</strong> is one particular framework for doing this. The idea of Bayesian statistics is that we use the data (together with Bayes’ theorem) to update our “prior” (“before”) beliefs about the underlying model to our “posterior” (“after”) beliefs about the model <em>given</em> the data we have observed.</p>
<p>We will start by illustrating the main idea with an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-105" class="example"><strong>Example 10.1  </strong></span><em>A joke shop sells three types of coins: normal fair coins; Heads-biased coins, which land Heads with probability 0.8; and Tails-biased coins, which land Heads with probability 0.2. I pick up a coin and examine it; since it looks mostly like a normal coin, I believe there’s 60% chance it’s s fair coin, and a 20% chance it’s biased either way. I decide to toss the coin four times, to gather some more evidence. The result is: Heads, Heads, Tails, Heads. How should I update my beliefs?</em></p>
<p>We know how to do this: we use Bayes’ theorem. We have
<span class="math display">\[\begin{align*}
\mathbb P(\text{fair} \mid \text{HHTH}) &amp;= \frac{\mathbb P(\text{fair})\, \mathbb P(\text{HHTH}\mid \text{fair})}{\mathbb P(\text{HHTH})} = \frac{0.6 \times 0.5^3 \times 0.5}{\mathbb P(\text{HHTH})} = \frac{0.0375}{\mathbb P(\text{HHTH})} \\
\mathbb P(\text{H-bias} \mid \text{HHTH}) &amp;= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHTH}\mid \text{H-bias})}{\mathbb P(\text{HHTH})} = \frac{0.2 \times 0.8^3 \times 0.2}{\mathbb P(\text{HHTH})} = \frac{0.02048}{\mathbb P(\text{HHTH})} \\
\mathbb P(\text{T-bias} \mid \text{HHTH}) &amp;= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHTH}\mid \text{T-bias})}{\mathbb P(\text{HHTH})} = \frac{0.2 \times 0.2^3 \times 0.8}{\mathbb P(\text{HHTH})} = \frac{0.00128}{\mathbb P(\text{HHTH})}  .
\end{align*}\]</span>
We also need to find <span class="math inline">\(\mathbb P(\text{HHTH})\)</span>. We could do that using the law of total probability. But a convenient short-cut is to notice that the above three probabilities have to add up to 1, and so that common denominator must be <span class="math inline">\(0.0375 + 0.02048 + 0.00128 = 0.05926\)</span>.</p>
<p>So, after tossing the coin four times, our belief has been updated from the “prior” (before) belief
<span class="math display">\[ \mathbb P(\text{fair}) = 0.6 \qquad \mathbb P(\text{H-bias}) = 0.2 \qquad \mathbb P(\text{T-bias}) = 0.2 \]</span>
to the “posterior” (after) belief
<span class="math display">\[ \mathbb P(\text{fair} \mid \text{data}) = 0.633 \qquad \mathbb P(\text{H-bias}\mid \text{data}) = 0.346 \qquad \mathbb P(\text{T-bias}\mid \text{data}) = 0.026 . \]</span></p>
</div>
</div>
<div id="bayesian-framework" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Bayesian framework</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe src="https://www.youtube.com/embed/9moh0KYc6fE">
</iframe>
</div>
</div>
<p>Let’s think more systematically about what we did in the previous example.</p>
<ul>
<li><strong>Model:</strong> The four coin tosses were modelled as four IID Bernoulli trials <span class="math inline">\(X_1, X_2, X_3, X_4 \sim \text{Bern}(\theta)\)</span> (if we let <span class="math inline">\(X_i = 1\)</span> denote that the <span class="math inline">\(i\)</span>th coin was Heads). Here, the probability of Heads is some unknown parameter <span class="math inline">\(\theta\)</span>. (Recall we talked about parametric models for data in Subsection <a href="S06-discrete-dist.html#models">6.5</a>.) This model gives a distribution that depends on the parameter: here we had a conditional PMF for one trial
<span class="math display">\[ p(x \mid \theta) = \theta^{x} (1 - \theta)^{1- x}  \]</span>
(this is a convenient way of writing the PMF for a Bernoulli trial), and the joint PMF for the IID trials
<span class="math display">\[ p(\mathbf x \mid \theta) = \prod_{i=1}^4 \theta^{x_i} (1 - \theta)^{1- x_i} = \theta^{x_1 + x_2 + x_3 + x_4} (1 - \theta)^{4- (x_1 + x_2 + x_3 + x_4)} . \]</span></li>
<li><strong>Prior:</strong> We started with a prior belief <span class="math inline">\(\pi(\theta)\)</span> on the value of the unknown parameter. In our case, we had the PMF
<span class="math display">\[ \pi(0.2) = 0.2 \qquad \pi(0.5) = 0.6 \qquad \pi(0.8) = 0.2 . \]</span></li>
<li><strong>Data:</strong> We collected the data <span class="math inline">\(\mathbf x\)</span>, which here had <span class="math inline">\(x_1 = 1\)</span>, <span class="math inline">\(x_2 = 1\)</span>, <span class="math inline">\(x_3 = 0\)</span>, <span class="math inline">\(x_4 = 1\)</span> (with 1 denoting Heads and 0 denoting Tails).</li>
<li><strong>Posterior:</strong> We calculated the posterior distribution <span class="math inline">\(\pi(\theta \mid \mathbf x)\)</span> for the parameter <em>given</em> the data. We did this using Bayes’ theorem:
<span class="math display">\[ \pi(\theta \mid \mathbf x) = \frac{\pi(\theta) \, p(\mathbf x \mid \theta)}{p(\mathbf x)} \propto \pi(\theta) \, p(\mathbf x \mid \theta) .\]</span>
We recovered the constant of proportionality – that is, the denominator of Bayes’ theorem – because we knew <span class="math inline">\(\pi(\theta \mid \mathbf x)\)</span> was a conditional PMF so must add up to 1. We ended up with
<span class="math display">\[ \pi(0.2 \mid \mathbf x) = 0.026 \qquad \pi(0.5 \mid \mathbf x) = 0.633 \qquad \pi(0.8 \mid \mathbf x) = 0.346 . \]</span></li>
</ul>
<p>This is the framework of how Bayesian statistics works: model, prior, data, posterior. To lay it out more generally, the procedure goes like this:</p>
<div class="thpart">
<ul>
<li><strong>Model:</strong> We start with a model for the data <span class="math inline">\(\mathbf x\)</span> that depends on one or more parameters <span class="math inline">\(\theta\)</span>, as expressed by a conditional PMF (for discrete data) or PDF (for continuous data) <span class="math inline">\(p(\mathbf x \mid \theta)\)</span>. This normally represents <span class="math inline">\(n\)</span> IID experiments, so
<span class="math display">\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n p(x_i \mid \theta) . \]</span>
This conditional distribution is often called the <strong>likelihood</strong>.</li>
<li><strong>Prior:</strong> We have a prior distribution <span class="math inline">\(\pi(\theta)\)</span> for the parameter <span class="math inline">\(\theta\)</span>, which can be either a PMF or PDF. The prior distribution represents our beliefs about the parameter before we collect the data; this can be based on previous evidence, expert opinion, personal intuition, etc.</li>
<li><strong>Data:</strong> We collect the data <span class="math inline">\(\mathbf x\)</span>.</li>
<li><strong>Posterior:</strong> We then form the posterior distribution <span class="math inline">\(\pi(\theta \mid \mathbf x)\)</span> for the parameter given the data, using Bayes’ theorem:
<span class="math display">\[\begin{align*}
\pi(\theta \mid \mathbf x) &amp;\propto \pi(\theta)\, p(\mathbf x \mid \theta) \\
\text{posterior} &amp;\propto \text{prior} \times \text{likelihood} .
\end{align*}\]</span>
This can either be a conditional PMF or PDF, but will be the same type as the prior <span class="math inline">\(\pi(\theta)\)</span>.</li>
</ul>
</div>
</div>
<div id="beta" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Beta distribution</h2>
<p>In our fake-coin example, we had a prior PMF for the parameter <span class="math inline">\(\theta = p\)</span> that could take only 3 possible value. But when doing Bayesian statistics with a parameter that represents a probability, it makes more sense to have a prior PDF that covers the whole interval <span class="math inline">\([0,1]\)</span>. After all, any parameter value that is given a probability of 0 in the prior always has a probability 0 in the posterior as well, no matter how strong the evidence in its favour; it’s considered good practice to only put 0 prior probability on parameter values that are <em>literally impossible</em>, such as probabilities below 0 or above 1. (This is sometimes called <a href="https://en.wikipedia.org/wiki/Cromwell%27s_rule">“Cromwell’s rule”</a>.)</p>
<p>One useful family of distributions to use as a prior distribution for a probability parameter is the Beta distribution, whose range is the whole interval <span class="math inline">\([0,1]\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-106" class="definition"><strong>Definition 10.1  </strong></span>A continuous random variable <span class="math inline">\(X\)</span> is said to have the <strong>Beta distribution</strong> with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> if it has the PDF
<span class="math display">\[ f(x) = \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^{\beta - 1} \qquad \text{for $0 \leq x \leq 1$}  \]</span>
and 0 otherwise. Here, the constant
<span class="math display">\[ B(\alpha, \beta) = \int_0^1 x^{\alpha-1} (1-x)^{\beta - 1} \, \mathrm dx , \]</span>
known as the “Beta function”, ensures that the PDF integrates to 1. We write <span class="math inline">\(X \sim \text{Beta}(\alpha, \beta)\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-107" class="theorem"><strong>Theorem 10.1  </strong></span>Let <span class="math inline">\(X \sim \text{Beta}(\alpha,\beta)\)</span>. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb EX = \displaystyle\frac{\alpha}{\alpha + \beta}\)</span></li>
<li><span class="math inline">\(\operatorname{Var}(X) = \displaystyle\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} = \displaystyle\frac{\mu(1-\mu)}{\alpha+\beta + 1}\)</span>, where <span class="math inline">\(\mu = \mathbb EX\)</span>.</li>
</ol>
</div>
<p>(Proving this requires some awkward messing around with Gamma functions, which we won’t bother with here.)</p>
<p>So the idea is that the expectation of <span class="math inline">\(X\)</span> is decided on by the <em>relative</em> values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, while the variance is decided by the <em>total</em> value of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The following two pictures illustrate this:</p>
<p><img src="math1710_files/figure-html/beta-pic-1-1.png" width="672" /></p>
<p><img src="math1710_files/figure-html/beta-pic-2-1.png" width="672" /></p>
<p>Note also that <span class="math inline">\(\text{Beta}(1,1)\)</span> is the continuous uniform distribution from Example <a href="S08-continuous.html#exm:unifex">8.1</a>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-108" class="example"><strong>Example 10.2  </strong></span><em>A statistician is studying the probability <span class="math inline">\(\theta\)</span> that ordinary coins land Heads. She would like to use a prior distribution for <span class="math inline">\(\theta\)</span> with prior expectation <span class="math inline">\(0.5\)</span> and prior standard deviation <span class="math inline">\(0.01\)</span>. What Beta distribution would be appropriate to use?</em></p>
<p>To get <span class="math inline">\(\mathbb E\theta = 0.5\)</span>, we need <span class="math inline">\(\alpha = \beta\)</span>. Then the variance, which needs to be <span class="math inline">\(0.01^2 = 0.0001\)</span>, is
<span class="math display">\[ \operatorname{Var}(\theta) = \frac{\mu(1-\mu)}{\alpha+\beta+1} = \frac{0.25}{\alpha + \beta + 1} . \]</span>
This requires <span class="math inline">\(\alpha = \beta = 1250\)</span>. (Well, actually <span class="math inline">\(1249.5\)</span>.)</p>
</div>
</div>
<div id="beta-bern" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Beta–Bernoulli model</h2>
<p>Consider a Bernoulli likelihood, where <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are IID <span class="math inline">\(\text{Bern}(\theta)\)</span>, so have joint PMF
<span class="math display">\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n \theta^{x_i} (1-\theta)^{1 - x_i} = \theta^{\sum_i x_i} (1 - \theta)^{n-\sum_i x_i} = \theta^y (1 - \theta)^{n-y}, \]</span>
where we have written <span class="math inline">\(y = \sum_i x_i\)</span> for the total number of successes.
Consider further using a <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> prior for <span class="math inline">\(\theta\)</span>, so that
<span class="math display">\[ \pi(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta - 1} \propto \theta^{\alpha-1} (1-\theta)^{\beta - 1} \]</span>
(Because we’re going to use the “posterior has to add up to 1” trick at the end, we’re free to drop constants whenever we want.) This is known as the <strong>Beta–Bernoulli model</strong>.</p>
<p>Suppose we collect data <span class="math inline">\(\mathbf x = (x_1, x_2, \dots, x_i)\)</span>, with <span class="math inline">\(y = \sum_i x_i\)</span> successes. What now is the posterior distribution for <span class="math inline">\(\theta\)</span> given this data?</p>
<p>Using Bayes’ theorem, we have
<span class="math display">\[\begin{align*}
\pi(\mathbf x \mid \theta)
  &amp;\propto \pi(\theta) p(\mathbf x \mid \theta) \\
  &amp;= \theta^{\alpha-1} (1-\theta)^{\beta - 1} \times \theta^y (1 - \theta)^{n-y} \\
  &amp;= \theta^{\alpha + y - 1} (1 - \theta)^{\beta + n - y - 1} .
\end{align*}\]</span>
We can recognise immediately that this is proportional to the PDF for a <span class="math inline">\(\text{Beta}(\alpha + y, \beta + n - y)\)</span> distribution, so in particular, the constant of proportionality must be <span class="math inline">\(1/B(\alpha + y, \beta + n - y)\)</span>.</p>
<p>So we see that, like the prior, the posterior is also a Beta distribution, where the first parameter has gone from <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\alpha + y\)</span> and the second parameter has gone from <span class="math inline">\(\beta\)</span> to <span class="math inline">\(\beta + (n-y)\)</span>. In other words, <span class="math inline">\(\alpha\)</span> has increased by the number of successes, and <span class="math inline">\(\beta\)</span> has increased by the number of failures.
The expectation has gone from the prior expectation
<span class="math display">\[ \frac{\alpha}{\alpha + \beta} \]</span>
to the posterior expectation
<span class="math display">\[ \frac{\alpha + y}{\alpha + \beta + n} .\]</span>
This can be thought of as a sort of average between the prior expectation <span class="math inline">\(\alpha/(\alpha + \beta)\)</span> and the mean of the data <span class="math inline">\(y/n\)</span>.</p>
</div>
<div id="normal-normal" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Normal–normal model</h2>
<p>Consider a normal likelihood, where <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are IID <span class="math inline">\(\text{N}(\theta, \sigma^2)\)</span>, and where the expectation <span class="math inline">\(\theta\)</span> is the unknown parameter but the variance <span class="math inline">\(\sigma^2\)</span> is known. So the model has joint PDF
<span class="math display">\[ p(\mathbf x \mid \theta) \propto \prod_{i=1}^n \exp \left(- \frac{(x_i - \theta)^2}{2\sigma^2}\right) = \exp \left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i - \theta)^2}{\sigma^2} \right) . \]</span>
(Again, we only worry about distributions up to proportionality, because we work out the multiplicative constant at the end.)
In fact, when doing Bayesian statistics, it’s often convenient to write <span class="math inline">\(\tau = 1/\sigma^2\)</span> for the inverse of the known variance; this <span class="math inline">\(\tau\)</span> is called the <strong>precision</strong> and is also known. So with this notation, the model is
<span class="math display">\[ p(\mathbf x \mid \theta) \propto \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau(x_i - \theta)^2 \right) . \]</span></p>
<p>Consider further using a normal <span class="math inline">\(\text{N}(\mu_0, 1/\tau_0)\)</span> prior for the unknown expectation parameter <span class="math inline">\(\theta\)</span>. So the prior PDF is
<span class="math display">\[ \pi(\theta) \propto \exp \left( - \tfrac{1}{2} \tau_0(\theta - \mu_0)^2 \right) \]</span>
This is known as the <strong>normal–normal model</strong>.</p>
<p>Suppose we collect data <span class="math inline">\(\mathbf x = (x_1, x_2, \dots, x_n)\)</span>, and recall that we write <span class="math inline">\(\bar x = (\sum_i x_i)/n\)</span> for the sample mean.</p>
<p>To get the posterior distribution requires a bit of an algebra slog (see below), but the outcome is that the posterior distribution is
<span class="math display">\[ \theta \mid \mathbf x \sim \mathrm{N} \left( \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x, \frac{1}{\tau_0 + n\tau} \right)  . \]</span>
In other words, the posterior expectation is a weighted average of the prior expectation <span class="math inline">\(\mu_0\)</span> and the mean of the data <span class="math inline">\(\bar x\)</span>, and the more datapoints <span class="math inline">\(n\)</span> you get, the heavier the weighting on the data compared to the prior. Further, the precision has increased from the prior precision <span class="math inline">\(\tau_0\)</span> to the posterior precision <span class="math inline">\(\tau_0 + n\tau\)</span>; so the more data we get, the larger the precision gets, so the smaller the variance gets, and the more sure we get about the true value of <span class="math inline">\(\theta\)</span>.</p>
<div class="thpart">
<p><em>The Algebra Slog.</em> Before even getting to Bayes, let’s remind ourselves from <a href="P4.html#P4-long">Problem Sheet 4 Question B5</a> that
<span class="math display">\[ \sum_{i=1}^n \tau(x_i - \theta)^2 = \sum_{i=1}^n \tau(x_i - \bar x)^2 + n\tau(\theta - \bar x)^2 . \]</span>
Recalling that we can ignore multiplicative terms that don’t contain <span class="math inline">\(\theta\)</span>, thanks to our proportionality trick, and note also that a multiplicative term becomes an additive term inside an exponential. So we can always ignore any “plus constants” that don’t involve <span class="math inline">\(\theta\)</span> that are inside an exponential. Thus we get
<span class="math display">\[\begin{align*}
 \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau (x_i - \theta)^2 \right)
 &amp;=  \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau(x_i - \bar x)^2 - n\tau(\theta - \bar x)^2 \right)\\
 &amp;\propto \exp \left( -\tfrac{1}{2} n\tau(\theta - \bar x)^2 \right) \\
 &amp;= \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + n\tau \bar{x}^2 ) \right) \\
 &amp;\propto \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta) \right).
\end{align*}\]</span></p>
<p>Now we can invoke Bayes’ theorem, and continue to ignore “plus constants”, to get
<span class="math display">\[\begin{align*}
\pi(\mathbf x \mid \theta)
  &amp;\propto \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + \tau_0(\theta - \mu_0)^2 ) \right) \\
  &amp;=\exp \left( - \tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + \tau_0\theta^2 - 2\tau_0\mu_0\theta + \tau_0 \mu_0^2 )\right) \\
  &amp;\propto \exp \left( - \tfrac{1}{2} \big( (\tau_0 + n\tau)\theta^2 - 2 (\tau_0 \mu_0 +n\tau \bar x )\theta \big)\right) \\
  &amp;= \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta^2 - 2 \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \theta \right)\right)  \\
  &amp;\propto \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) . 
\end{align*}\]</span>
This is (proportional to) the PDF for a normal distribution with expectation
<span class="math display">\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]</span>
and precision <span class="math inline">\(\tau_0 + n\tau\)</span>.</p>
</div>
</div>
<div id="modern-bayes" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> Modern Bayesian statistics</h2>
<p>In this section, we’ve given just a brief taster of Bayesian statistics. Bayesian statistics is a deep and complicated subject, and you may have the opportunity to find out a lot more about it later in your university career.</p>
<p>We have seen that in Bayesian statistics, one brings in a subjective “prior” based on previous beliefs and evidence, then updates this prior based on the data. This contrasts with the more traditional <strong>frequentist statistics</strong>. In frequentist one only uses the data – no prior beliefs! – and judges to what extent the data is consistent or inconsistent with a hypothesis. (Frequentist statistics is the main subject studied in MATH1712 Probability and Statistics II.)</p>
<p>In the two main examples of Bayesian statistics we have looked at – the Bernoulli likelihood and the normal likelihood – we ended up with a posterior in the same parametric family as prior, just with different parameters. Such a prior is called a “conjugate prior”. Of course, these are very convenient and easy to work with. However, with more complicated likelihoods and more complicated priors – especially those not with a single parameter but with many parameters – calculating the posterior distribution can be very difficult. In particular, working out the constant of proportionality (even just approximately) and/or sampling from the posterior distribution are very hard problems.</p>
<p>For this reason, Bayesian statistics was for a long time a minor area of statistics. However, increase in computer power in the 1980s made some of these problems more tractable, and Bayesian statistics has increased in importance and popularity since then.</p>
<p>For a while, there was an occasionally fierce debate between “Bayesians” and “frequentists”. Frequentists thought that bringing subjective personal beliefs into things was unmathematical, while Bayesians thought that ignoring how plausible a hypothesis is before testing it is unscientific. The debate has now largely dissipated, and it is accepted that modern statisticians need to know about both frequentist and Bayesian methods.</p>
<p>There are still plenty of open problems in Bayesian statistics, and lots of these involve the computational side: finding algorithms that can efficiently calculate the normalising constants in posterior distributions or sample from those posterior distributions, especially when the parameter(s) have very high dimension.</p>
</div>
<div id="summary-10" class="section level2 unnumbered">
<h2>Summary</h2>
<div class="mysummary">
<ul>
<li>In Bayesian statistics, we start with a prior distribution for a parameter <span class="math inline">\(\theta\)</span>, and update to a posterior distribution given the data <span class="math inline">\(\mathbf x\)</span>, through <span class="math inline">\(\pi(\theta \mid \mathbf x) \propto \pi(\theta)p(\mathbf x \mid \theta)\)</span>, or <span class="math inline">\(\text{posterior} \propto \text{prior} \times \text{likelihood}\)</span>.</li>
<li>The Beta distribution is a useful family of distributions to use as priors for probability parameters.</li>
<li>A Beta prior for a Bernoulli likelihood leads to a Beta posterior with different parameters.</li>
<li>A normal prior for the expectation of a normal likelihood wioth known variance leads to a normal posterior with different parameters.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="P5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="P6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["math1710.pdf", "math1710.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
