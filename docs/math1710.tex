% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MATH1710 Probability and Statistics I},
  pdfauthor={Matthew Aldridge},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}

\usepackage{titlesec, environ}
\newif\ifcomm\commtrue
\NewEnviron{myanswers}{\ifcomm\BODY\fi}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{MATH1710 Probability and Statistics I}
\author{\href{mailto:math1710@leeds.ac.uk}{Matthew Aldridge}}
\date{University of Leeds, 2021--22}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{schedule}{%
\chapter*{Schedule}\label{schedule}}
\addcontentsline{toc}{chapter}{Schedule}

\textbf{Week 5} (25--29 October):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S05-discrete-rv}{\textbf{Section 5:} Discrete random variables}
\item
  \protect\hyperlink{P3}{\textbf{Problem Sheet 3:}} deadline for assessed questions: Monday 8 November
\item
  \protect\hyperlink{r-work}{\textbf{R Worksheet 5:} Plots II: Making plots better} deadline for assessed questions: Monday 1 November
\end{itemize}

\textbf{Week 4} (18--22 October):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S04-conditional}{\textbf{Section 4:} Independence and conditional probability}
\item
  \protect\hyperlink{P3}{\textbf{Problem Sheet 3:}} deadline for assessed questions: Monday 8 November
\item
  \protect\hyperlink{r-work}{\textbf{R Worksheet 4:} Plots I: making plots}
\end{itemize}

\textbf{Week 3} (11--15 October):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S03-classical}{\textbf{Section 3:} Classical probability}
\item
  \protect\hyperlink{P2}{\textbf{Problem Sheet 2:}} deadline for assessed questions: Monday 25 October
\item
  \protect\hyperlink{r-work}{\textbf{R Worksheet 3:} Data in R} deadline for assessed questions: Monday 18 October
\end{itemize}

\textbf{Week 2} (4--8 October):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S02-probability}{\textbf{Section 2:} Probability spaces}
\item
  \protect\hyperlink{P2}{\textbf{Problem Sheet 2:}} deadline for assessed questions: Monday 25 October
\item
  \protect\hyperlink{r-work}{\textbf{R Worksheet 2:} Vectors}
\end{itemize}

\textbf{Week 1} (27 September -- 1 October):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S01-eda}{\textbf{Section 1:} Exploratory data analysis}
\item
  \protect\hyperlink{P1}{\textbf{Problem Sheet 1:}} deadline for assessed questions: Monday 11 October
\item
  \protect\hyperlink{r-work}{\textbf{R Worksheet 1:} R basics}
\end{itemize}

\hypertarget{about}{%
\chapter*{About MATH1710}\label{about}}
\addcontentsline{toc}{chapter}{About MATH1710}

\hypertarget{organisation}{%
\section*{Organisation of MATH1710}\label{organisation}}
\addcontentsline{toc}{section}{Organisation of MATH1710}

This module is \textbf{MATH1710 Probability and Statistics I}. A few students will be taking this module as half of \textbf{MATH2700 Probability and Statistics for Scientists}.

This module lasts for 11 weeks from 27 September to 10 December 2021. The exam will take place between 10 and 21 January 2022.

The core teaching team are:

\begin{itemize}
\tightlist
\item
  Dr Matthew Aldridge (you can call me ``Matt'' or ``Dr Aldridge''): I am the module leader, the main lecturer, and the main author of these notes.
\end{itemize}

The shared email address for the core teaching team is \href{mailto:math1710@leeds.ac.uk}{\nolinkurl{math1710@leeds.ac.uk}}; please use this address, rather than emailing our personal addresses; this will ensure your email is seen as soon as possible.

\hypertarget{notes}{%
\subsection*{Notes and videos}\label{notes}}
\addcontentsline{toc}{subsection}{Notes and videos}

The main way you will learn new material for this module is by reading these notes and by watching the accompanying pre-recorded videos. There will be one section of notes each week, for a total of 11 sections, with the final section being a summary and revision.

Reading mathematics is a slow process. Each section should take one and a half to two hours to work through; we recommend you split this into two or more sessions. If you find yourself regularly getting through sections in much less than that amount of time, you're probably not reading carefully enough through each sentence of explanation and each line of mathematics, including understanding the motivation, checking the accuracy, and making your own notes.

You are probably reading the web version of the notes. If you want a PDF or ebook copy (to read offline or to print out), they can be downloaded via the top ribbon of the page. (Warning: I have not made as much effort to make the PDF and ebook as neat and tidy as I have the web version, and there may be formatting errors.)

We are very keen to hear about errors in the notes mathematical, typographical or otherwise. Please, please \href{mailto:math1710@leeds.ac.uk}{email us} if think you may have found any.

\hypertarget{problem-sheets}{%
\subsection*{Problem sheets}\label{problem-sheets}}
\addcontentsline{toc}{subsection}{Problem sheets}

There will be 5 problem sheets. Each problem sheet has a number of short and long questions for you to cover in your own time to help you learn the material, and two assessed questions, which you should submit for marking. The assessed questions on each problem sheet make up 3\% of your mark on this module, for a total of 15\%. Although the deadlines are on Mondays, you are advised to complete and submit the work in the previous week.

\begin{longtable}[]{@{}ccc@{}}
\toprule
Problem Sheet & Sections covered & Assessed work due \\
\midrule
\endhead
1 & 1 & Monday 11 October (Week 3) \\
2 & 2 and 3 & Monday 25 October (Week 5) \\
3 & 4 and 5 & Monday 8 November (Week 7) \\
4 & 6 and 7 & Monday 22 November (Week 9) \\
5 & 8, 9 and 10 & Monday 6 December (Week 11) \\
\bottomrule
\end{longtable}

Assessed questions should be submitted in PDF format through Gradescope. (Further Gradescope details will follow.) Most students choose to hand-write their solutions and then scan them to PDF using their phone; you should use a proper scanning app -- we recommend Microsoft Office Lens or Adobe Scan -- and not just submit photographs.

\hypertarget{lectures}{%
\subsection*{Lectures}\label{lectures}}
\addcontentsline{toc}{subsection}{Lectures}

You will have one online synchronous (that is, live, not recorded) ``lecture'' session each week, with me, run through Zoom. Because this is a large cohort, we will split into two groups:

\begin{itemize}
\tightlist
\item
  Group 1: Mondays at 1200
\item
  Group 2: Mondays at 1500
\end{itemize}

You should check your timetable to see which lecture group you are in.

This will not be a ``lecture'' in the traditional sense of the term, but will be an opportunity to re-emphasise material you have already learned from notes and videos, to give extra examples, and to answer common student questions, with some degree of interactivity via quizzes, polls, and the chat box.

We will assume you have completed all the work for the previous week by the time of the lecture.

We are very keen to hear about things you'd like to go through in the lectures; please \href{mailto:math1710@leeds.ac.uk}{email us} with your suggestions.

\hypertarget{tutorials}{%
\subsection*{Tutorials}\label{tutorials}}
\addcontentsline{toc}{subsection}{Tutorials}

Tutorials are small groups of about a dozen students. You have been assigned to one of 38 tutorial groups, each with a member of staff as the tutor. Your tutorial group will meet five times, in Weeks 2, 4, 6, 8, and 10. Tutorial groups will meet in person on campus; you should check your timetable to see when and where your tutorial group meets. (For those not yet on campus, due to travel restrictions or health conditions, there will be an extra online tutorial group for the first few tutorials.)

The main goal of the tutorials will be to go over your answers to the non-assessed questions on the problems sheets in an interactive session. In this smaller group, you will be able to ask detailed questions of your tutor, and have the chance to discuss your answers to the problem sheet. Your tutor may ask you to present some of your work to your fellow students, or may give you the opportunity to work together with others during the tutorial. Your tutor may be willing to give you a hint on the assessed questions if you've made a first attempt but have got stuck.

My recommended approach to problem sheets and tutorials is the following:

\begin{itemize}
\tightlist
\item
  Work through the problem sheet before the tutorial, spending plenty of time on it, and making multiple efforts at questions you get stuck on. I recommend spending \emph{at least 3 hours per week} on the problem sheets, which will usually mean a total of \emph{at least 6 hours per problem sheet} (as most problem sheets cover two weeks). Collaboration is encouraged when working through the non-assessed problems, but I recommend writing up your work on your own; answers to assessed questions must be solely your own work.
\item
  Take advantage of the small group setting of the tutorial to ask for help or clarification on questions you weren't able to complete.
\item
  After the tutorial, attempt again the questions you were previously stuck on.
\item
  If you're still unable to complete a question after this second round of attempts, \emph{then} consult the solutions.
\end{itemize}

Your tutor will also be the marker of your answers to the assessed questions on the problem sheets.

\hypertarget{r-worksheets}{%
\subsection*{R worksheets}\label{r-worksheets}}
\addcontentsline{toc}{subsection}{R worksheets}

R is a programming language that is particularly good at working with probability and statistics. Learning to use R is an important part of this module, and is used in many other modules in the University, particularly in MATH1712 Probability and Statistics II. R is used by statisticians throughout academic and increasingly in industry too. Learning to program is a valuable skill for all students, and learning to use R is particularly valuable for students interested in statistics and related topics like actuarial science.

You will learn R by working through one R worksheet each week in your own time. Worksheets 3, 5, 7, 9 and 11 will also contain a couple of questions for assessment. Each of these is worth 3\% of your mark for a total of 15\%. I recommend spending one hour per week on the week's R worksheet, plus one extra hour if there are assessed questions that week.

You can read more about the language R, and about the program RStudio that we recommend you use to interact with R, in \protect\hyperlink{R}{the R section of these notes}.

To help you if you have problems with R, we have organised optional \textbf{R troubleshooting drop-in sessions}, where you can discuss any problems you have with an R expert, in Weeks 2 and 3. Check your timetable for details -- these will be listed on your timetable as ``practicals''.

\hypertarget{dropin}{%
\subsection*{Optional ``office hours'' drop-in sessions}\label{dropin}}
\addcontentsline{toc}{subsection}{Optional ``office hours'' drop-in sessions}

If you there is something in the module you wish to discuss privately one-on-one with the module core teaching team, the place for the is the optional weekly ``office hours'', which will operate as drop-in sessions. These sessions are an optional opportunity for you to ask questions you have to a member of staff; these are particularly useful if there's something on the module that you are stuck on or confused about, but we're happy to discuss any statistics-related issues or questions you have.

There will be two ``office hours'' drop-in sessions per week:

\begin{itemize}
\tightlist
\item
  Wednesday at 1000 in PRD 9.320 (Physics Research Deck)
\item
  Wednesday at 1200 in \href{https://students.leeds.ac.uk/rooms?type=room\&id=99930}{Emmanuel Centre SR 02}
\end{itemize}

(For timetabling reasons, the 1000 sessions appear on the timetable for MATH2700 students and the 1200 sessions appear on the timetable for MATH1710 students, but I'm happy for anyone to attend either hour.)

\hypertarget{time}{%
\subsection*{Time management}\label{time}}
\addcontentsline{toc}{subsection}{Time management}

It is, of course, up to you how you choose to spend your time on this module. But my recommendations for your weekly work would be something like this:

\begin{itemize}
\tightlist
\item
  \textbf{Notes and videos:} 2 hours per week/section
\item
  \textbf{Problem sheet:} 3 hours per week (so 6 hours for most problem sheets) plus 1 extra hour for writing up and submitting answers to assessed questions
\item
  \textbf{R worksheet:} 1 hour per week/worksheet, plus 1 extra hour if there are assessed questions
\item
  \textbf{Lecture:} 1 hour per week
\item
  \textbf{Tutorial:} 1 hour every other week
\item
  \textbf{Revision:} 13 hours total at the end of the module
\end{itemize}

That's roughly 8 hours a week, and makes 100 hours in total. (MATH1710 is a 10 credit module, so is supposed to represent 100 hours work. MATH2700 students are expected to be able to use their greater experience to get through the material in just 75 hours, so should scale these recommendations accordingly.)

\hypertarget{exam}{%
\subsection*{Exam}\label{exam}}
\addcontentsline{toc}{subsection}{Exam}

There will be an exam in January, which makes up the remaining 70\% of your mark. The exam will consist of 20 short and 2 long questions, and will be time-limited to 2 hours. We'll talk more about the exam format near the end of the module.

\hypertarget{ask}{%
\subsection*{Who should I ask about\ldots?}\label{ask}}
\addcontentsline{toc}{subsection}{Who should I ask about\ldots?}

Remember that the email address for the core module teaching team is \href{mailto:math1710@leeds.ac.uk}{\nolinkurl{math1710@leeds.ac.uk}}. Please don't email our personal addresses; it will take longer for us to reply, and we may miss your email all together.

\begin{itemize}
\tightlist
\item
  \emph{I don't understand something in the notes or on a problem sheet}: Come to office hours, or (if the timing works) ask your tutor in your next tutorial.
\item
  \emph{I'm having difficulties with R:} In Weeks 2 or 3, you should attend the R trouble-shooting drop-in session; at other times, come to office hours.
\item
  \emph{I have an admin question about arrangements for the module:} Come to office hours or \href{mailto:math1710@leeds.ac.uk}{email the core module teaching team}.
\item
  \emph{I have an admin question about arrangements for my tutorial:} Contact your tutor.
\item
  \emph{I have an admin question about general arrangements for my course as a whole:} \href{mailto:Maths.Taught.Students@leeds.ac.uk}{Email the Maths Taught Students Office (Maths.Taught.Students@leeds.ac.uk)} or speak to your personal academic tutor.
\item
  \emph{I have a question about the marking of my assessed work on the problem sheets:} First, check your feedback on Gradescope; if you still have questions, contact your tutor.
\item
  \emph{I have a question about the marking of my assessed work on the R worksheets:} Come to office hours or \href{mailto:math1710@leeds.ac.uk}{email the core module teaching team}.
\item
  \emph{I have suggestion for something to cover in the lectures:} \href{mailto:math1710@leeds.ac.uk}{Email the core module teaching team}.
\item
  \emph{Due to exceptional personal circumstances I require an extension on or exemption from assessed work:} \href{mailto:Maths.Taught.Students@leeds.ac.uk}{Email the Maths Taught Students Office}; neither the core module teaching team nor your tutor are able to offer extensions or exemptions. (Only exemptions, not extensions, are available for R worksheets.)
\end{itemize}

\hypertarget{about-content}{%
\section*{Content of MATH1710}\label{about-content}}
\addcontentsline{toc}{section}{Content of MATH1710}

\hypertarget{prereqs}{%
\subsection*{Prerequisites}\label{prereqs}}
\addcontentsline{toc}{subsection}{Prerequisites}

The formal prerequisite for MATH1710 is ``Grade B in A-level Mathematics or equivalent''. We'll assume you have some basic school-level maths knowledge, but we don't assume you've studied probability or statistics in detail before (although we recognise that many of you will have). If you have studied probability and/or statistics at A-level (or post-16 equivalent) level, you'll recognise some of the material in this module; however you should find that we go deeper in some areas, and that we treat the material through with a greater deal of mathematical formality and rigour. ``Rigour'' here means precisely stating our assumptions, and carefully \emph{proving} how other statements follow from those assumptions.

\hypertarget{syllabus}{%
\subsection*{Syllabus}\label{syllabus}}
\addcontentsline{toc}{subsection}{Syllabus}

The module has three parts: a short first part on ``exploratory data analysis'', a long middle part on probability theory, and a short final part on a statistical framework called ``Bayesian statistics''. There's also the weekly R worksheets, which you could count as a fourth part running in parallel, but which will connect with the other parts too.

An outline plan of the topics covered is the following. (Remember that one section is one week's work.)

\begin{itemize}
\tightlist
\item
  \textbf{Exploratory data analysis} {[}1 section{]} Summary statistics, data visualisation
\item
  \textbf{Probability} {[}8 sections{]}

  \begin{itemize}
  \tightlist
  \item
    Probability with events: Probability spaces, probability axioms, examples and properties of probability, ``classical probability'' of equally likely events, independence, conditional probability, Bayes' theorem {[}3 sections{]}
  \item
    Probability with random variables: Discrete random variables, expectation and variance, binomial distribution, geometric distribution, Poisson distribution, multiple random variables, law of large numbers, continuous random variables, exponential distribution, normal distribution, central limit theorem {[}5 sections{]}
  \end{itemize}
\item
  \textbf{Bayesian statistics} {[}1 section{]}: Bayesian framework, Beta prior, normal--normal model
\item
  Summary and revision {[}1 section{]}
\end{itemize}

\hypertarget{books}{%
\subsection*{Books}\label{books}}
\addcontentsline{toc}{subsection}{Books}

You can do well on this module by reading the notes and watching the videos, attending the lectures and tutorials, and working on the problem sheets and R worksheets, without needing to do any further reading beyond this. However, students can benefit from optional extra background reading or an alternative view on the material, especially in the parts of the module on probability.

For exploratory data analysis, you can stick to Wikipedia, but if you really want a book, I'd recommend:

\begin{itemize}
\tightlist
\item
  GM Clarke and D Cooke, \emph{A Basic Course in Statistics}, 5th edition, Edward Arnold, 2004.
\end{itemize}

For the probability section, any book with a title like ``Introduction to Probability'' would do. Some of my favourites are:

\begin{itemize}
\tightlist
\item
  JK Blitzstein and J Hwang, \emph{Introduction to Probability}, 2nd edition, CRC Press, 2019.
\item
  G Grimmett and D Welsh, \emph{Probability: An Introduction}, 2nd edition, Oxford University Press, 2014. (The library has \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991002938669705181}{online access}.)
\item
  SM Ross, \emph{A First Course in Probability}, 10th edition, Pearson, 2020.
\item
  RL Scheaffer and LJ Young, \emph{Introduction to Probability and Its Applications}, 3rd edition, Cengage, 2010.
\item
  D Stirzaker, \emph{Elementary Probability}, 2nd edition, Cambridge University Press, 2003. (The library has \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991013131349705181}{online access}.)
\end{itemize}

I also found lecture notes by \href{https://people.maths.bris.ac.uk/~maotj/teaching.html}{Prof Oliver Johnson} (University of Bristol) and \href{http://www.statslab.cam.ac.uk/~rrw1/prob/index.html}{Prof Richard Weber} (University of Cambridge) to be useful.

On Bayesian statistics, I recommend:

\begin{itemize}
\tightlist
\item
  JV Stone, \emph{Bayes' Rule: A Tutorial Introduction to Bayesian Analysis}, Sebtel Press, 2013.
\end{itemize}

For R, there are many excellent resources online, and Google is your friend for finding them.

(For all these books I've listed the newest editions, but older editions are usually fine too.)

\hypertarget{about-notes}{%
\section*{About these notes}\label{about-notes}}
\addcontentsline{toc}{section}{About these notes}

These notes were written by Matthew Aldridge in 2021. They are based in part on previous notes by Dr Robert G Aykroyd and Prof Wally Gilks. Dr Jason Anquandah and Dr Aykroyd advised on the R worksheets. Dr Aykroyd's help and advice on many aspects of the module was particularly valuable.

These notes (in the web format) should be accessible by screenreaders. The videos have (highly imperfect) automated subtitles. If you have accessibility difficulties with these notes, contact \href{mailto:maths1710@leeds.ac.uk}{\nolinkurl{maths1710@leeds.ac.uk}}.

\hypertarget{part-part-i-eda}{%
\part*{Part I: EDA}\label{part-part-i-eda}}
\addcontentsline{toc}{part}{Part I: EDA}

\hypertarget{S01-eda}{%
\chapter{Exploratory data analysis}\label{S01-eda}}

\hypertarget{what-is-eda}{%
\section{What is EDA?}\label{what-is-eda}}

\textbf{Statistics} is the study of data. \textbf{Exploratory data analysis} (or \textbf{EDA}, for short) is the part of statistics concerned with taking a ``first look'' at some data. Later, toward the end of this course, we will see more detailed and complex ways of building models for data, and in MATH1712 Probability and Statistics II (for those who take it) you will see many other statistical techniques -- in particular, ways of testing formal hypotheses for data. But here we're just interested in first impressions and brief summaries.

In this section, we will concentrate on two aspects of EDA:

\begin{itemize}
\tightlist
\item
  \textbf{Summary statistics:} That is, calculating numbers that briefly summarise the data. A summary statistic might tell us what ``central'' or ``typical'' values of the data are, how spread out the data is, or about the relationship between two different variables.
\item
  \textbf{Data visualisation:} Drawing a picture based on the data is an another way to show the shape (centrality and spread) of data, or the relationship between different variables.
\end{itemize}

Even before calculating summary statistics or drawing a plot, however, there are other questions it is important to ask about the data:

\begin{itemize}
\tightlist
\item
  \emph{What is the data?} What variables have been measured? How were they measured? How many datapoints are there? What is the possible range of responses?
\item
  \emph{How was the data collected?} Was data collected on the whole population or just a smaller sample? (If a sample: How was that sample chosen? Is that sample representative of the population?) How were there variable measured?
\item
  \emph{Are there any outliers?} ``Outliers'' are datapoints that seem to be very different from the other datapoints -- for example, are much larger or much smaller than the others. Each outlier should be investigated to seek the reason for it. Perhaps it is a genuine-but-unusual datapoint (which is useful for understanding the extremes of the data), or perhaps there is an extraordinary explanation (a measurement or recording error, for example) meaning the data is not relevant. Once the reason for an outlier is understood, it then \emph{might} be appropriate to exclude it from analysis (for example, the incorrectly recorded measurement). It's usually bad practice to exclude an outlier merely for being an outlier before understanding what caused it.
\item
  \emph{Ethical questions:} Was the data collected ethically and, where necessary, with the informed consent of the subjects? Has it been stored properly? Are their privacy issues with the collection and storage of the data? What ethical issues should be considered before publishing (or not publishing) results of the analysis? Should the data be kept confidential, or should it be openly shared with other researchers for the betterment of science?
\end{itemize}

\hypertarget{what-is-R}{%
\section{What is R?}\label{what-is-R}}

\textbf{R} is a programming language that is particularly good at working with probability and statistics. A convenient way to use the language R is through the program \textbf{RStudio}. An important part of this module is learning to use R, by completing weekly worksheets -- you can read more in \protect\hyperlink{R}{the R section of these notes}.

R can easily and quickly perform all the calculations and draw all the plots in this section of notes on exploratory data analysis. In this text, we'll show the relevant R code. Code will appear like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.428571
\end{verbatim}

Here, the code in the first shaded box is the R commands that are typed into
RStudio, which you can type in next to the \texttt{\textgreater{}} arrow in the RStudio ``console''. The numerical answers that R returns are shown here in the second unshaded box next to a double hashsign \texttt{\#\#}. The \texttt{{[}1{]}} can be ignored (this is just R's way of saying that this is the first part of the answer -- but all the answers here only have one part anyway). Plots produced by R are displayed here as pictures.

Most importantly for now, \emph{you are not expected to understand the R code in this section yet}. The code is included so that, in the future, as you work through the R worksheets week by week, you can look back at the code in the section, and it will start to make sense. By the time you have finished R Worksheet 5 in week 5, you should be able understand most of the R code in this section.

\hypertarget{summary-stat}{%
\section{Summary statistics and boxplots}\label{summary-stat}}

Suppose we have collected some data on a certain variable. We will assume here that we have \(n\) datapoints, each of which is a single real number. We can write this data as a vector
\[ \mathbf x = (x_1, x_2, \dots, x_n) . \]

A \textbf{statistic} is a calculation from the data \(\mathbf x\), which is (usually) also a real number. In this section we will look at two types of ``summary statistics'', which are statistics that we feel will give us useful information about the data.

We'll look here at two types of summary statistic:

\begin{itemize}
\tightlist
\item
  \textbf{Measures of centrality}, which tell us where the ``middle'' of the data is.
\item
  \textbf{Measures of spread}, which tell us how far the data typically spreads out from that middle.
\end{itemize}

Some measures of centrality are the following.

\begin{definition}

Consider some real-valued data \(\mathbf x = (x_1, x_2, \dots, x_n)\).

\begin{itemize}
\tightlist
\item
  The \textbf{mode} is the most common value of \(x_i\). (If there are multiple joint-most common values, they are all modes.)
\item
  Suppose the data is ordered as \(x_1 \leq x_2 \leq \cdots \leq x_n\). Then the \textbf{median} is the central value in the ordered list. If \(n\) is odd, this is \(x_{(n+1)/2}\); if \(n\) is even, we normally take halfway between the two central points, \(\frac12(x_{n/2}+x_{n/2 + 1})\).
\item
  The \textbf{mean} \(\bar x\) is
  \[ \bar x = \frac{1}{n}(x_1 + x_2 + \cdots + x_n) = \frac1n \sum_{i=1}^n x_i . \]
\end{itemize}

\end{definition}

(In that last expression, we've made use of \href{https://www.mathcentre.ac.uk/resources/workbooks/mathcentre/sigma.pdf}{Sigma notation} to write down the sum.)

\begin{example}
Some packets of Skittles (a small fruit-flavoured sweet) were opened, and the number of Skittles in each packet counted. There were 13 packets, and the number of sweets (sorted from smallest to largest) were:
\[ 59, \ 59, \ 59, \ 59, \ 60, \ 60, \ 60, \ 61, \ 62, \ 62, \ 62, \ 63, \ 63 .\]
The mode is 59, because there were 4 packets containing 59 sweets; more than any other number. Since there are \(n = 13\) packets, the middle packet is number \(i = 7\), so the median is \(x_7 = 60\). The mean is
\[ \bar x = \frac{1}{13} (59 + 59 + \cdots + 63) = \frac{789}{13} = 60.7 .\]
\end{example}

The median is one example of a ``quantile'' of the data. Suppose our data is increasing order again. For \(0 \leq \alpha \leq 1\), the \textbf{\(\alpha\)-quantile} \(q(\alpha)\) of the data is the datapoint \(\alpha\) of the way along the list. So the median is the \(\frac12\)-quantile \(q(\frac12)\), the minimum is the 0-quantile \(q(0)\), and the maximum is the 1-quantile \(q(1)\). Generally, \(q(\alpha)\) is equal to \(x_{1+\alpha(n-1)}\) when \(1+\alpha(n-1)\) is an integer. (If \(1+\alpha(n-1)\) isn't an integer, there are various conventions of how to choose that we won't go into here. R has \emph{seven} different settings for choosing quantiles! -- we will always just use R's default choice.)

Two other common terms: \(q(\frac34)\) is called the \textbf{upper quartile} and \(q(\frac14)\) is called the \textbf{lower quartile} (note ``quartile'' -- as in ``quarter'' -- not ``quantile'', here). The upper and lower quartiles of the \(n = 13\) Skittles packets are the \(q(\frac14) = x_4 = 59\) and \(x_{10} = 62\).

Some measures of spread are:

\begin{definition}
The \textbf{number of distinct observations} is precisely that: the number of different datapoints you have after removing any repeats.

The \textbf{interquartile range} is the difference between the upper and lower quartiles \(\text{IQR} = q(\frac34) - q(\frac14)\).

The \textbf{sample variance} is
\[  s^2_x = \frac{1}{n-1} \left((x_1 - \bar x)^2 + \cdots + (x_n - \bar x)^2 \right) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 , \]
where \(\bar x\) is the sample mean from before. The \textbf{standard deviation} \(s_x = \sqrt{s^2_x}\) is the square-root of the sample variance.
\end{definition}

The formula we've given for sample variance is sometimes called the ``definitional formula'', as it's the formula used to \emph{define} the sample variance. We can rearrange that formula as follows:
\begin{align*}
  s^2_x &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 \\
      &= \frac{1}{n-1} \sum_{i=1}^n (x_i^2 - 2x_i\bar x + \bar x^2) \\
      &= \frac{1}{n-1}\left(\sum_{i=1}^nx_i^2 - \sum_{i=1}^n 2x_i\bar x + \sum_{i=1}^n\bar x^2 \right) \\
      &= \frac{1}{n-1} \left(\sum_{i=1}^n x_i^2 - 2\bar x \sum_{i=1}^n x_i + \bar x^2 \sum_{i=1}^n 1 \right) \\
      &= \frac{1}{n-1} \left(\sum_{i=1}^n x_i^2 - 2n\bar x^2 + n\bar x^2 \right) \\
      &= \frac{1}{n-1} \left(\sum_{i=1}^n x_i^2 -  n\bar x^2 \right) .
\end{align*}
Here, the first line is the definitional formula; the second line is from expanding out the bracket; the third line is taking the sum term-by-term; the fourth line takes any constants (things not involving \(i\)) outside the sums; the fifth line uses \(\sum_{i=1}^n x_i = n\bar x\), from the definition of the mean, and \(\sum_{i=1}^n 1 = 1 + 1 + \cdots 1 = n\); and the sixth line simplifies the final two terms.

This has left us with
\[ s^2_x = \frac{1}{n-1} \left(\sum_{i=1}^n x_i^2 -  n\bar x^2 \right) . \]
This is sometimes called the ``computational formula''; this is because it's usually more convenient to calculate the sample variance using this formula rather than the definitional formula.

The following R code reads in some data which has the daily average temperature in Leeds in 2020, divided into months. We can find, for example, the mean October temperature or the sample variance of the July temperature.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temperature }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://mpaldridge.github.io/math1710/data/temperature.csv"}\NormalTok{)}
\NormalTok{jul }\OtherTok{\textless{}{-}}\NormalTok{ temperature[temperature}\SpecialCharTok{$}\NormalTok{month }\SpecialCharTok{==} \StringTok{"jul"}\NormalTok{, ]}
\NormalTok{oct }\OtherTok{\textless{}{-}}\NormalTok{ temperature[temperature}\SpecialCharTok{$}\NormalTok{month }\SpecialCharTok{==} \StringTok{"oct"}\NormalTok{, ]}

\FunctionTok{mean}\NormalTok{(oct}\SpecialCharTok{$}\NormalTok{temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11.93548
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(jul}\SpecialCharTok{$}\NormalTok{temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.03226
\end{verbatim}

A \textbf{boxplot} is a useful way to illustrate data. It can be easier to tell the difference between different data sets ``by eye'' when looking at a boxplot, rather than examining raw summary statistics.

A boxplot is drawn as follows:

\begin{itemize}
\tightlist
\item
  The vertical axis represents the data values.
\item
  Draw a box from the lower quartile \(q(\frac14)\) to the median \(q(\frac12)\).
\item
  Draw another box on top of this from the median \(q(\frac12)\) to the upper quartile \(q(\frac34)\). Note that size of these two boxes put together is the interquartile range.
\item
  Decide which datapoints are outliers, and plot these with circles. (The R default is that any data point less than \(q(\frac14) - 1.5 \times \text{IQR}\) or greater than \(q(\frac34) + 1.5 \times \text{IQR}\) is an outlier.)
\item
  Out from the two previous boxes, draw ``whiskers'' to the smallest and largest non-outlier datapoints.
\end{itemize}

\begin{center}\includegraphics{math1710_files/figure-latex/boxplot1-1} \end{center}

Here are two boxplots from the July and October temperature data. What do you conclude about the data from these boxplots?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(jul}\SpecialCharTok{$}\NormalTok{temp, oct}\SpecialCharTok{$}\NormalTok{temp,}
        \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"July"}\NormalTok{, }\StringTok{"October"}\NormalTok{),}
        \AttributeTok{ylab =} \StringTok{"Daily maximum temperature (degrees C) in Leeds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{math1710_files/figure-latex/boxplot-temp-1} \end{center}

(And yes, I \href{https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/weather/learn-about/uk-past-events/interesting/2020/2020_05_july_temperature.pdf}{did check the outlier} to make sure it was a genuine datapoint.)

\hypertarget{binned}{%
\section{Binned data and histograms}\label{binned}}

Often when collecting data, we don't collect exact data, but rather collect data clumped into ``bins''. For example, suppose a student wished to use a questionnaire to collect data on how long it takes people to reach campus from home; they might not ask ``Exactly how long does it take?'', but rather give a choice of tick boxes: ``0--5 minutes'', ``5--10 minutes'', and so on.

Consider the following binned data, from \(n = 100\) students:

\begin{longtable}[]{@{}ccc@{}}
\toprule
Time & Frequency & Relative frequency \\
\midrule
\endhead
0--5 minutes & 4 & 0.04 \\
5--10 minutes & 8 & 0.08 \\
10--15 minutes & 21 & 0.21 \\
15--30 minutes & 42 & 0.42 \\
30--45 minutes & 15 & 0.15 \\
45--60 minutes & 8 & 0.08 \\
60--120 minutes & 2 & 0.02 \\
\textbf{Total} & 100 & 1 \\
\bottomrule
\end{longtable}

Here the \textbf{frequency} \(f_j\) of bin \(j\) is simply the number of observations in that bin; so, for example, 42 students had journey lengths of between 15 and 30 minutes. The \textbf{relative frequency} of bin \(j\) is \(f_j/n\); that is, the proportion of the observations in that bin.

What is the median journey length? Well, we don't know exactly, but \(0.04 + 0.08 + 0.21\) (the first three bins) is less than 0.5, while \(0.04 + 0.08 + 0.21 + 0.42\) (including the fourth bin) is greater than 0.5. So we know that the median student is in the fourth bin, the ``15--30 minute'' bin, and we can say that the median journey length is between 15 and 30 minutes.

What about the mode? The bin with the most observations in it is the ``15--30 minute'' bin. But this bin covers 15 minutes, while some of the other bins only cover 5 minutes. It would be a fairer comparison to look at the \textbf{frequency density}: the relative frequency divided by the size of the bin.

\begin{longtable}[]{@{}cccc@{}}
\toprule
Time & Frequency & Relative frequency & Frequency density \\
\midrule
\endhead
0--5 minutes & 4 & 0.04 & 0.008 \\
5--10 minutes & 8 & 0.08 & 0.016 \\
10--15 minutes & 21 & 0.21 & 0.042 \\
15--30 minutes & 42 & 0.42 & 0.028 \\
30--45 minutes & 15 & 0.15 & 0.010 \\
45--60 minutes & 8 & 0.08 & 0.005 \\
60--120 minutes & 2 & 0.02 & 0.0003 \\
\textbf{Total} & 100 & 1 & \\
\bottomrule
\end{longtable}

In the first row, for example, the relative frequency is 0.04 and the size of the bin is 5 minutes, so the frequency density is \(0.04/5 = 0.008\). So the modal bin -- the bin with the highest frequency \emph{density} -- is in fact the ``10--15 minutes'' bin.

Since we don't have the exact data, it's not possible to exactly calculate the mean and variance. However, we can often get a good estimate by assuming that each observation was in fact right in the centre of its bin. So, for example, we can assume that all 4 observations in the ``0--5 minutes'' bin were journeys of exactly 2.5 minutes. Of course, this isn't true (or is highly unlikely to be true), but we can often get a good approximation this way.

For our journey-time data, our approximation of the mean would be
\[ \bar x = \frac{1}{100} \big(4\times 2.5 + 8 \times 7.5 + \cdots + 2\times90) = 24.4 . \]
More generally, if \(m_j\) is the midpoint of bin \(j\) and \(f_j\) its frequency, then we can calculate the binned mean and binned variance by
\begin{align*}
  \bar x &= \frac{1}{n} \sum_j f_j m_j \\
  s^2_x  &= \frac{1}{n-1} \sum_j f_j (m_j - \bar x)^2
\end{align*}

Data in bins can be illustrated with a \textbf{histogram}. A histogram has the measurement on the x-axis, with one bar across the width of each bin, with bars drawn up to the height of the corresponding frequency density. Note that this means that the area of the bar is exactly the relative frequency of the corresponding bin. (If all the bins are the same width, frequency density is directly proportional to frequency and to relative frequency, so it can be clearer use one of those as the y-axis instead.)

Here is a histogram for our journey-time data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{journeys }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://mpaldridge.github.io/math1710/data/journeys.csv"}\NormalTok{)}
\NormalTok{bins }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{120}\NormalTok{)}

\FunctionTok{hist}\NormalTok{(journeys}\SpecialCharTok{$}\NormalTok{midpoint, }\AttributeTok{breaks =}\NormalTok{ bins,}
     \AttributeTok{xlab =} \StringTok{"Journey length (min)"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{math1710_files/figure-latex/journeys-1.pdf}

Often we draw histograms because the data was collected in bins. But even when we have exact data, we might choose to divide it into bins for the purposes of drawing a histogram. In this case we have to decide where to put the ``breaks'' between the bins. Too many breaks too close together, and the small number of observations in each bin will give ``noisy'' results (see left); too few breaks too far apart, and the histogram will lose detail (see right).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hist\_data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{2}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{3}\NormalTok{))  }\CommentTok{\# Some fake data}
\FunctionTok{hist}\NormalTok{(hist\_data, }\AttributeTok{breaks =} \DecValTok{40}\NormalTok{, }\AttributeTok{main =} \StringTok{"Too many bins"}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(hist\_data, }\AttributeTok{breaks =} \DecValTok{3}\NormalTok{,  }\AttributeTok{main =} \StringTok{"Too few bins"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.48\linewidth]{math1710_files/figure-latex/hist-bins-1} \includegraphics[width=0.48\linewidth]{math1710_files/figure-latex/hist-bins-2}

\hypertarget{multiple}{%
\section{Multiple variables and scatterplots}\label{multiple}}

Often, more than one piece of data is collected from each subject, and we wish to compare that data, to see if there is a relationship between the variables.

For example, we could take \(n\) second-year maths students, and for each student \(i\), collect their mark \(x_i\) in MATH1710 and their mark \(y_i\) in MATH1712. This gives is two ``paired'' datasets, \(\mathbf x = (x_1, x_2, \dots, x_n)\) and \(\mathbf y = (y_1, y_2, \dots, y_n)\). We can calculate sample statistics of draw plots for \(\mathbf x\) and for \(\mathbf y\) individually. But we might also want to see if there is a relationship \emph{between} \(\mathbf x\) and \(\mathbf y\): Do students with high marks in MATH1710 also get high marks in MATH1712?

A good way to visualise the relationship between two variables is to use a \textbf{scatterplot}. In a scatterplot, the \(i\)th data pair \((x_i, y_i)\) is illustrated with a mark (such as a circle or cross) whose x-coordinate has the value \(x_i\) and whose y-coordinate has the value \(y_i\).

In the following scatterplot, we have \(n = 50\) datapoints for the 50 US states; for each state \(i\), \(x_i\) is the Republican share of the vote in that state in the 2016 Trump--Clinton presidential election, and \(y_i\) is the Republican share of the vote in that state in the 2020 Trump--Biden election.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://mpaldridge.github.io/math1710/data/elections.csv"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(elections}\SpecialCharTok{$}\NormalTok{X2016, elections}\SpecialCharTok{$}\NormalTok{X2020,}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Republican share of the two{-}party vote, 2016 (\%)"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Republican share of the two{-}party vote, 2020 (\%)"}\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{50}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{50}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FloatTok{0.195}\NormalTok{, }\FloatTok{0.963}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{math1710_files/figure-latex/elections-1.pdf}

We see that there is a strong relationship between \(\mathbf x\) and \(\mathbf y\), with high values of \(x\) corresponding to high values of \(y\) and vice versa. Further, the points on the scatterplot lie very close to a straight line.

A useful summary statistic here is the \textbf{correlation}
\[ r_{xy} = \frac{s_{xy}}{s_x s_y} , \]
where \(s_{xy}\) is the \textbf{sample covariance}
\[ s_{xy} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) , \]
and \(s_x\) and \(s_y\) are the standard deviations.

The correlation \(r_{xy}\) is always between \(-1\) and \(+1\). Values of \(r_{xy}\) near \(+1\) indicate that the scatterpoints are close to a straight line with an upward slope (big \(x\) = big \(y\)); values of \(r_{xy}\) near \(-1\) indicate that the scatterpoints are close to a straight line with a downward slope (big \(x\) = small \(y\)); and values of \(r_{xy}\) near 0 indicate that there is a weak linear relationship between \(x\) and \(y\).

For the elections data, the correlation is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(elections}\SpecialCharTok{$}\NormalTok{X2016, elections}\SpecialCharTok{$}\NormalTok{X2020)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9919659
\end{verbatim}

which, as we expected, is extremely high.

\hypertarget{summary-01}{%
\section*{Summary}\label{summary-01}}
\addcontentsline{toc}{section}{Summary}

\begin{itemize}
\tightlist
\item
  Exploratory data analysis is about taking a first look at data.
\item
  Summary statistics are numbers calculated from data that give us useful information about the data.
\item
  Summary statistics that measure the centre of the data include the mode, median, and mean.
\item
  Summary statistics that measure the spread of the data include the number of distinct outcomes, the interquartile range, and the sample variance.
\item
  A summary statistic that measures the linear relationship between two variables is the correlation.
\item
  Boxplots, histograms, and scatterplots are useful ways of visualising data.
\end{itemize}

\hypertarget{P1}{%
\chapter*{Problem Sheet 1}\label{P1}}
\addcontentsline{toc}{chapter}{Problem Sheet 1}

\commfalse

This is Problem Sheet 1, which covers material from \protect\hyperlink{S01-eda}{Section 1} of the notes. You should work through all the questions on this problem sheet during Week 1, in preparation for your tutorial in Week 2. Questions C1 and C2 are assessed questions, and are due in by \textbf{2pm on Monday 11 October}. I recommend spending about 3 hours on this problem sheet in Week 1, plus 1 extra hour in Week 2 to neatly write up and submit your answers to the assessed questions.

\hypertarget{P1-short}{%
\section*{A: Short questions}\label{P1-short}}
\addcontentsline{toc}{section}{A: Short questions}

The first two questions are \textbf{short questions}, which are intended to be mostly not too difficult. Short questions usually follow directly from the material in the notes. Here, your should clearly state your final answer, and give enough working-out (or a short written explanation) for it to be clear how you reached that answer. You can check your answers with the solutions-without-working at the bottom of this sheet; solutions-with-working will be available later. If you get stuck on any of these questions, you might want to ask for guidance in your tutorial.

\textbf{A1.} Consider again the ``number of Skittles in each packet'' data from Example 1.1.
\[ 59, \ 59, \ 59, \ 59, \ 60, \ 60, \ 60, \ 61, \ 62, \ 62, \ 62, \ 63, \ 63 .\]

\textbf{(a)} Calculate the mean number of Skittles in each packet.

\begin{myanswers}
\emph{Solution.}
This was in the notes:
\[ \bar x = \frac{1}{13} (59 + 59 + \cdots + 63) =  \frac{789}{13} = 60.7 .\]

\end{myanswers}

\textbf{(b)} Calculate the sample variance using the computational formula.

\begin{myanswers}
\emph{Solution.}
\begin{align*}
  s_x^2 &= \frac{1}{13 - 1} \left( (59^2 + 59^2 + \cdots + 63^2) - 13 \times 60.6923^2)\right) \\
        &= \frac{1}{12} (47915 - 47886.2) \\
        &= 2.40
\end{align*}

\textbf{Group feedback:} With the computational formula, the value \(\sum_i x_i^2 - n \bar{x}^2\) is typically a fairly small number given as the difference between two very big numbers \(\sum_i x_i^2\) and \(n \bar x^2\). This means you have to get the two big numbers very precise, to ensure the cancellation happens correctly; in particular, make sure you use plenty of decimal places of accuracy in \(\bar x\).

\end{myanswers}

\textbf{(c)} Calculate the sample variance using the definitional formula.

\begin{myanswers}
\emph{Solution.}
\begin{align*}
  s_x^2 &= \frac{1}{13 - 1} \left( (59 - 60.7)^2 + (59 - 60.7)^2 + \cdots + (63 - 60.7)^2 \right) \\
        &= \frac{1}{12} (2.86 + 2.86 + \cdots + 5.33) \\
        &= \frac{1}{12} \times 28.77 \\
        &= 2.40
\end{align*}

\end{myanswers}

\textbf{(d)} Out of (b) and (c), which calculation did you find easier, and why?

\begin{myanswers}
\emph{Solution.}
The computational formula required fewer presses of the calculator buttons, because \(\sum_i x_i^2\) is fewer button-presses than \(\sum_i (x_i - \bar x)^2\), where you have to subtract the means before squaring.

On the other hand, the expression inside the brackets of the computational formula is a fairly small number given as the difference of two very large numbers, so it was necessary to use lots of decimal places of accuracy in \(\bar x\) to make sure the second large number was accurate and therefore that the subtraction cancelled correctly.

\end{myanswers}

\textbf{A2.} Consider the following data sets of the age of elected politicians on a local council. (The ``18--30'' consists of people older than and including 18, and younger than but \emph{not} including 30.)

\begin{longtable}[]{@{}cccc@{}}
\toprule
Age (years) & Frequency & Relative frequency & Frequency density \\
\midrule
\endhead
18--30 & 1 & & \\
30--40 & 3 & & \\
40--45 & 4 & & \\
45--50 & 5 & & \\
50--55 & 3 & & \\
55--60 & 1 & & \\
60--70 & 3 & & \\
\textbf{Total} & 20 & 1 & --- \\
\bottomrule
\end{longtable}

\textbf{(a)} Complete the table by filling in the relative frequency and frequency densities.

\begin{myanswers}

\emph{Solution.}

\begin{longtable}[]{@{}cccc@{}}
\toprule
Age (years) & Frequency & Relative frequency & Frequency density \\
\midrule
\endhead
18--30 & 1 & 0.05 & 0.0041 \\
30--40 & 3 & 0.15 & 0.015 \\
40--45 & 4 & 0.2 & 0.04 \\
45--50 & 5 & 0.25 & 0.05 \\
50--55 & 3 & 0.15 & 0.03 \\
55--60 & 1 & 0.05 & 0.01 \\
60--70 & 3 & 0.15 & 0.015 \\
\textbf{Total} & 20 & 1 & --- \\
\bottomrule
\end{longtable}

\end{myanswers}

\textbf{(b)} What is the median age bin?

\begin{myanswers}
\emph{Solution.} The 10th- and 11th-largest observations are both in the 45--50 bin, which is therefore the median bin.

\end{myanswers}

\textbf{(c)} Calculate (an approximation of) the mean age of the politicians.

\begin{myanswers}
\emph{Solution.}
Pretending that each person is in the centre of their bin, we have
\[ \bar x = \frac{1}{20} (1\times24 + 3\times 35 + \cdots + 3 \times 65) = \frac{946.5}{20} = 47.3 . \]

\end{myanswers}

\hypertarget{P1-long}{%
\section*{B: Long questions}\label{P1-long}}
\addcontentsline{toc}{section}{B: Long questions}

The next four questions are \textbf{long questions}, which are intended to be harder. Long questions often require you to think originally for yourself, not just directly follow procedures from the notes. Here, your answers should be written in complete sentences, and you should carefully explain in words each step of your working. Your answers to these questions -- not only their mathematical content, but also how to clearly write good solutions -- are likely to be the main topic for discussion in your tutorial.

\textbf{B1.} For each of the two datasets below, calculate the following summary statistics, or explain why it is not possible to do so: mode; median; mean; number of distinct outcomes; inter-quartile range; and sample variance.

\textbf{(a)} Six packets of Skittles are opened together, and the total number of sweets of each colour is:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\textbf{Colour} & Red & Orange & Yellow & Green & Purple \\
\midrule
\endhead
\textbf{Number of Skittles} & 67 & 71 & 87 & 74 & 62 \\
\bottomrule
\end{longtable}

\begin{myanswers}
\emph{Solution.}
The modal colour is Yellow. The number of distinct outcomes is 5.

It's not possible to calculate the median or the quartiles, because, unlike numerical data, the colours can't be put ``in order'' from smallest to largest.

It's not possible to calculate the mean or sample variance, as these require us to have numerical data that can be ``added up'', but this can't be done with colours.

\end{myanswers}

\textbf{(b)} Shirt sizes for a university football squad:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\textbf{Colour} & Xtra Small & Small & Medium & Large & Xtra Large \\
\midrule
\endhead
\textbf{Number of shirts} & 0 & 1 & 6 & 4 & 5 \\
\bottomrule
\end{longtable}

\emph{{[}\textbf{Note:} This has been corrected from an earlier version, where the 4 Large and 5 Xtra Large were the wrong way round.{]}}

\begin{myanswers}
\emph{Solution.}
The modal shirt size is medium. The number of distinct outcomes is 4 (we don't quite ``Xtra Small'', which was not observed in the data).

This time, we can order the data from smallest to largest, even though the data is not numerical. Since \((16 + 1)/2 - 8.5\), the median datapoint is the 8th or 9th datapoints, which are Large.

Since \(1 + 0.25(16 - 1) = 4.75\) the lower quartile is the 4th or 5th datapoints, which are Medium. Since \(1 + 0.75(16-1) = 12.25\), the upper quartile is the 12th or 13th datapoints, which are Xtra Large. So we can certainly say that the inner quartiles range from Medium to Xtra Large. We could probably also say that the interquartile range is 3 shirt sizes (Medium, Large, Xtra Large).

Again, because the data is not numerical, we can't add it up, so can't calculate a mean or sample variance.

\textbf{Group feedback:} Make sure your explanation is clear for why we can't calculate a median for the Skittles data but can for the shirts: they key is whether or not the data can be \emph{ordered}.

\end{myanswers}

\textbf{B2.} A summary statistic is informally said to be ``robust'' if it typically doesn't change much if a small number of outliers are introduced to a large dataset, or ``sensitive'' if it often changes a lot when a small number of outliers are introduced. Briefly discuss the robustness or sensitivity of the following summary statistics: \textbf{(a)} mode; \textbf{(b)} median; \textbf{(c)} mean; \textbf{(d)} number of distinct outcomes; \textbf{(e)} inter-quartile range; and \textbf{(f)} sample variance.

\begin{myanswers}
\emph{Solutions.}

\textbf{(a)} The mode will typically not change at all if a small number of outliers are introduced, so is robust. (The exception is for data where every observation is likely to be different, so the outliers become ``joint modes'' along with everything else; but in this case the mode is not a useful statistic in the first place.)

\textbf{(b)} The introduction of outliers will typically only change the median a little bit, by shifting it between different nearby values in the ``central mass'' of the data. In particular, the size of the outliers won't make any difference at all (only whether they are ``high outliers'' or ``low outliers''). So the median is robust.

\textbf{(c)} The mean can change a lot is outliers are introduced. (Think about the mean net worth of people in you tutorial group, and how it would change if Jeff Bezos or Elon Musk joined your tutorial group.) So the mean is sensitive.

\textbf{(d)} The number of distinct outcomes will only increase by (at most) 1 for each outlier introduced, so is robust.

\textbf{(e)} The interquartile range is robust, for the same reason as the median.

\textbf{(f)} The sample variance is sensitive, for the same reason as the mean.

(You might like to think about situations where it's better to use a robust statistic or better to use a sensitive statistic.)

\textbf{Group feedback:} Remember that ``robust'' and ``sensitive'' are general descriptions rather than precise mathematical definitions. So it doesn't matter if you disagree with my opinions provided that you give clear and detailed explanations to back up your opinion.

\end{myanswers}

\textbf{B3.} Let \(\mathbf a = (a_1, a_2, \dots a_n)\) and \(\mathbf b = (b_1, b_2, \dots, b_n)\) be two real-valued vectors of the same length. Then the \emph{Cauchy--Schwarz inequality} says that
\[ \left( \sum_{i=1}^n a_i b_i \right)^2 \leq \left( \sum_{i=1}^n a_i^2 \right) \left(\sum_{i=1}^n b_i^2 \right) . \]
Use the Cauchy--Schwarz inequality to show that the correlation \(r_{xy}\) satisfies \(-1 \leq r_{xy} \leq 1\).

(\emph{Hint:} Try to prove that \(s_{xy}^2 \leq s_x^2 s_y^2\). How does this help?)

\begin{myanswers}
\emph{Solutions.}
The first thing we want do is get from the Cauchy--Schwarz inequality
\[ \left( \sum_{i=1}^n a_i b_i \right)^2 \leq \left( \sum_{i=1}^n a_i^2 \right) \left(\sum_{i=1}^n b_i^2 \right) . \]
to the hint \(s_{xy}^2 \leq s_x^2 s_y^2\). We'll do this by making a clever choice for \((a_i)\) and \((b_i)\) in Cauchy--Schwarz that tells us something useful about \(s_{xy}\), \(s_x^2\), and \(s_y^2\).

Recalling the formulas for \(s_{xy}\), \(s_x^2\), and \(s_y^2\),
\begin{align*}
s_{xy} &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) ,\\
s_{x}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 ,\\
s_{y}^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar y)^2 ,
\end{align*}
and comparing them with the Cauchy--Schwarz inequality, it looks like taking \(a_i = x_i - \bar x\) and \(b_i = y_i - \bar y\) might be useful. Making the substitution, we get
\[ \left( \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) \right)^2 \leq \left( \sum_{i=1}^n (x_i - \bar x)^2 \right) \left(\sum_{i=1}^n (y_i - \bar y)^2 \right) . \]
These are very close to the formulas for \(s_{xy}\), \(s_x^2\), and \(s_y^2\), but are just missing the ``\(1/(n-1)\)''s; what we in fact have is
\[ \left( (n-1) s_{xy} \right)^2 \leq (n-1)s_x^2 \cdot (n-1) s_y^2 .\]
Cancelling \((n-1)^2\) from each side, we have \(s_{xy}^2 \leq s_x^2 s_y^2\), and we've proved the hint.

We now want to get from the hint to the desired statement \(-1 \leq r_{xy} \leq 1\). Recall the formula for the correlation is
\[ r_{xy} = \frac{s_{xy}}{s_xs_y} . \]
We can make the hint look a bit like this dividing both sides by \(s_x^2 s_y^2\), to get
\[\frac{s_{xy}^2}{s_x^2 s_y^2} \leq 1.   \]
In fact that's the square of the correlation on the left-hand side, so we've shown that \(r_{xy}^2 \leq 1\).

Finally, we note that if a number squared is less than or equal to 1, then the number must be between -1 and +1 inclusive. (Numbers bigger than 1 get bigger still when squared; number smalles than -1 become bigger than +1 when squared.) Hence we have shown that \(-1 \leq r_{xy} \leq 1\), as required.

\textbf{Group feedback:} There are two halves to this question: first get from the Cauchy--Schwarz inequality to the hint, and second get from the hint to the answer \(-1 \leq r_{xy} \leq 1\). Many students struggled with the first half -- but you can still try to do the second half. Especially in an exam, look for places where you can get marks for doing part of the question -- and it doesn't need to be the \emph{first} part!

\end{myanswers}

\textbf{B4.} A researcher wishes to study the effect of mental health on academic achievement. The researcher will collect data on the mental health of a cohort of students by asking them to fill in a questionnaire, and will measure academic achievement via the students' scores on their university exams. Discuss some of the ethical issues associated with the collection, storage, and analysis of this data, and with the publication of the results of the analysis. Are there ways to mitigate these issues?

(It's not necessary to write an essay for this question -- a few short bulletpoints will suffice. There may be an opportunity to discuss these issues in more detail in your tutorial.)

\begin{myanswers}
\textbf{Group feedback:} There are no ``correct'' or ``incorrect'' answers here, but here are a few things that students in my own tutorials brought up, which may act as a prompt for your own discussions.

\begin{itemize}
\tightlist
\item
  It's important the students/subjects have given their consent for their data to be used this way. It must be ``informed consent'', where they understand for what purpose the data will be used, how it will be stored, and so on. It must be possible and painless for students to decline to take part.
\item
  Consideration should be given on how to anonymise the data as much as possible -- it's not necessary for those analysing the data to know which questionnaire or which exam result belongs to which student, only that the questionnaire and results can be paired up.
\item
  Even if after data is anonymised, care should be taken about whether the students could be worked out from the data. For example, if only one student did a certain combination of modules, their identity could ``leak'' that way. Perhaps imprecise data, such as classes rather than exact marks, might help while only slightly reducing the usefulness of the data?
\item
  On one hand, it seems like this data should perhaps be deleted once analysis has been carried out, for the privacy of the students. On the other hand, principles of ``open science'' suggest that the data should be kept -- and even publically made available -- for other researchers to check the work. There are competing ethical considerations here.
\item
  If correlations are found in the data, care should be taken when publishing the analysis not to wrongly suggest a causation. (Just because X and Y are positively correlated, it doesn't mean that X \emph{causes} Y -- or that Y causes X.)
\end{itemize}

You can probably think of many other things.

\end{myanswers}

\hypertarget{P1-assessed}{%
\section*{C: Assessed questions}\label{P1-assessed}}
\addcontentsline{toc}{section}{C: Assessed questions}

The last two questions are \textbf{assessed questions}. This means you will submit your answers, and your answers will be marked by your tutor. These two questions count for 3\% of your final mark for this module. If you get stuck, your tutor may be willing to give you a hint in your tutorial.

The deadline for submitting your solutions is \textbf{2pm on Monday 11 October} at the beginning of Week 3, although I strongly recommend completing and submitting your work during Week 2. Submission will be via Gradescope; submission will open on Monday 4 October.
You should submit your answers as a single PDF file. Most students choose to hand-write their work, then scan it to PDF using their phone; if you do this, you should use a proper scanning app (like Microsoft Lens or Adobe Scan) -- please do not just submit photographs. We will discuss Gradescope submission further in the Week 2 lectures. Your work will be marked by your tutor and returned on Monday 18 September, when solutions will also be made available.

Question C1 is a ``short question'', where brief explanations or working are sufficient; Question C2 is a ``long question'', where the marks are not only for mathematical accuracy but also for the clarity and completeness of your explanations.

You should not collaborate with others on the assessed questions: your answers must represent solely your own work. The University's rules on \href{https://library.leeds.ac.uk/info/1401/academic_skills/46/academic_integrity_and_plagiarism}{academic integrity} -- and the related punishments for violating them -- apply to your work on the assessed questions.

\textbf{C1.} The monthly average exchange rate for US dollars into British pounds over a 12-month period was:
\begin{gather*}
1.306, \ 1.301, \ 1.290, \ 1.266, \ 1.290, \ 1.302,\\
1.317, \ 1.304, \ 1.284, \ 1.268, \ 1.247, \ 1.215.
\end{gather*}

\textbf{(a)} Calculate the median for this data.

\textbf{(b)} Calculate the mean for this data.

\textbf{(c)} Calculate the sample variance for this data.

\begin{myanswers}
\emph{Hints.}
Have you checked the definitions of these statistics from Subsection 1.3 of the notes?

\end{myanswers}

\textbf{(d)} Is the mode an appropriate summary statistic for this data? Why/why not?

\begin{myanswers}
\emph{Hint.}
Is there a unique mode for this data? Why not?

\end{myanswers}

\textbf{C2.}
~\textbf{(a)} Prove the following computational formula for the sample covariance:
\[ s_{xy} = \frac{1}{n-1} \left( \sum_{i=1}^n x_iy_i - n\bar x \bar y \right). \]

\begin{myanswers}
\emph{Hint.}
In Subsection 1.3 of the notes, we went from the definitional formula for the sample \emph{variance} to a computational formula. Can you follow a similar argument here?

\end{myanswers}

\textbf{(b)} Suppose that a dataset \(\mathbf x = (x_1, x_2, \dots, x_n)\) (with \(n \geq 2\)) has sample variance \(s_x^2 = 0\). Show that all the datapoints are in fact equal.

\begin{myanswers}
\emph{Hint.}
When is the square of something equal to 0? What can you say about the value of a square when it's nonzero? What can you say about a ``sum of squares'' -- that is, some numbers squared then added together?

\end{myanswers}

\hypertarget{P1-short-sols}{%
\section*{Solutions to short questions}\label{P1-short-sols}}
\addcontentsline{toc}{section}{Solutions to short questions}

\textbf{A1.} (a) 60.7 (b) \& (c) 2.40 (d) --- \textbf{A2.} (a) --- (b) 45--50 (c) 47.3

\hypertarget{part-part-ii-probability}{%
\part*{Part II: Probability}\label{part-part-ii-probability}}
\addcontentsline{toc}{part}{Part II: Probability}

\hypertarget{S02-probability}{%
\chapter{Probability spaces}\label{S02-probability}}

\renewcommand{\complement}{\mathsf{c}}
\newcommand{\comp}{\complement}

\hypertarget{what-is-prob}{%
\section{What is probability?}\label{what-is-prob}}

We now begin the big central block of this module, on probability theory.

Probability theory is the study of randomness. Probability, as an area of mathematics, is a fascinating subject in its own right. However, probability is particularly important due to its usefulness in applications -- especially in statistics (the study of data), in finance, and in actuarial science (the study of insurance).

Probability is well suited to modelling situations that involve randomness, uncertainty, or unpredictability. If we you want to predict the time of the next solar eclipse, a deterministic (that is, non-random) model based on physical laws will tell you when the sun, the moon, and the earth will be in the correct positions; but if you want to predict the weather tomorrow, or the price of a share of Apple stock next month, or the results of an election next year, you will need a probabilistic model that takes into account the uncertainty in the outcome. A probabilistic model could tell you the most likely outcome, or a range of the most probable outcomes.

So what do we mean when we talk about the ``probability'' of an event occurring? You might say that the probability of an event is a measure of ``how likely'' it is to occur, or what the ``chance'' of it occurring is.

More concretely, here are some interpretations of probability:

\begin{itemize}
\tightlist
\item
  \textbf{Subjective} (or \textbf{Bayesian}) \textbf{probability:} The probability of an event is the way someone expresses their degree of belief that the event will occur, based on their own judgement, and given the evidence they have seen. Their belief is measured on a scale from 0 to 1, from probabilities near 0 meaning they believe the event is very unlikely to occur to probabilities near 1 meaning they believe the event is very likely to occur.

  \begin{itemize}
  \tightlist
  \item
    This interpretation is philosophically sound, but a bit vague to be the basis for a mathematics module.
  \end{itemize}
\item
  \textbf{Classical} (or \textbf{enumerative}) \textbf{probability:} Suppose there are a finite number of equally likely outcomes. Then the probability of an event is the proportion of those outcomes that correspond to the event occurring. So when we say that a randomly dealt card has a probability \(\frac{1}{13}\) of being an ace, this is because there are 52 cards of which 4 are aces, so the proportion of favourable outcomes is \(\frac{4}{52} = \frac{1}{13}\).

  \begin{itemize}
  \tightlist
  \item
    This interpretation is good for simple procedures like flipping a fair coin, rolling a dice, or dealing cards, where the ``finite number of equally likely outcomes'' assumption holds. But we want to be able to study more complicated situations, where some outcomes are more likely than others, or where infinitely many different outcomes are possible.
  \end{itemize}
\item
  \textbf{Frequentist probability:} In a repeated experiment, the probability of an event is its long-run frequency. That is, if we repeat an experiment a very large number of times, the probability of the event is (approximately) the proportion of the experiments in which the event occurs. So when we say a biased coin has probability 0.9 of landing heads, we mean that were we toss it 1000 times, we would expect to see very close to \(0.9 \times 1000 = 900\) heads.

  \begin{itemize}
  \tightlist
  \item
    There are two problems with this. First, this doesn't deal with events that can't be repeated over and over again (like ``What's the probability that England win the 2022 World Cup?''). Second, to answer the question, ``Yes, but \emph{how} close to the probability should the proportion of occurrences be?'', you end up having to answer, ``Well, it depends on the probability,'' and you've got a circular definition.
  \end{itemize}
\item
  \textbf{Mathematical probability:} We have a function that assigns to each event a number between 0 and 1, called its probability, and that function has to obey certain mathematical rules, called ``axioms''.
\end{itemize}

It will not surprise you to learn that, in this mathematics course, we will take the ``mathematical probability'' approach. However, we will also learn useful things about the other approaches: we will see that classical probability is one special case of mathematical probability; we will see a result called the ``law of large numbers'' that says that the long-run frequency does indeed get closer and closer to the mathematical probability; and a result called ``Bayes' theorem'' will advise a subjectivist on how to update her subjective beliefs when she sees new evidence.

\hypertarget{sample-events}{%
\section{Sample spaces and events}\label{sample-events}}

Taking the ``mathematical probability'' approach, we will want to give a formal mathematical definition of the \emph{probability} of an event. But even before that, we need to give a formal mathematical definition of an \emph{event} itself. Our setup will be this:

\begin{itemize}
\tightlist
\item
  There is a set called the \textbf{sample space}, normally given the letter \(\Omega\) (upper-case Omega), which is the set of all possible outcomes.
\item
  An element of the sample space \(\Omega\) is a \textbf{sample outcome}, sometimes given the letter \(\omega\) (lower-case omega), represents one of the possible outcomes.
\item
  An \textbf{event} is a set of sample outcomes; that is, a subset of the sample space \(\Omega\). Events are often given letters like \(A\), \(B\), \(C\). We write \(A \subset \Omega\) to mean that \(A\) is an event in (or, equivalently, is a subset of) the sample space \(\Omega\).
\end{itemize}

This will be easier to understand with some concrete examples. We write a set (such as a sample space or an event) by writing all the elements of that set inside curly brackets \(\{\ \}\), separated by commas.

\begin{example}
Suppose we toss a (possibly biased) coin, and record whether it lands heads or tails. Then our sample space is \(\Omega = \{\mathrm H, \mathrm T\}\), where the sample outcome H denotes heads and the sample outcome T denotes tails.

The event that the coin lands heads is \(\{\mathrm H\}\).
\end{example}

\begin{example}
Suppose we roll a dice, and record the number rolled. Then our sample space is \(\Omega = \{1,2,3,4,5,6\}\), where the sample outcome \(1\) corresponds to rolling a one, and so on.

The event ``we roll an even number'' is \(\{2,4,6\}\). The event ``we roll at least a five'' is \(\{5,6\}\).
\end{example}

\begin{example}
Suppose we wish to count how many claims are made to an insurance company in a year. We could model this by taking the sample space \(\Omega\) to be \(\mathbb Z_+ = \{0, 1, 2, \dots\}\), the set of all non-negative integers.

The event ``the company receives less than 1000 claims'' is \(\{0, 1, 2, \dots, 998, 999\}\).
\end{example}

\begin{example}
Suppose we want a computer to pick a random number between 0 and 1. We could model this by taking the sample space \(\Omega\) to be the interval \([0, 1]\) of all real numbers between 0 and 1.

The event ``the number is bigger than \(\frac12\)'' is the sub-interval \((\frac12, 1]\) of all real numbers greater than \(\frac12\) but no bigger than 1. The event ``the first digit is a 7'' is the sub-interval \([0.7, 0.8)\). The event ``the random number is exactly \(1/\sqrt{2}\)'' is \(\{1/\sqrt{2}\}\).
\end{example}

In the first two examples, the sample space \(\Omega\) was finite. In third example, the sample space was infinite but ``countably infinite'', in that it could be counted using the discrete values of the positive integers. Both of these were for \emph{counting} discrete observations. In the fourth example, the sample space was infinite but ``uncountably infinite'', in that it had a sliding scale or ``continuum'' of gradually varying measurements. This was for \emph{measuring} continuous observations. This distinction will be important later in the course.

For any sample space \(\Omega\), there are two special events that always exist. There's \(\Omega\) itself, the event containing all of the sample outcomes, which represents ``something happens''. There's also the empty set \(\varnothing\), which contains none of the sample outcomes, which represents ``nothing happens''. Common sense suggests that \(\Omega\) should have probability 1, because \emph{something} is bound to happen -- this will later be one of our probability ``axioms''. Common sense also suggests that \(\varnothing\) should have probability 0, because it can't be that \emph{nothing} happens -- this will not be one probability axioms, but we'll show that it follows logically from the axioms we do choose.

\hypertarget{set-theory}{%
\section{Basic set theory}\label{set-theory}}

Since we've now defined events as being sets -- specifically, subsets of the sample space \(\Omega\) -- it will be useful to mention a little set theory here.

First, there are ways we can build new sets (or events) out of old. It's fine to just read the words and look at the pictures for these definitions, but those who want to read the equations too will need to know this:

\begin{itemize}
\tightlist
\item
  \(\omega \in A\) means ``\(\omega\) is in \(A\)'' or ``\(\omega\) is an element of \(A\)'', while \(\omega \not\in A\) means the opposite: \(\omega\) is \emph{not} in \(A\);
\item
  a colon \(:\) in the middle of set notation should be read as ``such that'';
\item
  so \(\{\omega \in \Omega : \text{fact about $\omega$}\}\) should be read as ``the set of sample points \(\omega\) in the sample space \(\Omega\) such that the fact is true''.
\end{itemize}

\begin{definition}

Consider a sample space \(\Omega\), and let \(A\) and \(B\) be events in that sample space.

\begin{itemize}
\tightlist
\item
  \textbf{{NOT:}} The \textbf{complement} of \(A\), written \(A^\mathsf{c}\) (and said ``\(A\) complement'' or ``not \(A\)''), is the set of sample points not in \(A\); that is
  \[ A^\mathsf{c}= \{\omega \in \Omega : \omega \not\in A \} . \]
  This represents the event that \(A\) does not occur.
\item
  \textbf{{AND}:} The \textbf{intersection} of \(A\) and \(B\), written \(A \cap B\) (and said ``\(A\) intersect \(B\)'' or ``\(A\) and \(B\)'') is the set of sample points in both \(A\) and \(B\); that is,\\
  \[ A \cap B = \{\omega \in \Omega : \omega \in A \text{ and } \omega \in B \} . \]
  This represents the event that both \(A\) and \(B\) occur.
\item
  \textbf{{OR:}} The \textbf{union} of \(A\) and \(B\), written \(A \cup B\) (and said ``\(A\) union \(B\)'' or ``\(A\) or \(B\)'') is the set of sample points in \(A\) or in \(B\); that is,
  \[ A \cup B = \{\omega \in \Omega : \omega \in A \text{ or } \omega \in B \} . \]
  This represents the event that \(A\) occurs or \(B\) occurs. (In mathematics, ``or'' includes ``both'', so a sample outcome in both \(A\) and \(B\) is in \(A\cup B\) too.)
\end{itemize}

~

\begin{center}\includegraphics[width=550pt]{math1710_files/figure-latex/venn-not-1} \end{center}

\begin{center}\includegraphics[width=550pt]{math1710_files/figure-latex/venn-and-1} \end{center}

\begin{center}\includegraphics[width=550pt]{math1710_files/figure-latex/venn-or-1} \end{center}

\end{definition}

\begin{example}
Suppose we are rolling a dice, so our sample space is \(\Omega = \{1,2,3,4,5,6\}\). Let \(A = \{2,4,6\}\) be the event that we roll and even number, and let \(B = \{5,6\}\) be the event that we roll at least a 5. Then
\begin{align*}
A^\mathsf{c}&= \{1,3,5\} = \{\text{roll an odd number}\} ,\\
A \cap B &= \{6\} = \{\text{roll a 6}\} ,\\
A \cup B &= \{2,4,5,6\} .
\end{align*}
\end{example}

An important case is when two events \(A, B\) cannot happen at the same time; that is, \(A \cap B = \varnothing\) (``\(A\) intersect \(B\) is the empty set''). In this case, we say that \(A\) and \(B\) are \textbf{disjoint} or \textbf{mutually exclusive}. For example, when \(\Omega\) is a deck of cards, then \(A = \{\text{the card is a spade}\}\) and \(B = \{\text{the card is red}\}\) are disjoint, because a card cannot be both a spade (a black suit) and red.

There are a few rules about combining the complement, intersection and union operations.

\begin{itemize}
\tightlist
\item
  The \textbf{double complement law} tells us that not-not-\(A\) is the same as \(A\):
  \[ (A^\mathsf{c})^\mathsf{c}= A .\]
  This says that if it's not ``not-raining'', then it's raining!
\item
  The \textbf{distributive laws} tells us we can ``mutiply out of the brackets brackets'' with sets:
  \begin{align*}
  A \cap (B \cup C) &= (A \cap B) \cup (A \cap C) ,\\
  A \cup (B \cap C) &= (A \cup B) \cap (A \cup C) .
  \end{align*}
\item
  \textbf{De Morgan's laws} tell us how complements interact with intersection/unions:
  \begin{align*}
  (A \cap B)^\mathsf{c}&= A^\mathsf{c}\cup B^\mathsf{c}\\
  (A \cup B)^\mathsf{c}&= A^\mathsf{c}\cap B^\mathsf{c}
  \end{align*}
  The first of these says that if it's not a Monday in October, then either it's not Monday or it's not October (or both). The second says that if a maths lecture is not ``useful or fun'', then it's not useful and it's not fun.
\end{itemize}

If you ever do need to prove one of these statements (or a similar one) you can use a Venn diagram or a truth table.

Let's prove the second distributive law,
\[   A \cup (B \cap C) = (A \cup B) \cap (A \cup C) , \]
with a Venn diagram. We can build the left-hand side of the law as:

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist1-1} \end{center}

~

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist2-1} \end{center}

~

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist3-1} \end{center}

The left-hand figure is \(\color{orange}{A}\), the middle figure is \(\color{purple}{B\cap C}\), and the right-hand figure is union of these, \(A\cup (B\cap C)\).

Then for the right-hand side of the law, we have:

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist4-1} \end{center}

~

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist5-1} \end{center}

~

\begin{center}\includegraphics[width=1\linewidth]{math1710_files/figure-latex/dist6-1} \end{center}

The left-hand figure is \(\color{orange}{A} \cup \color{blue}{B}\), the middle figure is \(\color{orange}{A}\cup \color{red}{C}\), and the right-hand figure is intersection of these, \((A\cup B)\cap (A\cup C)\).

We see that the areas shaded in two right-hand figures are the same, so it is indeed the case that
\(A\cup (B\cap C) = (A\cup B)\cap (A\cup C)\).

Let's also prove the first of De Morgan's laws,
\[ (A \cap B)^\mathsf{c}= A^\mathsf{c}\cup B^\mathsf{c}, \]
this time using a truth table. (This bit might be more clear from the video above, from 14:30.) We start with a table like this, with the four possibilities of whether \(A\) and/or \(B\) are true:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}@{}}
\toprule
\(A\) & \(B\) & \(A\cap B\) & \((A \cap B)^\mathsf{c}\) & \(A^\mathsf{c}\) & \(B^\mathsf{c}\) & \(A^\mathsf{c}\cup B^\mathsf{c}\) \\
\midrule
\endhead
False & False & & & & & \\
False & True & & & & & \\
True & False & & & & & \\
True & True & & & & & \\
\bottomrule
\end{longtable}

We fill in the first half to find a column for the left-hand side of the law \((A \cap B)^\mathsf{c}\):

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}@{}}
\toprule
\(A\) & \(B\) & \(A\cap B\) & \((A \cap B)^\mathsf{c}\) & \(A^\mathsf{c}\) & \(B^\mathsf{c}\) & \(A^\mathsf{c}\cup B^\mathsf{c}\) \\
\midrule
\endhead
False & False & False & \textbf{True} & & & \\
False & True & False & \textbf{True} & & & \\
True & False & False & \textbf{True} & & & \\
True & True & True & \textbf{False} & & & \\
\bottomrule
\end{longtable}

and the second half to find a column for the right-hand side of the law \(A^\mathsf{c}\cup B^\mathsf{c}\).

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}@{}}
\toprule
\(A\) & \(B\) & \(A\cap B\) & \((A \cap B)^\mathsf{c}\) & \(A^\mathsf{c}\) & \(B^\mathsf{c}\) & \(A^\mathsf{c}\cup B^\mathsf{c}\) \\
\midrule
\endhead
False & False & False & \textbf{True} & True & True & \textbf{True} \\
False & True & False & \textbf{True} & True & False & \textbf{True} \\
True & False & False & \textbf{True} & False & True & \textbf{True} \\
True & True & True & \textbf{False} & False & False & \textbf{False} \\
\bottomrule
\end{longtable}

Since the \((A \cap B)^\mathsf{c}\) column and the \(A^\mathsf{c}\cup B^\mathsf{c}\) column are the same, these must be the same sets.

\hypertarget{axioms}{%
\section{Probability axioms}\label{axioms}}

Recall that, in this mathematics course, a probability will be a real number that satisfies certain properties, which we call axioms.

\begin{definition}
\protect\hypertarget{def:axioms}{}\label{def:axioms}Let \(\Omega\) be a sample space. A \textbf{probability measure} on \(\Omega\) is a function \(\mathbb P\) that assigns to each event \(A \subset \Omega\) a real number \(\mathbb P(A)\), called the \textbf{probability} of \(A\), and that satisfies the following three axioms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbb P(A) \geq 0\) for all events \(A \subset \Omega\);
\item
  \(\mathbb P(\Omega) = 1\);
\item
  if \(A_1, A_2, \dots\) is a finite or infinite sequence of disjoint events, then
  \[ \mathbb P(A_1 \cup A_2 \cup \cdots) = \mathbb P(A_1) + \mathbb P(A_2) + \cdots . \]
\end{enumerate}

The sample space \(\Omega\) together with the probability measure \(\mathbb P\) are called a \textbf{probability space}.
\end{definition}

Axiom 1 says that all probabilities are non-negative numbers. Axiom 2 says the probability that \emph{something} happens is 1. Axiom 3 says that \emph{for disjoint events} the probability that one of them happens is the sum of the individual probabilities. (Those who like their mathematical statements super-precise should note that an infinite sequence in Axiom 3 must ``countable''; that is, indexed by the natural numbers \(1, 2, 3. \dots\).)

These axioms of probability (and our later results that follow from them) were first written down by the Russian mathematician \href{https://mathshistory.st-andrews.ac.uk/Biographies/Kolmogorov/}{Andrey Nikolaevich Kolmogorov} in 1933. This marked the point from when probability theory could now be considered a proper branch of mathematics -- just as legitimate as geometry or number theory -- and not just a past-time that can be useful to help gamblers calculate their odds. I always find it surprising that the axioms of probability are less than 90 years old!

There are other properties that it seems natural that a probability measure should have aside from the axioms -- for example, that \(\mathbb P(A) \leq 1\) for all events \(A\). But we will show shortly that other properties can be proven just by starting from the three axioms.

But first, let's see some examples.

\begin{example}

Suppose we wish to model tossing an biased coin the is heads with probability \(p\), where \(0 \leq p \leq 1\).

Our probability space is \(\Omega = \{\text{H}, \text{T}\}\). The probability measure is given by
\begin{align*}
   \mathbb P(\varnothing) &= 0  &  \mathbb P(\{\text{H}\}) &= p \\
   \mathbb P(\{\text{T}\}) &= 1 - p  &  \mathbb P(\{\text{H},\text{T}\})  &= 1 .
\end{align*}

Let's check that the axioms hold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since \(0 \leq p \leq 1\), all the probabilites are greater than or equal to 0.
\item
  It is indeed the case that \(\mathbb P(\Omega) = \mathbb P(\{\text{H},\text{T}\}) = 1\).
\item
  The only nontrivial disjoint union to check is \(\{\text{H}\} \cup \{\text{T}\} = \{\text{H},\text{T}\}\). But
  \[ \mathbb P(\{\text{H}\}) + \mathbb P(\{\text{T}\}) = p + (1 - p) = 1 = \mathbb P(\{\text{H},\text{T}\}) , \]
  as required.
\end{enumerate}

\end{example}

\begin{example}
Suppose we wish to model rolling a dice.

Our sample space is \(\{1,2,3,4,5,6\}\). The probability measure is given by
\[ \mathbb P(A) = \frac{|A|}{6} , \]
where \(|A|\) is the number of sample outcomes in \(A\).

So, for example, the probability of rolling an even number is
\[ \mathbb P(\{2,4,6\}) = \frac36 = \frac12 . \]
\end{example}

The dice rolling is a particular case of the ``classical probability'' of equally likely outcomes. We'll look at this more in the next section, next week, and prove that the classical probability measure does indeed satisfy the axioms

\hypertarget{prob-properties}{%
\section{Properties of probability}\label{prob-properties}}

The axioms of Definition \ref{def:axioms} only gave us some of the properties that we would like a probability measure to have. Our task now (in this subsection and the next) is to carefully prove how these other properties follow from just those axioms. In particular, we're not allowed to make claims that ``seem likely to be true'' or ``are common sense'' -- we can only use the three axioms together with logical deductions and nothing else.

\begin{theorem}

Let \(\Omega\) be a sample space with a probability measure \(\mathbb P\). Then we have the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbb P(\varnothing) = 0\).
\item
  \(\mathbb P(A^\mathsf{c}) = 1 - \mathbb P(A)\) for all events \(A \subset \Omega\).
\item
  For events \(A\) and \(B\) with \(B \subset A\), we have \(\mathbb P(B) \leq \mathbb P(A)\).
\item
  \(0 \leq \mathbb P(A) \leq 1\) for all events \(A \subset \Omega\).
\end{enumerate}

\end{theorem}

Importantly, the third result here tells us how to deal with complements or ``not'' events: the probability of \(A\) \emph{not} happening is 1 minus the probability it does happen. This is often very useful.

\begin{proof}
Statements 1 and 2 are exercise for you on \protect\hyperlink{P2}{Problem Sheet 2}. We'll start with the third statement.

The key with most of these ``prove from the axioms'' problems is to think of a way to write the relevant events as part of a \emph{disjoint} union, then use Axiom 3. Here, since \(B\) is a subset of \(A\), it would be useful to write \(A\) as a disjoint union of \(B\) and "the bit of \(A\) that isn't in \(B\). That is, we have the disjoint union
\[ B \cup (A \cap B^\mathsf{c}) = A .\]

\begin{center}\includegraphics[width=320pt]{math1710_files/figure-latex/subs-1} \end{center}

Applying Axiom 3 to this disjoint union gives
\[ \mathbb P(B) + \mathbb P(A \cap B^\mathsf{c}) = \mathbb P(A) . \]

We're happy to see the first term on the left-hand side and the term on the right-hand side. But what about the awkward \(\mathbb P(A \cap B^\mathsf{c})\)? Well, by Axiom 1, we know that \(\mathbb P(A \cap B^\mathsf{c}) \geq 0\), and hence
\[ \mathbb P(B) + 0 \leq \mathbb P(A) , \]
and we are done with the third statement.

For the fourth statement, we have \(\mathbb P(A) \geq 0\) directly from Axiom 1, so only need to show that \(\mathbb P(A) \leq 1\). We can do this using the third statement of this theorem. For any event \(A \subset \Omega\), the third statement tells us that \(\mathbb P(A) \leq \mathbb P(\Omega)\). But Axiom 2 tells us that \(\mathbb P(\Omega) = 1\), so we are done.
\end{proof}

\hypertarget{addition}{%
\section{Addition rules for unions}\label{addition}}

If we have two or more events, we'd like to work out the probability of their union; that is, the probability that at least one of them occurs.

We already have an addition rule for \emph{disjoint} unions.

\begin{theorem}
Let \(A, B \subset \Omega\) be two disjoint events. Then
\[ \mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) . \]
\end{theorem}

\begin{proof}
In Axiom 3, take the finite sequence \(A_1 = A\), \(A_2 = B\).
\end{proof}

But what about if \(A\) and \(B\) are not disjoint? Then we have the following.

\begin{theorem}
Let \(A, B \subset \Omega\) be two events. Then
\[ \mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) - \mathbb P(A \cap B) . \]
\end{theorem}

You may have seen this result before. You've perhaps justified it by saying something like this: ``We can add the two probabilities together, except now we've double-counted the overlap, so we have to take the probability of that away.'' Maybe you drew a Venn diagram. That's OK as a way to remember the result -- but this is a proper university mathematics course, so we have to carefully \emph{prove} it starting from just the axioms and nothing else.

As always, the key is to find a way of writing \(A \cup B\) as a \emph{disjoint} union. (In general, \(A \cup B\) can be a non-disjoint union that has an overlap.) Well, if we want \(A \cup B = A \cup \{\text{something}\}\) to be a \emph{disjoint} union, then the ``something'' will have to be the bit of \(B\) that's not also in \(A\), which is \(B \cap A^\mathsf{c}\).

~

\begin{center}\includegraphics[width=1000pt]{math1710_files/figure-latex/add1-1} \end{center}

\begin{proof}
First note, following the discussion above, that we have
\[ A \cup B = A \cup (B \cap A^\mathsf{c}) , \]
where the union on the right is of the disjoint events \(A\) and \(B \cap A^\mathsf{c}\). Therefore we can use Axiom 3 to get
\begin{equation}
\mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B \cap A^\mathsf{c}) .    \label{eq:union1}
\end{equation}

The left-hand side looks good, and the first term on the right-hand side looks good. To deal with the second term on the right-hand side, we need to write it down as part of a disjoint union again. Can we find another one? Yes! We can use \(B \cap A^\mathsf{c}\) together with \(B \cap A\) to build the whole of \(B\). So have a disjoint union
\[ (B \cap A^\mathsf{c}) \cup (B \cap A) = B .\]

\begin{center}\includegraphics[width=320pt]{math1710_files/figure-latex/add2-1} \end{center}

Since this union is disjoint, we can use Axiom 3 again, to get
\[ \mathbb P(B \cap A^\mathsf{c}) + \mathbb P(B \cap A) = \mathbb P(B) . \]
Rearranging this gives
\begin{equation}
\mathbb P(B \cap A^\mathsf{c}) = \mathbb P(B) - \mathbb P(B \cap A).  \label{eq:union2}
\end{equation}

Finally, substituting \eqref{eq:union2} into \eqref{eq:union1} gives
\[ \mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) - \mathbb P(A \cap B) , \]
as required.
\end{proof}

\begin{example}
\emph{Consider picking a card from a deck at random, with \(\mathbb P(A) = |A|/52\). What's the probability the card is a spade or an ace?}

It is possible to just to work this out directly. But let's use our addition law for unions.

We have \(\mathbb P(\text{spade}) = \frac{13}{52}\) and \(\mathbb P(\text{ace}) = \frac{4}{52}\). So we have
\[ \mathbb P(\text{spade or ace}) = \tfrac{13}{52} + \tfrac{4}{52} - \mathbb P(\text{spade and ace}) . \]
But \(\mathbb P(\text{spade and ace})\) is the probability of picking the ace of spades, which is \(\frac{1}{52}\). Therefore
\[ \mathbb P(\text{spade or ace}) = \tfrac{13}{52} + \tfrac{4}{52}  - \tfrac{1}{52} = \tfrac{16}{52} = \tfrac{4}{13} . \]
\end{example}

Similar addition rules can be proven in the same way for unions of more events. For three events, we have
\[
  \mathbb P(A \cup B \cup C) = \mathbb P(A) + \mathbb P(B) + \mathbb P(C) 
  - \mathbb P(A \cap B) - \mathbb P(A \cap C) - \mathbb P(B \cap C) + \mathbb P(A \cap B \cap C) .
\]
Note that we add the probabilities of individual events, then subtract the probabilities of pairs, then add the probability of the triple.

The \textbf{inclusion--exclusion principle} is the general rule:
\begin{multline*} \mathbb P(A_1 \cup A_2 \cup \cdots \cup A_n)
  = \sum_i \mathbb P(A_i)
    - \sum_{i \neq j} \mathbb P(A_i \cap A_j) \\
    + \sum_{i \neq j \neq k} \mathbb P(A_i \cap A_j \cap A_k)
    - \cdots
    + (-1)^{n-1} \mathbb P(A_1 \cap A_2 \cap \cdots \cap A_n) , \end{multline*}
where we continue by subtracting the probabilities of quadruples, adding the probabilities of five events, etc.

\hypertarget{summary-02}{%
\section*{Summary}\label{summary-02}}
\addcontentsline{toc}{section}{Summary}

\begin{itemize}
\tightlist
\item
  A sample space \(\Omega\) is a set representing all possible sample outcomes. An event is a subset of \(\Omega\).
\item
  For events \(A\) and \(B\), we also have the complement ``not \(A\)'' \(A^\mathsf{c}\), the intersection ``\(A\) and \(B\)'' \(A \cap B\), and the union ``\(A\) or \(B\)'' \(A \cup B\).
\item
  The axioms of probability are (1) \(\mathbb P(A) \geq 0\); (2) \(\mathbb P(\Omega) = 1\); and (3) that for disjoint events \(A_1, A_2, \dots\), we have \(\mathbb P(A_1 \cup A_2 \cup \cdots) = \mathbb P(A_1) + \mathbb P(A_2) + \cdots\).
\item
  Other properties can be proven from these axioms, like the complement rule \(\mathbb P(A^\mathsf{c}) = 1 - \mathbb P(A)\), and the addition rule for unions \(\mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) - \mathbb P(A \cap B)\).
\end{itemize}

\hypertarget{S03-classical}{%
\chapter{Classical probability}\label{S03-classical}}

\newcommand{\ff}[2]{{#1}^{\underline{#2}}}

\hypertarget{classical-intro}{%
\section{Probability with equally likely outcomes}\label{classical-intro}}

\textbf{Classical probability} is the name we give to probability where there are a finite number of equally likely outcomes.

Classical probability was the first type of probability to be formally studied -- partly because it is the simplest, and partly because it was useful for working out how to win at gambling. Tossing fair coins, rolling dice, and dealing cards are all common gambling situations that can be studied using classical probability -- in a deck of cards, for example, there are 52 cards that are equally likely to be drawn. Among the first works to seriously study classical probability were ``Book on Games of Chance'' by \href{https://mathshistory.st-andrews.ac.uk/Biographies/Cardan/}{Girolamo Cardano} (written in 1564, but not published until 1663, one hundred years later), and a famous series of letters letters between \href{https://mathshistory.st-andrews.ac.uk/Biographies/Pascal/}{Blaise Pascal} and \href{https://mathshistory.st-andrews.ac.uk/Biographies/Fermat/}{Pierre de Fermat} in 1654.

\begin{definition}
Let \(\Omega\) be a finite sample space. Then the \textbf{classical probability measure} on \(\Omega\) is given by
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} . \]
\end{definition}

So to work out a classical probability \(\mathbb P(A)\), crucially we need to be able to count how many outcomes \(|A|\) are in the event \(A\) and count how many outcomes \(|\Omega|\) are in the whole sample space \(\Omega\). (This is why classical probability is also called ``enumerative probability'' -- ``enumeration'' is another word for counting.) In this section, we'll look at some different ways in which we can count the number of outcomes in common events and sample spaces.

There's something we ought to check before going any further!

\begin{theorem}
Let \(\Omega\) be a finite nonempty sample space. Then the classical probability measure on \(\Omega\),
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} , \]
is indeed a probability measure, in that is satisfies the three axioms in Definition \ref{def:axioms}.
\end{theorem}

\begin{proof}

We'll take the axioms one by one.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since \(|\Omega| \geq 1\) and \(|A| \geq 0\), it is indeed the case that \(\mathbb P(A) = |A|/|\Omega| \geq 0\).
\item
  We have \({\displaystyle \mathbb P(\Omega) = \frac{|\Omega|}{|\Omega|} = 1}\), as required.
\item
  Since we have a finite sample space, we only need to show Axiom 3 for a sequence of two disjoint events; the argument can be repeated to get any finite number of events. Let \(A = \{a_1, a_2, \dots, a_k\}\) and \(B = \{b_1, b_2, \dots, b_l\}\) be two disjoint events with \(|A| = k\) and \(|B| = l\). Note that we can enumerate the elements of the disjoint union \(C = A \cup B\) as
  \[ c_1 = a_1, c_2 = a_2, \dots, c_k = a_k, c_{k+1} = b_1, c_{k+2} = b_2, \dots, c_{k+l} = b_l . \]
  Since \(A\) and \(B\) are disjoint, this list has no repeats, and we see that \(|C| = |A \cup B| = k+l\). Hence
  \[ \mathbb P(A \cup B) = \frac{k+l}{|\Omega|} = \frac{k}{|\Omega|} + \frac{l}{|\Omega|} = \mathbb P(A) + \mathbb P(B) , \]
  and Axiom 3 is fulfilled.
\end{enumerate}

\end{proof}

\hypertarget{multiplication}{%
\section{Multiplication principle}\label{multiplication}}

In classical probability, to find the probability of an event \(A\), we need to count the number of outcomes in \(A\) and the total number of possible outcomes in \(\Omega\). This can be easy when we're just looking at one choice -- like the 2 outcomes from tossing a single coin, the 6 outcomes of rolling a single dice, or the 52 outcomes from dealing a single card. Now we're going to look at what happens if there are a number of choices one after another -- like tossing multiple coins, rolling more than one dice, or dealing a hand of cards.

Here, an important principle is the \textbf{multiplication principle}. The multiplication principle says that if you have \(n\) choices followed by \(m\) choices, than all together you have \(n \times m\) total choices. You can see this by imagining the choices in a \(n \times m\) grid, with the \(n\) columns representing the first choice and \(m\) rows representing the second choice. For example, suppose you go to a burger restaurant where there are 3 choices of burger (beefburger, chicken burger, veggie burger) and 2 choices of sides (fries, salad), then altogether there are \(3 \times 2 = 6\) choices of meal.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\toprule
& Beefburger & Chicken burger & Veggie burger \\
\midrule
\endhead
\textbf{Fries} & 1: Beefburger with fries & 2: Chicken burger with fries & 3: Veggie burger with fries \\
\textbf{Salad} & 4: Beefburger with salad & 5: Chicken burger with salad & 6: Veggie burger with salad \\
\bottomrule
\end{longtable}

More generally, if you have \(m\) stages of choosing, with \(n_1\) choices in the first stage, then \(n_2\) choices in the second stage, all the way to \(n_m\) choices in the final stage, you have \(n_1 \times n_2 \times \cdots \times n_m\) total choices altogether.

\begin{example}
\emph{Five fair coins are tossed. What is the probability they all show the same face?}

Here, the sample space \(\Omega\) is the set of all sequences of 5 coin outcomes. How many sample outcomes are in \(\Omega\)? Well, the first coin can be heads or tails (2 choices); the second coin can be heads or tails (2 choices) and so on, until the fifth and final coin. So, by the multiplication principle, \(|\Omega| = 2 \times 2 \times 2 \times 2 \times 2 = 2^5 = 32\).

The event we're interested in is \(A = \{\text{HHHHH}, \text{TTTTT}\}\), the event that the faces are all the same -- either all heads or all tails. This clearly has \(|A| = 2\) outcomes.

So the probability all five coins show the same face is
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{2}{32} = \frac{1}{16} \approx 0.06. \]
\end{example}

\begin{example}
\emph{Five dice are rolled. What is the probability we get at least one 6?}

Here, \(\Omega\) is the set of all possible sequences of 5 dice rolls. Clearly \(|\Omega| = 6^5 = 7776\).

Also, \(A\) is the set of all dice roll sequences with at least one 6. Whenever you see a question with the phrase ``at least one'' in it, it's very often to look at the complementary event \(A^\mathsf{c}\) instead. We know from the last section that \(\mathbb P(A) = 1 - \mathbb P(A^\mathsf{c})\), but in ``at least one'' questions, it's often easier to count \(|A^\mathsf{c}|\) than to count \(|A|\).

Here, is \(A\) is the set of all dice roll sequences with at least one 6, then \(A^\mathsf{c}\) is the set of dice roll sequence with no 6 at all. This means all five dice must have rolled a 1, 2, 3, 4, or 5. Since each of the five dice rolls has 5 possibilities, this means that \(|A^\mathsf{c}| = 5^5 = 3125\).

Finally, we see that
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{|A^\mathsf{c}|}{|\Omega|} = 1 - \frac{5^5}{6^5} = \frac{4651}{7776} \approx 0.70 .\]
\end{example}

\hypertarget{sampling}{%
\section{Sampling with and without replacement}\label{sampling}}

\begin{example}
\emph{A bag contains 15 balls: 10 black balls and 5 white balls. We draw 3 balls out of the bag. What is the probability all 3 balls are black \textbf{(a)} if we put each ball back into the bag after it is chosen; \textbf{(b)} if we do not put each ball back into the bag after it is chosen.}

Let's start with (a). The number of ways to choose a ball out 15 on three occasions is \(|\Omega| = 15^3\). The number of ways to choose a black balls out of 10 on three occasions is \(|A| = 10^3\). Hence
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{10^3}{15^3} =  \frac{1000}{3375} = \frac{8}{27} \approx 0.30. \]

What about (b)? Here we don't put the ball back in the bag once it has been chosen. There are 15 ways to pick the first ball. But then there are only 14 balls left in the bag for the second choice, and only 13 balls for the third choice. So \(|\Omega| = 15\times14\times13\). Similarly, there are 10 ways the first ball can be black. But once that black ball is removed, only 9 choices for the second black ball, and only 8 for the third. So \(|A| = 10\times9\times8\). So this time we have
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{10\times9\times8}{15\times14\times13} =  \frac{720}{2730} = \frac{24}{91} \approx 0.26, \]
which is smaller than the answer in part (a).
\end{example}

This example illustrated the difference between \textbf{sampling with replacement} (when the balls were put back into the bag) and \textbf{sampling without replacement} (when the balls were not put back). If we want to sample \(k\) items from a set of \(n\) items, then:

\begin{itemize}
\tightlist
\item
  the number of ways to sample with replacement is
  \[ n^k = n\times n\times\cdots\times n;  \]
\item
  the number of ways to sample without replacement is
  \[ {n}^{\underline{k}} = n\times(n-1)\times \cdots\times (n-k+1) .\]
\end{itemize}

Here, we've defined the notation \({n}^{\underline{k}}\) for the number of ways to sample without replacement; this is called the \textbf{falling factorial} or \textbf{permutation number}. (Notice that the subscript is underlined here; other notations include \((n)_k\), \(P(n,k)\), or \({}^nP_k\).)

\hypertarget{ordering}{%
\section{Ordering}\label{ordering}}

\begin{example}
\emph{Suppose a lecturer marks a pile of \(n\) exam papers, all of which receive a different mark. What is the probability she ends up marking them in order from lowest scoring first in the pile to highest scoring last in the pile?}

Here, the sample space \(\Omega\) is the set of all orderings of the \(n\) exam papers by mark, and \(A\) is the event that the papers are in order from lowest to highest scoring. It's clear that \(|A| = 1\): since the exams scored different marks, there's only one way of putting the exams in the correct lowest-to-highest order. But what's \(|\Omega|\)?

There are \(n\) choices for the first exam paper to be marked. Then, for the second exam paper, there are \(n - 1\) choices left, because I'm not going to mark the same paper twice. There are \(n-2\) choices for the third exam paper. And so on, until I have marked \(n-1\) papers, and there is only 1 choice left for the final paper. So we have
\[ |\Omega| = {n}^{\underline{n}} = n(n-1)(n-2)\cdots3\cdot2\cdot1 = n! \]
ways to order the exam papers.

Hence, the probability the papers are marked in order is
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{1}{n(n-1)\cdots2\cdot1} = \frac{1}{n!} . \]
\end{example}

This number
\[ n! = {n}^{\underline{n}} = n(n-1)(n-2)\cdots3\cdot2\cdot1 \]
is called \textbf{\(n\) factorial} and denoted \(n!\). It is the number of ways that \(n\) different objects can be ordered.

It can sometimes be useful to write the falling factorial \({n}^{\underline{k}}\) in terms of the factorial, like this:
\begin{align*}
{n}^{\underline{k}} &= n(n-1)\cdots(n-k+1) \\
  &= n(n-1)\cdots(n-k+1)\times\frac{(n-k)(n-k-1)\cdots2\cdot1}{(n-k)(n-k-1)\cdots2\cdot1} \\
  &= \frac{n(n-1)\cdots(n-k+1)(n-k)(n-k-1)\cdots2\cdot1}{(n-k)(n-k-1)\cdots2\cdot1} \\
  &= \frac{n!}{(n-k)!}.
\end{align*}

\begin{example}
Suppose you shuffle a pack of cards. The resulting ordering of the deck has \(52!\) possibilities. This is an unimaginably huge number -- it's roughly
\[ 52! \approx 8 \times 10^{67} ; \]
that is, an 8 followed by 67 zeroes.

In comparison, the universe has existed for about \(4 \times 10^{17}\) seconds, and there are about \(7 \times 10^{9}\) people alive. If every person on the planet had shuffled a deck of cards one million times a second for the entire lifetime of the universe, they could only expect to have got through
\[ (4 \times 10^{17}) \times (7 \times 10^{9}) \times 10^6 \approx 3 \times 10^{33} .\]
This is only the most tiny, microscopic fraction of \(52!\). So every time you have ever shuffled a deck of cards, it is essentially certain that you have created an ordering of the deck that has never existed before.
\end{example}

\hypertarget{combinations}{%
\section{Sampling without replacement in any order}\label{combinations}}

\begin{example}
\protect\hypertarget{exm:lotto}{}\label{exm:lotto}\emph{In ``Lotto'', the UK national lottery, you can buy a ticket for £2 and choose 6 numbers between 1 and 59. If your 6 numbers match the 6 numbers chosen by the lottery machine, you win the jackpot (usually between £2 million and £20 million, shared between the tickets that get all 6 numbers). If you buy a ticket, what is the probability you win the jackpot?}

Here, \(\Omega\) is the set of all possible sets of 6 winning numbers, and \(A\) is the set of numbers on your ticket. Clearly \(|A| = 1\), but what is \(|\Omega|\)?

Well, the first number out of the machine has 59 possibilities, the second number has 58 possibilities, and so on, making
\[ 59 \times 58 \times 57 \times 56 \times 55 \times 54 = {59}^{\underline{6}} . \]

But this isn't the correct answer, because the same set of numbers could be drawn from the machine in any order! The sets of numbers \(\{1,2,3,4,5,6\}\) and \(\{1,2,3,4,6,5\}\) and \(\{6,5,4,3,2,1\}\) are all the same set of numbers. How many ways can we see the same list of numbers? This is precisely the number of orderings of 6 numbers, which we know is \(6!\). So the number of possible sets of 6 numbers to come out of the machine is actually
\[ \binom{59}{6} = \frac{{59}^{\underline{6}}}{6!} = \frac{59 \times 58 \times 57 \times 56 \times 55 \times 54}{6\times5\times4\times3\times2\times1} \approx 45 \text{ million} . \]

Thus the probability that your ticket wins the jackpot is
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{1}{\binom{59}{6}} \approx \frac{1}{45 \text{ million}} \approx 0.00000002 . \]
\end{example}

Here, we have introduced the notation
\[ \binom{n}{k} = \frac{{n}^{\underline{k}}}{k!} = \frac{n(n-1) \cdots (n-k+1)}{k(k-1)\cdots2\cdot1}  \]
for the number of ways to choose \(k\) objects out of \(n\) with replacement \emph{but where the order they were chosen in doesn't matter}. This is called the \textbf{binomial coefficient}, although when we say it out loud we normally just say \textbf{``\(n\) choose \(k\)''}. (Another notation for the binomial coefficient is \({}^n C_k\).)

It can sometimes be useful to remember that \({n}^{\underline{k}} = n!/(n-k)!\) allows us to write the binomial coefficient in terms of the factorial function as
\[ \binom nk = \frac{{n}^{\underline{k}}}{(n-k)!} = \frac{n!}{k!(n-k)!} . \]

\begin{example}
\emph{You are dealt a ``hand'' of 13 cards from a deck of 52 cards. What is the probability that you have the Ace, King, Queen, and Jack of Spades?}

Here, \(\Omega\) is the set of all 13-card hands from the deck, and \(A\) is the subset of those that contain the AKQJ of Spades.

Using the binomial coefficient notation, it's clear that
\[ |\Omega| = \binom{52}{13} = \frac{52\times51\times\cdots\times41\times40}{13\times12\times\cdots\times2\times1} . \]

What about \(|A|\)? If we fix the fact that the hand contains the 4 cards AKQJ of Spades, then it also contains \(13-4=9\) cards out of the other \(52-4 = 48\) remaining cards in the deck. This makes
\[ |A| = \binom{48}{9} = \frac{48\times47\times\cdots\times41\times40}{9\times8\times\cdots\times2\times1}\]
hands.

Thus the probability that the hand contains AKQJ of Spades is
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{\binom{48}{9}}{\binom{52}{13}} . \]

Conveniently, we can simplify the expression quite a lot, because plenty of cancellation will go on. We have
\begin{align*}
\mathbb P(A) = \frac{\binom{48}{9}}{\binom{52}{13}}
  &= \frac{\frac{48\times47\times\cdots\times41\times40}{9\times8\times\cdots\times2\times1}}{\frac{52\times51\times\cdots\times41\times40}{13\times12\times\cdots\times2\times1}} \\
  &= \frac{48\times47\times\cdots\times41\times40}{52\times51\times\cdots\times41\times40} \times \frac{13\times12\times\cdots\times2\times1}{9\times8\times\cdots\times2\times1} \\
  &= \frac{13\times12\times11\times10}{52\times51\times50\times49} \\
  &\approx 0.0026 ,
\end{align*}
or about 1 in every 380 hands.
\end{example}

There's one other useful fact about the binomial coefficient we should mention.

\begin{theorem}
\[ \binom nk = \binom{n}{n-k}. \]
\end{theorem}

We'll give two different proofs of this fact.

\begin{proof}
We can use the formula for the binomial coefficient in terms of the factorial function. This formula gives
\[ \binom nk = \frac{n!}{k!(n-k)!} \qquad \binom{n}{n-k} = \frac{n!}{(n-k)!k!} . \]
It's clear from the factorial expression that these two quantities are equal.
\end{proof}

\begin{proof}
Suppose we have \(n\) balls, and we want to paint \(k\) of the red and the other \(n-k\) blue. How many ways can we do this?

One way is to say there are \(\binom nk\) ways to choose the \(k\) balls to paint red, then we are forced paint the other \(n - k\) blue.

Another way is to say there are \(\binom{n}{n-k}\) to choose the \(n-k\) balls to paint blue, then we are forced to paint the other \(k\) red.

We have ``double counted'' the same quantity two different ways, so the answer must be the same, so
\[ \binom nk = \binom{n}{n-k}. \]
\end{proof}

Which proof do you prefer? (I strongly prefer the second proof, because I think it doesn't just \emph{verify} that the theorem is true, but further explains \emph{why} the result is true.)

\hypertarget{birthday}{%
\section{Birthday problem}\label{birthday}}

\emph{There are \(k = 23\) students in a class. What is the probability that at least two of the students share a birthday?}

This a famous problem, known as the ``birthday problem''. You may have seen this problem before. But let's try to solve it using the techniques from this section of notes. (We'll assume all days are equally likely for birthdays, and ignore the effect of leap days.)

The sample space \(|\Omega|\) is the set of possible birthdays for the \(k\) students. Clearly \(|\Omega| = 365^k\).

Let \(A\) be the even that at least two students share a birthday. Since this is an ``at least'' event, it seems like it might be a good idea to look instead at the complement event \(A^\mathsf{c}\) instead. If \(A\) is the event that there's at least one shared birthday, then \(A^\mathsf{c}\) is the event that there are \emph{no} shared birthdays; that is, \(A^\mathsf{c}\) is the event that all \(k\) students have \emph{different} birthdays.

So what is \(|A^\mathsf{c}|\), the number of ways the \(k\) students can have different birthdays? Well, the first student can have any of the 365 days for their birthday. For them to have different birthdays, the second student only has 364 days available. Then the third student must avoid the birthday of students 1 and 2, so has 363 available days, and so on. We see that
\[ |A^\mathsf{c}| = 365 \times 364 \times \cdots \times (365 - k + 1) = 365^{\underline{k}} . \]

Hence, the probability at least two students share a birthday is
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{365^{\underline{k}}}{365^k} = 1 - \frac{365}{365} \cdot \frac{364}{365} \cdots \frac{365-k+1}{365} . \]

Setting \(k = 23\), we can calculate the required answer in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{23}
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{prod}\NormalTok{((}\DecValTok{365}\SpecialCharTok{:}\NormalTok{(}\DecValTok{365} \SpecialCharTok{{-}}\NormalTok{ k }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{/} \DecValTok{365}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5072972
\end{verbatim}

The probability is 50.7\%. So it's more likely than not that at least two students share a birthday.

Some people find it surprising that only 23 students have such a high probability of sharing a birthday, since 23 is so small compared to 365. But remember there are \(\binom{23}{2} = 253\) \emph{pairs} of birthdays, each of which is a potential match, and 253 is quite a big number.

\hypertarget{summary-03}{%
\section*{Summary}\label{summary-03}}
\addcontentsline{toc}{section}{Summary}

\begin{itemize}
\tightlist
\item
  ``Classical probability'' describes the situation where there are finitely many equally likely outcomes. The classical probability \(\mathbb P(A) = |A|/|\Omega|\) requires us to count how many outcomes there are in events or sample spaces.
\item
  The multiplication principle says that \(n\) choices followed by \(m\) choices makes \(n \times m\) choices in total.
\item
  Sampling \(k\) objects out of \(n\) with replacement gives \(n^k\) choices.
\item
  Sampling \(k\) objects out of \(n\) without replacement gives \(n^{\underline{k}} = n(n-1)\dots(n-k+1)\) choices.
\item
  Ordering \(n\) objects can be done in \(n! = n^{\underline{n}} = n(n-1)\dots2\cdot1\) ways.
\item
  The number of ways to sample \(k\) objects out of \(n\) when the order doesn't matter is given by the binomial coefficient \(\binom nk = {n}^{\underline{k}}/k!\).
\end{itemize}

\hypertarget{P2}{%
\chapter*{Problem Sheet 2}\label{P2}}
\addcontentsline{toc}{chapter}{Problem Sheet 2}

\commfalse

This is Problem Sheet 2. This problem sheet covers \protect\hyperlink{S02-probability}{Section 2} and \protect\hyperlink{S03-classical}{Section 3} of the notes. You should work through all the questions on this problem sheet during Weeks 2 and 3, in preparation for your tutorial in Week 4. The problem sheet contains two assessed questions, which are due in by \textbf{2pm on Monday 25 October}.

\hypertarget{P2-short}{%
\section*{A: Short questions}\label{P2-short}}
\addcontentsline{toc}{section}{A: Short questions}

\textbf{A1.} Suppose you toss a coin 10 times. What would you suggest for a sample space \textbf{(a)} if you only care about the total number of heads; \textbf{(b)} if you care about the result of every coin toss?

\begin{myanswers}
\emph{Solution.}

\textbf{(a)} The number of heads can be any number from 0 to 10, so we should take \(\Omega = \{0,1,2,\dots, 10\}\). (This sample space contains 11 sample outcomes that are not equally likely.)

\textbf{(b)} The vector of coin outcomes will be something like \((\text{H}, \text{H}, \text{T}, \text{T}, \text{H}, \text{T}, \text{T}, \text{T}, \text{H}, \text{T})\). So our sample space \(\Omega\) should be the set of all vectors of length 10 whose entries are either H, T; the notation \(\Omega = \{\text{H}, \text{T}\}^{10}\) is sometimes used for this. (Note that, by the multiplication principle, this sample space contains \(2^{10} = 1024\) sample outcomes that are equally likely.)

\end{myanswers}

\textbf{A2.} Let \(A\), \(B\) and \(C\) be events in a sample space \(\Omega\). Write the following events using only \(A\), \(B\), \(C\) and the complement, intersection, and union operations.

\textbf{(a)} \(C\) happens but \(A\) doesn't.

\begin{myanswers}
\emph{Solution.} This is ``\(C\) and not \(A\)'': \(C\cap A^{\mathsf{c}}\).

\end{myanswers}

\textbf{(b)} At least one of \(A\), \(B\) and \(C\) happens.

\begin{myanswers}
\emph{Solution.} This is simply the union \(A \cup B\cup C\).

\end{myanswers}

\textbf{(c)} Exactly one of \(B\) or \(C\) happens.

\begin{myanswers}
\emph{Solution.} One way to write this is to split it up as ``\,`\(B\) but not \(C\)' or `\(C\) but not \(B\)'\,'', which is \((B \cap C^{\mathsf{c}}) \cup (B^{\mathsf{c}} \cap C)\).

An alternative is to split it up as ``\,`\(B\) or \(C\)' but not `both \(B\) and \(C\)'\,'', which is \((B \cup C) \cap (B\cap C)^{\mathsf{c}}\).

You can check these are equal by (for example) using De Morgan's law and the distributive law to expand out the second version.

\end{myanswers}

\textbf{(d)} Exactly two of \(A\), \(B\) and \(C\) happens.

\begin{myanswers}
\emph{Solution.} I would split this up into ``\(A\) and \(B\) but not \(C\)'', ``\(A\) and \(C\) but not \(B\)'', and ``\(B\) and \(C\) but not \(A\)'' and take the union. This gives
\[  (A \cap B \cap C^{\mathsf{c}}) \cup (A \cap B^{\mathsf{c}} \cap C) \cup (A^{\mathsf{c}} \cap B \cap C) . \]
There are other equivalent formulations.

\end{myanswers}

\textbf{A3.} Let \(\Omega\) be a sample space with a probability measure \(\mathbb P\), and let \(A, B \subset \Omega\) be events. State, with brief explanations, whether the following statements are true or false:

\textbf{(a)} If \(\mathbb P(A) \leq \mathbb P(B)\), then \(A \subset B\).

\begin{myanswers}
\emph{Solution.} False. It is true that \emph{if} \(A \subset B\) \emph{then} \(\mathbb P(A) \leq \mathbb P(B)\), but here the implication is the wrong way around.

For a counterexample, consider rolling a dice, and let \(A = \{1\}\) and \(B = \{2,3\}\). Then \(\mathbb P(A) = \frac16 \leq \frac26 = \mathbb P(B)\), but it's not true that \(A \subset B\).

\end{myanswers}

\textbf{(b)} \(\mathbb P(A \cap B) + \mathbb P(A \cap B^{\mathsf{c}}) = \mathbb P(A)\).

\begin{myanswers}
\emph{Solution.} True. Note that \((A \cap B) \cup (A \cap B^{\mathsf{c}}) = A\) and that the union is disjoint. (Try drawing a Venn diagram, if this isn't obvious.) The result follows from applying Axiom 3.

\end{myanswers}

\textbf{(c)} \(\mathbb P(A \cup B) \leq \mathbb P(A)\)

\begin{myanswers}
\emph{Solution.} False. On the contrary, \(A \subset A \cup B\), so the inequality should be the other way round. The same \(A\) and \(B\) as in part (a) gives a concrete counterexample. The statement would be true with a reversed inequality, or with the union replaced by an intersection.

\end{myanswers}

\textbf{(d)} If \(A\) and \(B\) are disjoint, then \(\mathbb P((A \cup B)^{\mathsf{c}}) = 1 - \mathbb P(A) - \mathbb P(B)\).

\begin{myanswers}
\emph{Solution.} True. From the complement rule, we have \(\mathbb P((A \cup B)^{\mathsf{c}}) = 1 - \mathbb P(A \cup B)\); then from the addition rule for disjoint unions we have \(\mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B)\). Putting these together gives the result.

\end{myanswers}

\textbf{A4.} What is the value of the following expressions?

\textbf{(a)} \(6!\)

\begin{myanswers}
\emph{Solution.}
\[ 6! = 6 \times 5 \times 4 \times 3 \times 2 \times 1 = 720. \]

\end{myanswers}

\textbf{(b)} \({8}^{\underline{4}}\)

\begin{myanswers}
\emph{Solution.}
\[ {8}^{\underline{4}} = 8 \times 7 \times 6 \times 5 = 1680 \]

\end{myanswers}

\textbf{(c)} \({\displaystyle \binom{10}{4}}\)

\begin{myanswers}
\emph{Solution.}
\[ \binom{10}{4} = \frac{10 \times 9 \times 8 \times 7}{4\times 3\times 2\times 1} = 210 \]

\end{myanswers}

\textbf{A5.} An urn contains 5 red balls and 7 blue balls. Four balls are drawn from the urn. What is the probability that at least one of the balls is red, if the balls are drawn \textbf{(a)} with replacement; \textbf{(b)} without replacement?

\begin{myanswers}
\emph{Solution.} This is an ``at least one'' question, so it will be better to look at the complementary event \(A^\mathsf{c}\) that none of the four balls drawn are red -- that is, that they are all blue.

\textbf{(a)} There are \(|\Omega| = 12^4 = 20736\) ways to draw four balls with replacement. There are \(|A^\mathsf{c}| = 2401\) to draw all blue balls. So
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{|A^\mathsf{c}|}{|\Omega|} = 1 - \frac{2041}{20736} = 0.884 . \]

\textbf{(b)} There are \(|\Omega| = {12}^{\underline{4}} = 11880\) ways to draw four balls without replacement. There are \(|A^\mathsf{c}| = {7}^{\underline{4}} = 840\) to draw all blue balls. So
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{|A^\mathsf{c}|}{|\Omega|} = 1 - \frac{840}{11880} = 0.929 . \]

\end{myanswers}

\hypertarget{P2-long}{%
\section*{B: Long questions}\label{P2-long}}
\addcontentsline{toc}{section}{B: Long questions}

\textbf{B1} Starting from just the three probability axioms, prove the following statements:

\textbf{(a)} \(\mathbb P(\varnothing) = 0\).

\begin{myanswers}
\emph{Solution.} As always, we seek a disjoint union, to allow us to use Axiom 3.

Let \(A\) be any event (such as \(A = \varnothing\) or \(A = \Omega\), for example). Then \(A \cup \varnothing = A\), and the union is disjoint -- since \(\varnothing\) contains no sample points, it certainly can't contain any sample points that are also in \(A\). Then applying Axiom 3, we get \(\mathbb P(A) + \mathbb P(\varnothing) = \mathbb P(A)\). Subtracting \(\mathbb P(A)\) from both sides gives the result.

\emph{Alternatively}, if you prove part (b) first, you can apply that with \(A = \Omega\). Since \(\Omega^\mathsf{c}= \varnothing\) and Axiom 2 tells us that \(\mathbb P(\Omega) = 1\), the result follows.

\end{myanswers}

\textbf{(b)} \(\mathbb P(A^\mathsf{c}) = 1 - \mathbb P(A)\).

\begin{myanswers}
\emph{Solution.} A very useful and relevant disjoint union is \(A \cup A^\mathsf{c}= \Omega\). Applying Axiom 3 gives us \(\mathbb P(A) + \mathbb P(A^\mathsf{c}) = \mathbb P(\Omega)\). But Axiom 2 tells us that \(\mathbb P(\Omega) = 1\), so \(\mathbb P(A) + \mathbb P(A^\mathsf{c}) = 1\). Rearranging gives the result.

\end{myanswers}

\textbf{B2.} Suppose we pick a number at random from the set \(\{1, 2, \dots, 2021\}\).

\textbf{(a)} What is the probability that the number is divisible by 5?

\begin{myanswers}
\emph{Solution.} The sample space is \(\Omega = \{1, 2, \dots, 2021\}\), and \(A\) is the set of numbers up to 2021 that are divisible by 5. Clearly \(|\Omega| = 2021\). Further, \(|A|\) is the largest integer no bigger than \(\frac{2021}{5} = 404.2\), which is 404, as this is how many times 5 ``goes into'' 2021. Hence
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{404}{2021} = 0.1999 , \]
just a tiny bit smaller than \(\frac{1}{5}\).

\end{myanswers}

\textbf{(b)} What is the probability the number is divisible by 5 or by 7?

\begin{myanswers}
\emph{Solution.} With the same \(\Omega\) and \(A\), we now have \(B\) being the numbers up to 2021 divisible by \(7\); so we're looking for \(\mathbb P(A \cup B)\). As before, \(|B|\) is the largest integer no bigger that \(\frac{2021}{7} = 288.7\), which is \(288\). So
\[ \mathbb P(A \cup B) = \frac{404}{2021} + \frac{288}{2021} - \mathbb P(A\cap B) . \]

Here, \(A \cap B\) is the numbers divisible by both 5 and 7, which is precisely the numbers divisible by \(5 \times 7 = 35\). Then \(|A \cap B|\) is \(\frac{2021}{35} = 57.7\) rounded down. So finally, we have
\[ \mathbb P(A \cup B) = \frac{404}{2021} + \frac{288}{2021} - \frac{57}{2021} = \frac{635}{2021} = 0.314. \]

\end{myanswers}

\textbf{B3.} In this question, you will have to use the standard two-event form of the addition rule for unions
\[ \mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) - \mathbb P(A \cap B) . \]

\textbf{(a)} Using the two-event addition rule, show that
\[ \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D \cup E) - \mathbb P\big(C \cap (D \cup E)\big).  \]

\begin{myanswers}
\emph{Solution.} As with the Cauchy--Schwarz question from Problem Sheet 1, the key is to make a good choice for what \(A\) and \(B\) should be. This time, \(A = C\) and \(D \cup E\) will work well, since \(C \cup (D \cup E) = C \cup D \cup E\). (You can call this ``associativity'', if you like.) Making that substitution immediately gives us
\[ \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D \cup E) - \mathbb P\big(C \cap (D \cup E)\big) ,  \]
as required.

\end{myanswers}

\textbf{(b)} Using the two-event addition rule and the distributive law, or otherwise, prove the three-event form of the addition rule for unions:
\[
  \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D) + \mathbb P(E) 
  - \mathbb P(C \cap D) - \mathbb P(C \cap E) - \mathbb P(D \cap E) + \mathbb P(C \cap D \cap E) .
\]

\begin{myanswers}
\emph{Solution.}
Let's take the three terms on the right of the equation from part (a) separately.

The first term is \(\mathbb P(C)\), which is fine as it is.

The second term is \(\mathbb P(D \cup E)\). This is the probability of the union of two events, so we can use addition rule for the union of two events to get
\[ \mathbb P(D \cup E) = \mathbb P(D) + \mathbb P(E) - \mathbb P(D \cap E) . \]

The third term is \(\mathbb P\big(C \cap (D \cup E)\big)\). If we use the distributive law, as suggested in the question, we get \(C \cap (D \cup E) = (C \cap D) \cup (C\cap E)\), so we want to find \(\mathbb P\big((C \cap D) \cup (C\cap E)\big)\). But this is another union of two events again, this time with \(A = C \cap D\) and \(B = C \cap E\). So the addition rule gives
\[ \mathbb P\big((C \cap D) \cup (C\cap E)\big) = \mathbb P(C \cap D) + \mathbb P(C \cap E) - \mathbb P(C \cap D \cap E) , \]
since \((C \cap D) \cap (C \cap E) = C \cap D \cap E\).

Finally, we put this all together, and get
\begin{align*}
  \mathbb P(C \cup D &\cup E) \\
  &= \mathbb P(C) + \big(\mathbb P(D) + \mathbb P(E) - \mathbb P(D \cap E)\big) - \big(\mathbb P(C \cap D) + \mathbb P(C \cap E) - \mathbb P(C \cap D \cap E)\big) \\
  &= \mathbb P(C) + \mathbb P(D) + \mathbb P(E) - \mathbb P(C \cap D) - \mathbb P(C \cap E) - \mathbb P(D \cap E) + \mathbb P(C \cap D \cap E) . 
\end{align*}
which is what we wanted.

\end{myanswers}

\textbf{B4.} Eight friends are about to sit down at random at a round table. Find the probability that

\textbf{(a)} Ashley and Brook sit next to each other, with Chris directly opposite Brook;

\begin{myanswers}
\emph{Solution.}
Let \(\Omega\) be the number of ways the friends can sit around the table. This is an ordering problem, so \(\Omega = 8!\).

Let \(A\) be the event in the question. What is \(|A|\)? Well,

\begin{itemize}
\tightlist
\item
  Ashley can sit anywhere, so has 8 choices of seat.
\item
  Brook can sit either directly to Ashley's left or directly to Ashley's right, so has 2 choices of seat.
\item
  Chris must sit directly opposite Brook, so only has 1 choice of seat.
\item
  The remaining five friends can fill up the remaining seats however they like, so have 5, 4, 3, 2, and 1 choices respectively.
\end{itemize}

Hence \(|A| = 8 \times 2 \times 1 \times 5 \times 4 \times 3 \times 2 \times 1\). Thus we get
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{8 \times 2 \times 1 \times 5 \times 4 \times 3 \times 2 \times 1}{8 \times 7 \times 6 \times 5 \times 4 \times 3 \times 2 \times 1} = \frac{2 \times 1}{7 \times 6} = \frac{1}{21} . \]

\end{myanswers}

\textbf{(b)} neither Ashley, Brook nor Chris sit next to each other.

\begin{myanswers}
\emph{Solution.}
The sample space \(\Omega\) is as before. Let's count the outcomes in \(B\), the event in the question.

\begin{itemize}
\tightlist
\item
  Ashley can sit anywhere, so has 8 choices of seat.
\item
  Chris's number of choices will depend on where Brook sits, so we'll have to count their choices together.

  \begin{itemize}
  \tightlist
  \item
    Brook cannot sit next to Ashley.
  \item
    If Brook sits next-but-one to Ashley, of which there are 2 choices, then Chris has 3 choices: Chris cannot sit on the seat directly between Ashley and Brook, nor directly next to Ashley on the other side, nor directly next to Brook on the other side.
  \item
    If Brook does sits neither next nor next-but-one to Ashley, of which there are 3 choices, then Chris has 2 choices: he cannot sit to the right or left of Ashley, nor to the right or left of Brook.
  \end{itemize}
\item
  The remaining friends have 5, 4, 3, 2, and 1 choices again.
\end{itemize}

Hence, \(|B| = 8 \times (2\times 3 + 3 \times 2) \times 5 \times 4 \times 3 \times 2 \times 1\). So
\[ \mathbb P(B) = \frac{|B|}{|\Omega|} = \frac{8 \times (2\times 3 + 3 \times 2) \times 5 \times 4 \times 3 \times 2 \times 1}{8 \times 7 \times 6 \times 5 \times 4 \times 3 \times 2 \times 1} = \frac{2\times 3 + 3 \times 2}{7 \times 6} = \frac{12}{42} = \frac{2}{7} .  \]

\emph{Alternatively}, last year a MATH1710 student suggested to me the following rather elegant solution. Suppose the five other friends are already sat at a round table with five chairs. Ashley, then Brook, then Chris will each bring along their own chair, and push into one of the gaps between the friends.

Ashley has 5 gaps to choose from, the Brook will have 6 (Ashley joining the table will have increased the number of gaps by 1), then Chris will have 7, so the total number of ways they can push in is \(|\Omega| = 5 \times 6 \times 7\).

To not sit next to each other, Ashley can push in any of the 5 gaps, Brook only has \(6 - 2 = 4\) choices (not in the gap directly to the left or right of Ashley), and Chris only has \(7 - 4 = 3\) choices (not in the gaps directly to the left or right of Ashley nor the gaps directly to the left or right of Brook -- these four gaps are distinct). Hence \(|B| = 5 \times 4 \times 3\), and we have
\[ \mathbb P(B) = \frac{5 \times 4 \times 3}{5 \times 6 \times 7} = \frac{4 \times 3}{6 \times 7} = \frac{12}{42} = \frac{2}{7}.  \]

\end{myanswers}

\textbf{B5.} Suppose your tutorial group contains 12 students -- you and 11 others. The tutor wishes to choose 4 members of the group to present their work.

\textbf{(a)} How many ways can the tutor choose the presentation group?

\begin{myanswers}
\emph{Solution.}
The tutor is sampling 4 items from 12, without replacement (the same person can't be picked twice) and where the order doesn't matter. So this is
\[ \binom{12}{4} = 495 . \]

\end{myanswers}

\textbf{(b)} How many ways can the tutor choose the presentation group if you are one of the presenters?

\begin{myanswers}
\emph{Solution.}
Once the tutor has chosen you, she must pick 3 other students to complete the presentation group out of the other 11 students. So this is
\[ \binom{11}{3} = 165 . \]

\end{myanswers}

\textbf{(c)} How many ways can the tutor choose the presentation group if you are \emph{not} one of the presenters?

\begin{myanswers}
\emph{Solution.}
If the tutor doesn't chose you, she must pick all 4 presenters out of the other 11 students. So this is
\[ \binom{11}{4} = 330 . \]

\end{myanswers}

\textbf{(d)} Pascal's formula says that
\[ \binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k} . \]
Prove Pascal's formula.

\begin{myanswers}
\emph{Solution.}
We'd like a ``double-counting'' argument, where we count the same number in two different ways. Let's follow the breadcrumbs from the first three parts of the question.

Suppose the tutorial group has \(n\) students and there will be \(k\) presenters. How many ways can the presentation group be chosen?

One way is simply to say that this is \(\displaystyle\binom{n}{k}\).

Another way is to separately count the presentation groups that do include you and the presentation groups that don't include you, and add them together. The presentation groups including you require another \(k-1\) presenters from the other \(n -1\) students, which makes \(\binom{n-1}{k-1}\). The presentation groups not including you require all \(k\) presenters from the other \(n-1\), which makes \(\binom{n-1}{k}\). In total, the number of presentation groups is
\[ \binom{n-1}{k-1} + \binom{n-1}{k} . \]

Since we've counted the number of presentation groups in two different ways, these expressions must be equal.

\emph{Alternatively}, you can prove algebraically that
\[  \frac{n(n-1)\cdots(n-k+1)}{k(k-1)\cdots2\cdot1} = \frac{(n-1)(n-2)\cdots(n-k+1)}{(k-1)(k-2)\cdots2\cdot1} + \frac{(n-1)(n-2)\cdots(n-k)}{k(k-1)\cdots2\cdot1} \]
(start by making \(k!\) the common denominator on the right), but I feel that's not in the spirit of the question.

\end{myanswers}

\hypertarget{P2-assessed}{%
\section*{C: Assessed questions}\label{P2-assessed}}
\addcontentsline{toc}{section}{C: Assessed questions}

The last two questions are \textbf{assessed questions}. These two questions count for 3\% of your final mark for this module.

The deadline for submitting your solutions is \textbf{2pm on Monday 25 October} at the beginning of Week 5, although I strongly recommend completing and submitting your work during Week 4. Submission will be via Gradescope; submission will open on Monday 18 October.
Your work will be marked by your tutor and returned on Monday 1 November, when solutions will also be made available.

Both questions are ``long questions'', where the marks are not only for mathematical accuracy but also for the clarity and completeness of your explanations.

You should not collaborate with others on the assessed questions: your answers must represent solely your own work. The University's rules on \href{https://library.leeds.ac.uk/info/1401/academic_skills/46/academic_integrity_and_plagiarism}{academic integrity} -- and the related punishments for violating them -- apply to your work on the assessed questions.

\textbf{C1.} Let \(A\) and \(B\) be two events with \(\mathbb P(A) = 0.8\) and \(\mathbb P(B) = 0.4\). The following questions concern the value of \(\mathbb P(A \cap B)\), the probability that both \(A\) and \(B\) occur.

\textbf{(a)} Prove the upper bound \(\mathbb P(A \cap B) \leq 0.4\).

\begin{myanswers}
\emph{Hint.}
Can you find an event \(C\) with \(A \cap B \subset C\) that will allow you to use the result \(\mathbb P(A \cap B) \leq C\)?

\end{myanswers}

\textbf{(b)} Prove that this upper bound can be achieved, by giving an example of a sample space \(\Omega\), a probability measure \(\mathbb P\) and events \(A, B \subset \Omega\) such that \(\mathbb P(A) = 0.8\) and \(\mathbb P(B) = 0.4\), with equality \(\mathbb P(A \cap B) = 0.4\) in the upper bound.

\begin{myanswers}
\emph{Hint.}
Try a sample space with five outcomes \(\Omega = \{a,b,c,d,e\}\) and the classical ``equally likely'' probability. How many elements are in \(A\)? How many in \(B\)? How many in the overlap \(A \cap B\)?

\end{myanswers}

\textbf{(c)} Give a lower bound for \(\mathbb P(A \cap B)\) -- that is, prove that \(\mathbb P(A \cap B) \geq \text{something}\) -- and show that this lower bound can be achieved. (Try to work out the correct bound, even if you can't formally prove it.)

\begin{myanswers}
\emph{Hint.}
Try the sample space with five-equally likely outcomes again. How small can you make the overlap \(A \cap B\)?

\end{myanswers}

\textbf{C2.} A ``random digit'' is a number chosen at random from \(\{0, 1, \dots, 9\}\), each with equal probability. A statistician choses \(n\) random digits.

\textbf{(a)} For \(k = 0, 1, \dots, 9\), let \(A_k\) be the event that all the digits are \(k\) or smaller. What is the probability of \(A_k\), as a function of \(k\) and \(n\)?

\begin{myanswers}
\emph{Hint.}
What's the sample space? Can you explain why \(|\Omega| = 10^n\)? What about \(|A_k\)\textbar?

\end{myanswers}

\textbf{(b)} Let \(B_k\) be the event that the largest digit chosen is equal to \(k\). What is the probability of \(B_k\)? Justify your answer carefully.

\begin{myanswers}
\emph{Hint.}
Can you find a relationship linking \(A_{k-1}\), \(A_k\), and \(B_k\)?

\end{myanswers}

\hypertarget{P2-short-sols}{%
\section*{Solutions to short questions}\label{P2-short-sols}}
\addcontentsline{toc}{section}{Solutions to short questions}

\textbf{A1.} (a) \(\{0,1,\dots, 10\}\) (b) The set of all \(2^{10}\) vectors of length 10 with entries from \(\{\text{H}, \text{T}\}\). \textbf{A2.} (a) \(C \cap A^\mathsf{c}\) (b) \(A \cup B \cup C\) (c) \((B \cup C) \cap (B \cap C)^\mathsf{c}\) or \((B \cap C^\mathsf{c}) \cup (B^\mathsf{c}\cap C)\) (d) \((A \cap B \cap C^\mathsf{c}) \cup (A \cap B^\mathsf{c}\cap C) \cup (A^\mathsf{c}\cap B \cap C)\) or other equivalent. \textbf{A3.} (a) False (b) True (c) False (d) True \textbf{A4.} (a) 720 (b) 1680 (c) 210 \textbf{A5.} (a) 0.884 (b) 0.929

\hypertarget{S04-conditional}{%
\chapter{Independence and conditional probability}\label{S04-conditional}}

\hypertarget{independent-events}{%
\section{Independent events}\label{independent-events}}

\emph{Suppose 40\% of people have blond hair, and 20\% of people have blue eyes. What proportion of people have both blond hair and blue eyes?}

The answer to this question is: we don't know. The question doesn't give us enough information to tell. However, \emph{if} it were the case that having blond hair didn't effect your chance of having blue eyes, then we could work out the answer. If that were true, we would think that the 20\% of people with blue eyes equally made up both 20\% of the blonds and also 20\% of the non-blonds. Thus the proportion of people with blond hair and blue eyes would be this 20\% of the 40\% of people with blond hair, and 20\% of 40\% is \(0.2 \times 0.4 = 0.08\), or 8\%.

To put it in probability language, \emph{if} blond hair and blue eyes were unrelated, then we would expect that
\[ \mathbb P(\text{blond hair and blue eyes}) = \mathbb P(\text{blond hair}) \times \mathbb P(\text{blue eyes}) . \]
This is an important property known as ``independence''.

\begin{definition}
Two events \(A\) and \(B\) are said to be \textbf{independent} if
\[ \mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B) .  \]
\end{definition}

There are two ways we can use this definition.

\begin{itemize}
\tightlist
\item
  If we know \(\mathbb P(A)\), \(\mathbb P(B)\), and \(\mathbb P(A \cap B)\), then we can find out whether or not \(A\) and \(B\) are independent by checking whether or not \(\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)\).
\item
  If we know \(\mathbb P(A)\) and \(\mathbb P(B)\) and we know that \(A\) and \(B\) are independent, then we can find \(\mathbb P(A \cap B)\) by calculating \(\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)\).
\end{itemize}

In this second case, we might know \(A\) and \(B\) are independent because we are specifically told they are. But we might reason that \(A\) and \(B\) because the related experiments are not physically related -- for example if we roll a dice then toss a coin, we might reason that \(\{\text{roll a 5}\}\) and \(\{\text{the coin lands Heads}\}\) must be independent because the dice roll doesn't effect the coin toss, and use the independence assumption in calculations.

\begin{example}
\emph{Consider rolling a dice. Let \(A = \{\text{even number}\} = \{2,4,6\}\), and let \(B = \{\text{roll at least 4}\} = \{4,5,6\}\). Are \(A\) and \(B\) independent?}

Clearly we have \(\mathbb P(A) = \frac36 = \frac12\) and \(\mathbb P(B) = \frac 36 = \frac12\). The intersection is \(A \cap B = \{4,6\}\), so \(\mathbb P(A \cap B) = \frac26 = \frac13\). So we see that
\[ \mathbb P(A\cap B) = \frac13  \qquad \text{and} \qquad  \mathbb P(A)\, \mathbb P(B) = \frac12 \times \frac12 = \frac14 . \]
So \(\mathbb P(A \cap B) \neq \mathbb P(A)\, \mathbb P(B)\), and the two events are not independent.
\end{example}

\begin{example}
\emph{A biased coin has probability \(p\) of landing Heads and probability \(1-p\) of landing Tails. You toss the coin 3 times. Assuming tosses of the coin are independent, calculate the probability of getting exactly 2 Heads.}

There are three ways we could get exactly 2 Heads: HHT, HTH, or THH. For the first of these,
\[ \mathbb P(\text{HHT}) = \mathbb P(\text{first coin H} \cap \text{second coin H} \cap \text{third coin T}) . \]
Since tosses of the coin are independent, we therefore have
\begin{align*}
\mathbb P(\text{HHT})
  &= \mathbb P(\text{first coin H}) \times \mathbb P ( \text{second coin H} )\times \mathbb P(\text{third coin T}) \\
  &=p \times p \times (1-p) \\
  &= p^2(1-p).
\end{align*}

Similarly,
\[ \mathbb P(\text{HTH}) = \mathbb P(\text{THH}) = p^2(1-p) \]
also.

Finally, because the events are disjoint, we have
\[ \mathbb P(\text{HHT} \cup\text{HTH} \cup \text{THH}) = \mathbb P(\text{HHT} ) + \mathbb P(\text{HTH}) + \mathbb P(\text{THH}) = 3p^2(1-p) . \]
\end{example}

\hypertarget{conditional}{%
\section{Conditional probability}\label{conditional}}

Let us to return to the example of blond hair and blue eyes. Suppose the population statistics are like this:

\begin{longtable}[]{@{}cccc@{}}
\toprule
& \textbf{Brown hair} & \textbf{Blond hair} & \textbf{Total} \\
\midrule
\endhead
\textbf{Brown eyes} & 50\% & 30\% & 80\% \\
\textbf{Blue eyes} & 10\% & 10\% & 20\% \\
\textbf{Total} & 60\% & 40\% & 100\% \\
\bottomrule
\end{longtable}

(It turns out that \(\mathbb P(\text{blond hair and blue eyes}) = 0.1 \neq 0.08\), so they are not independent.)

We know that 20\% of people have blue eyes. But suppose you already know that someone has blond hair: what then is their probability of have blue eyes \emph{given} that they have blond hair?

Well, the 40\% of blond-haired people is made up of the 10\% of people who also have blue eyes to go along with their blond hair, and the 30\% of people who have brown eyes to go along with their blond hair. So of the 40\% of blond-haired people, three times as many have brown eyes, so only one quarter of that 40\% have blue eyes. If we use a vertical line \(|\) in a probability to mean ``given'' (or ``assuming that'' or ``conditional upon''), then we can write this as
\[  \mathbb P(\text{blue eyes} \mid \text{blond hair}) = \frac{\mathbb P(\text{blue eyes and blond hair})}{\mathbb P(\text{blond hair})} = \frac{0.1}{0.4} = \frac14. \]

What we've seen here is called a ``conditional probability''.

\begin{definition}
Let \(A\) and \(B\) be events, with \(\mathbb P(A) > 0\). Then the \textbf{conditional probability of \(B\) given \(A\)} is defined to be
\[  \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} . \]
\end{definition}

The condition \(\mathbb P(A) > 0\) is to ensure we don't have any ``divide by 0'' errors. (I normally won't bother saying this explicitly -- any statement about conditional probability will implicitly assume that the event being conditioned on has nonzero probability.)

As with independence, conditional probability can be used in different ways: given any two of \(\mathbb P(A)\), \(\mathbb P(A \cap B)\), and \(\mathbb P(B \mid A)\) you can work out the other one.

Conditional probability ties in with independence in an important way. Suppose \(A\) and \(B\) are independent, so \(\mathbb P(A \cap B) = \mathbb P(A) \, \mathbb P(B)\). Then the conditional probability becomes
\[ \mathbb P(B \mid A) = \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} =  \frac{\mathbb P(A) \, \mathbb P(B)}{\mathbb P(A)} = \mathbb P(B) , \]
so \(\mathbb P(B \mid A) = \mathbb P(B)\). In other words, if \(A\) and \(B\) are independent, then \(A\) happening doesn't affect the probability of \(B\) happening (and vice versa).

So when we have independence, \(\mathbb P(A \cap B) = \mathbb P(A)\,\mathbb P(B)\), and the mathematics is quite easy. But conditional probability tells us how things work when we don't have independence.

\hypertarget{chain-rule}{%
\section{Chain rule}\label{chain-rule}}

We can rewrite the definition of conditional probability like this:
\[ \mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B \mid A). \]
This can be a useful way to think when \(A\) concerns the first stage of an experiment and \(B\) the second stage. This says that the probability \(A\) happens then \(B\) happens is equal to the probability \(A\) happens multiplied the probability, given that \(A\) has happened, that \(B\) then happens too.

We can extend this to more events. For three events, we have
\begin{align*}
\mathbb P(A \cap B \cap C)
  &= \mathbb P(A \cap B) \, \mathbb P(C \mid A \cap B) \\
  &= \mathbb P(A) \, \mathbb P(B \mid A)\, \mathbb P(C \mid A \cap B) ,
\end{align*}
which can be useful when we have three stages of an experiment.

Continuing that process, we get a general rule.

\begin{theorem}[Chain rule]
\protect\hypertarget{thm:thchain}{}\label{thm:thchain}For events \(A_1, A_2, \dots, A_n\), we have
\begin{multline*}  \mathbb P(A_1 \cap A_2 \cap \cdots \cap A_n) \\
  = \mathbb P(A_1) \, \mathbb P(A_2 \mid A_1) \, \mathbb P(A_3 \mid A_1 \cap A_2) \cdots \mathbb P(A_n \mid A_1 \cap A_2 \cap \cdots \cap  A_{n-1}) .\end{multline*}
\end{theorem}

Often questions that can be solved using the classical probability counting methods from Section 3 also be solved in stages using the chain rule. (It's a matter of personal taste which you prefer.)

\begin{example}
\emph{Recall the Lotto problem from Example \ref{exm:lotto}: What is the probability we match 6 balls from 59?}

Let \(A_1, A_2, \dots, A_6\) be the events that the first, second, \ldots, sixth balls out of the machine are on our ticket. Clearly \(\mathbb P(A_1) = \frac{6}{59}\), as we have six numbers the ball could match. Then the conditional probability that the second ball matches given that the first ball matched is \(\mathbb P(A_2 \mid A_1) = \frac{5}{58}\), because there are 58 balls left in the machine and, given that we got the first number right, there are 5 numbers left on our ticket. Similarly, \(\mathbb P(A_3 \mid A_1 \cap A_2) = \frac{4}{57}\), and so on, down to \(\mathbb P(A_6 \mid A_1 \cap \cdots\cap A_5) = \frac{1}{54}\).

So, using the chain rule, we get
\begin{align*}
\mathbb P(A_1 \cap A_2 &\cap \cdots \cap A_6) \\
&= \mathbb P(A_1) \, \mathbb P(A_2 \mid A_1) \, \mathbb P(A_3 \mid A_1 \cap A_2) \cdots \mathbb P(A_6 \mid A_1 \cap \cdots \cap A_5) \\
&= \frac{6}{59} \times \frac{5}{58} \times \frac{4}{57} \times \frac{3}{56} \times \frac{2}{55} \times \frac{1}{54} .
\end{align*}

The answer we got before was
\[ \frac{6 \times 5 \times 4 \times 3 \times 2 \times 1}{59 \times 58 \times 57 \times 56 \times 55 \times 54} . \]
It's easy to see that this is the same answer, and the structure of the answers shows how the old method got the answer ``all at once'', while this new method gets the answer ``one stage at a time''.
\end{example}

\hypertarget{total-prob}{%
\section{Law of total probability}\label{total-prob}}

\begin{example}
\emph{My friend has three dice: a 4-sided dice, a 6-side dice, and a 10-side dice. He picks one of them at random, with each dice equally likely. What is the probability my friend rolls a 5?}

If my friend were to tell which dice he picked, then this question would be very easy! If we write \(D_4\), \(D_6\) and \(D_{10}\) to be the events that he picks the 4-sided, 6-sided, or 10-sided dice, then we know immediately that
\[ \mathbb P(\text{roll 4} \mid D_4) = 0 \qquad \mathbb P(\text{roll 4} \mid D_6) = \tfrac16 \qquad \mathbb P(\text{roll 4} \mid D_{10}) = \tfrac{1}{10} .  \]
What we need is a way to combine the results for different ``sub-cases'' into an over-all answer.
\end{example}

Luckily, there exists just such a tool for this job! It's called the ``law of total probability'' (also known as the ``partition theorem''). The important point is to make sure that the different sub-cases cover all possibilities, but that only one of them happens at a time.

\begin{definition}

A set of events \(A_1, A_2, \dots, A_n\) are said to be a \textbf{partition} of the sample space \(\Omega\) if

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  they are disjoint, in that \(A_i \cap A_j = \varnothing\) for all \(i \neq j\);
\item
  they cover space, in that \(A_1 \cup A_2 \cup \cdots \cup A_n = \Omega\).
\end{enumerate}

\end{definition}

\begin{theorem}[Law of total probability]
\protect\hypertarget{thm:thlawtotal}{}\label{thm:thlawtotal}Let \(A_1, A_2, \dots, A_n\) be a partition, and \(B\) another event. Then
\[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
\end{theorem}

So the law of total probability tells us we can add up the probabilities \(\mathbb P(B \mid A_i)\) for each of the sub-cases provided we weight them by how likely \(\mathbb P(A_i)\) by how likely each sub-case is.

\begin{proof}
Since the partition of \(A_i\)s cover space, we can split up \(B\) depending on which part of the partition it is in:
\[  B = (B \cap A_1) \cup (B \cap A_2) \cup \cdots \cup (B \cap A_n) .  \]

\emph{{[}I meant to draw a picture here, but didn't get round to it -- perhaps you'd like to draw your own?{]}}

Since the \(A_i\) are disjoint, the union on the right is disjoint also.
Therefore we can use Axiom 3 to get
\[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(B \cap A_i) . \]
But using the definition of conditional probability, each ``summand'' (term inside the sum) is
\[ \mathbb P(B \cap A_i) = \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
The result follows.
\end{proof}

Returning to our dice example, \(D_4, D_6, D_{10}\) is indeed a partition, since these are the only possibilities and we only choose one dice. So the law of total probability tells us that
\[ \mathbb P(\text{roll 5}) = \mathbb P(D_4) \, \mathbb P(\text{roll 5} \mid D_4) +  \mathbb P(D_6) \, \mathbb P(\text{roll 5} \mid D_6) + \mathbb P(D_{10}) \, \mathbb P(\text{roll 5} \mid D_{10}) . \]

We were told that all the dice were picked with equal probability, so \(\mathbb P(D_4) = \mathbb P(D_6) = \mathbb P(D_{10}) = \frac13\), and we calculated the individual conditional probabilities as
\[ \mathbb P(\text{roll 4} \mid D_4) = 0 \qquad \mathbb P(\text{roll 4} \mid D_6) = \tfrac16 \qquad \mathbb P(\text{roll 4} \mid D_{10}) = \tfrac{1}{10} .  \]

Therefore, we have
\[ \mathbb P(\text{roll 5}) = \tfrac13\times 0 +  \tfrac13\times\tfrac16 +  \tfrac13\times\tfrac1{10} = \tfrac{8}{90} = 0.089. \]

\hypertarget{bayes}{%
\section{Bayes' theorem}\label{bayes}}

In this subsection, we will discuss an important result called \textbf{Bayes' theorem}.
Let's first state and prove this result, and do an example, and then afterwards we'll talk about two reasons why Bayes' theorem is so important.

\begin{theorem}[Bayes' theorem]
\protect\hypertarget{thm:thbayes}{}\label{thm:thbayes}For events \(A\) and \(B\) with \(\mathbb P(A), \mathbb P(B) > 0\), we have
\[ \mathbb P(A \mid B) = \frac{\mathbb P(A) \,\mathbb P(B \mid A)}{\mathbb P(B)} .  \]
\end{theorem}

Bayes' theorem is thought to have first appeared in the writings of Rev.~\href{https://mathshistory.st-andrews.ac.uk/Biographies/Bayes/}{Thomas Bayes}, a British church minister and mathematician, shortly after his death, in the 1760s. However, his work was significantly edited by \href{https://mathshistory.st-andrews.ac.uk/Biographies/Price/}{Richard Price}, another minister--mathematician, and many people think that Price deserves a large share of the credit.

\begin{proof}
From the definition of conditional probability, we can write \(\mathbb P(A \cap B)\) in two different ways: we can write it as
\[  \mathbb P(A \cap B) = \mathbb P(A) \, \mathbb P(B\mid A) , \]
but we can also write it as
\[  \mathbb P(A \cap B) = \mathbb P(B) \, \mathbb P(A\mid B) . \]
Since these are two different ways of writing the same thing, we can equate them, to get
\[ \mathbb P(A) \, \mathbb P(B\mid A) = \mathbb P(B) \, \mathbb P(A\mid B) . \]
Dividing both sides by \(\mathbb P(B)\) gives the result.
\end{proof}

\begin{example}
\emph{My friend again secretly picks the 4-sided, 6-sided, or 10-sided dice, each with probability \(\frac13\). He rolls that secret dice, and tells me he rolled a 5. What is the probability he picked the 6-sided dice?}

This is asking us to calculate \(\mathbb P(D_6 \mid \text{roll 5})\). Bayes' theorem tells us that
\[
  \mathbb P(D_6 \mid \text{roll 5})
  = \frac{\mathbb P(D_6) \, \mathbb P(\text{roll 5} \mid D_6)}{\mathbb P(\text{roll 5})} 
  = \frac{\frac13 \times \frac16}{\frac{8}{90}} 
  = \tfrac{5}{8} ,
\]
since we had calculated \(\mathbb P(\text{roll 5}) = \frac{8}{90}\) in the previous subsection.
\end{example}

The first way to think about Bayes' theorem is that it tells us how to relate \(\mathbb P(A \mid B)\) and \(\mathbb P(B \mid A)\). Remember that \(\mathbb P(A \mid B)\) and \(\mathbb P(B \mid A)\) are not the same thing! The conditional probability someone is under 40 given they are a Premiership footballer is very high, but the conditional probability someone is a Premiership footballer given they are under 40 is very low.

Bayes' theorem, in this first view, is a useful technical result that helps us switch the order of a conditional probability from \(B\) given \(A\) to \(A\) given \(B\): we have
\[ \mathbb P(A \mid B) = \frac{\mathbb P(A)}{\mathbb P(B)} \times \mathbb P(B \mid A) .  \]

In the dice example, the probability \(\mathbb P(\text{roll 5} \mid D_6) = \frac16\) was very obvious, but Bayes' theorem allowed us to reverse the conditioning, to find \(\mathbb P(D_6 \mid \text{roll 5}) = \frac58\) instead.

The second way to think about Bayes' rule is that it tells us how to update our beliefs as we acquire more evidence. That is, we might start by believing that the probability some event \(A\) will occur is \(\mathbb P(A)\). But then we find out that \(B\) has occurred, so we want to incorporate that knowledge and update our belief of the probability \(A\) will occur to \(\mathbb P(A \mid B)\), the conditional probability \(A\) will occur given this new evidence \(B\).

Bayes theorem, in this second view, tells us how to update from \(\mathbb P(A)\) to \(\mathbb P(A \mid B)\): we have
\[ \mathbb P(A \mid B) = \mathbb P(A) \times \frac{\mathbb P(B \mid A)}{\mathbb P(B)} .  \]

In the dice example, we initially believed there was a \(\mathbb P(D_6) = \frac13 = 0.333\) chance our friend had chosen the six-sided dice. But when we heard that our friend had rolled a 5, we updated our belief to now thinking there was now a \(\mathbb P(D_6 \mid \text{roll 5}) =\frac58 = 0.625\) chance it was the 6-sided dice.

This second way of thinking about Bayes' theorem is at the heart of \textbf{Bayesian statistics}. In Bayesian statistics, we start with a ``prior'' belief about a model, then, after collecting some data, we update to a ``posterior'' belief, according to the rules of Bayes' theorem. We will discuss Bayesian statistics much more in Section 10.

Quite often we use Bayes' theorem and the law of total probability together. If we have a partition \(A_1, A_2, \dots, A_n\), perhaps representing some possible hypotheses, and we observe an event \(B\), then Bayes' theorem tells us how likely each hypothesis is given the observation:
\[ \mathbb P(A_i \mid B) = \frac{\mathbb P(A_i) \,\mathbb P(B \mid A_i)}{\mathbb P(B)} .  \]
But this shared denominator \(\mathbb P(B)\) can be expanded using the law of total probability
\[ \mathbb P(B) = \sum_{j=1}^n \mathbb P(A_j) \,\mathbb P(B \mid A_j) . \]
Together, we get the following.

\begin{theorem}
\protect\hypertarget{thm:bayes-total}{}\label{thm:bayes-total}Let \(\{A_1, A_2, \dots, A_n\}\) be a partition of a sample space and let \(B\) be another event. Then, for all \(i=1,2,\dots,n\), we have
\[ \mathbb P(A_i \mid B) = \frac{\mathbb P(A_i) \,\mathbb P(B \mid A_i)}{\sum_{j=1}^n \mathbb P(A_j) \, \mathbb P(B \mid A_j)} .  \]
\end{theorem}

This is essentially what we did with the dice example, although we split it up into two separate parts rather than using this formula directly.

\hypertarget{screening}{%
\section{Diagnostic testing}\label{screening}}

\emph{Members of the public are tested for a certain disease. About 2\% of the population have the disease. The test is 95\% accurate, in the following sense: if you have the disease, there's a 95\% chance you correctly get a positive test result, while if you don't have the disease, there's a 95\% chance you correctly get a negative test result. Suppose you get a positive test result. What is the probability you have the disease?}

The first thing we have to do is translate the words in the question into probability statements. Let \(D\) be the event you have the disease, so \(D^\mathsf{c}\) is the event you don't have the disease, and let \(+\) be the event you get a positive result. Then the question tells us that

\begin{itemize}
\tightlist
\item
  \(\mathbb P(D) = 0.02\) and \(\mathbb P(D^\mathsf{c}) = 0.98\);
\item
  \(\mathbb P({+} \mid D) = 0.95\) and \(\mathbb P({+}\mid D^\mathsf{c}) = 0.05\);
\item
  we want to find \(\mathbb P(D \mid {+})\).
\end{itemize}

Note also that \(D\) (you have the disease) and \(D^\mathsf{c}\) (you don't) make up a partition. Then Theorem \ref{thm:bayes-total} tells us that
\[  \mathbb P(D \mid {+}) = \frac{\mathbb P(D) \,\mathbb P({+} \mid D)}{\mathbb P(D) \,\mathbb P({+} \mid D)+\mathbb P(D^\mathsf{c}) \,\mathbb P({+} \mid D^\mathsf{c})} . \]
Putting in all the numbers we have, we get
\[ \mathbb P(D \mid {+}) = \frac{0.02 \times 0.95}{0.02 \times 0.95 + 0.98 \times 0.05} = 0.28 .\]

So if you get a positive result on this 95\%-accurate test, there's still only about a 1 in 4 chance you actually have the disease.

Many people find this result surprising. It sometimes helps to put more concrete numbers on things. Suppose 1000 people get tested. On average, we expect about 20 of them to have the disease, and 980 of to not have the disease. Of the 20 with the disease, on average 19 will correctly test positive, while 1 will test negative. Of the 980 without the disease, an average 931 will correctly test negative, but 49 will wrongly test positive. So of the \(19+49 = 68\) people with positive tests, only 19 of them actually have the disease, which is 28\%.

The key point is that the disease is rare -- only 2\% of people have it. So even though positive test increases the likelihood you have the disease a lot (it's about 14 times more likely), it's not enough to make it a very large probability.

\hypertarget{summary-034}{%
\section*{Summary}\label{summary-034}}
\addcontentsline{toc}{section}{Summary}

\begin{itemize}
\tightlist
\item
  Two events are independent if \(\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)\).
\item
  The conditional probability of \(B\) given \(A\) is \({\displaystyle \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)}}\).
\item
  The law of total probability says that if \(A_1, A_2, \dots A_n\) is a partition of the sample space, then
  \[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
\item
  Bayes' theorem says that \({\displaystyle \mathbb P(A \mid B) = \frac{\mathbb P(A) \,\mathbb P(B \mid A)}{\mathbb P(B)} }\).
\end{itemize}

\hypertarget{S05-discrete-rv}{%
\chapter{Discrete random variables}\label{S05-discrete-rv}}

\hypertarget{rv}{%
\section{What is a random variable?}\label{rv}}

Let's consider again the case of rolling two dice. We know that the sample space is the set of pairs of numbers between 1 and 6
\[  \Omega = \big\{ \boldsymbol\omega = (\omega_1, \omega_2) : \omega_1, \omega_2 \in \{1,2,3,4,5,6\} \big\} , \]
which is equipped with the classical probability measure
\[ \mathbb P(A) = \frac{|A|}{36} . \]

But if we are rolling the two dice as part of a board game, we might only care about the total score on the two dice, rather than the two individual dice scores. In this case, we could write \(X\) for the score on the first dice plus the score on the second dice -- that is,
\[ X(\boldsymbol\omega) = \omega_1 + \omega_2 . \]

If we want to know the probability we roll a total of 7, say, then we could write this as \(\mathbb P(A)\) where
\[ A = \{ \boldsymbol\omega : \omega_1 + \omega_2 = 7\}  \]
is the set of dice rolls with total 7.
But it might just be easier to write this as \(\mathbb P(X = 7)\). We might also be interested in other things about the total score \(X\), like what the average total score is.

Here, \(X\) is an example of what we call a \textbf{random variable}. A random variable can be thought of as a numerical summary of an experiment (like the total summarising the two individual dice rolls). But by considering the random variable directly, it often means we don't have to worry so much about exactly what the sample space is, or what probability measure is being used, or which sample outcomes are in a particular event. This often makes our life easier when thinking about probability problems.

Random variables are typically given capital letters from late in the alphabet, like \(X\), \(Y\), \(Z\). Values that those random variables take are often given lower-case letters, like \(x\), \(y\), \(z\).

The formal definition of a random variable is as a function that turns the outcome into this numerical summary.

\begin{definition}
Let \(\Omega\) be a sample space. Then a \textbf{random variable} is a function \(X\) from \(\Omega\) to the real numbers \(\mathbb R\); that is, to each sample outcome \(\omega\) it assigns a real number \(X(\omega)\).

Expressions like \(\mathbb P(X = x)\) should be understood as representing more formal probability
\[ \mathbb P \big( \{\omega : X(\omega) = x \}\big) . \]
\end{definition}

This formal definition of a random variable as a function was summarised by my own first-year probability lecturer as ``There's only two things you need to know about the definition of a random variable: first, it's not random; second, it's not a variable.''

However, the way we actually \emph{think} about random variables \emph{is} as random and is as variables. In this more informal way of thinking, a random variable is a variable that can take different values with different probabilities -- just as the total of the two dice can be 2 with probability \(\frac{1}{36}\), or the value 3 with probability \(\frac{2}{36}\), and so on.

\newcommand{\Range}{\operatorname{Range}}

\begin{definition}
The set of values a random variable \(X\) can take is called its \textbf{range}, \(\operatorname{Range}(X) = \{X(\omega) : \omega \in \Omega \}\).
\end{definition}

So, for example, the range of the dice sum \(X\) is \(\operatorname{Range}(X) = \{2, 3, \dots, 12\}\).

Random variables that we will consider in this module will be one of two types:

\begin{itemize}
\tightlist
\item
  \textbf{Discrete random variables} have a range that is finite (like the dice total being an integer between 2 and 12) or countably infinite (like the positive integers, for example). Discrete random variables can be used as models for ``count data''.
\item
  \textbf{Continuous random variables} have a range that is uncountably infinite (like the real numbers, the positive real numbers, or the interval \([0,1]\), for example). Continuous random variables can be used as models for ``measurement data''.
\end{itemize}

In this section and the next two, we will look at discrete random variables; later in Section 8 and 9 we will look at continuous random variables.

\hypertarget{pmf}{%
\section{Probability mass functions}\label{pmf}}

We now consider only discrete random variables \(X\), where the range \(\operatorname{Range}(X)\) is a finite or countably infinite set. In this case, Axiom 3 tells us that for any set \(A\), we have
\[ \mathbb P(X \in A) = \sum_{x \in A} \mathbb P(X = x) . \]
(Recall that the symbol \(\in\) means ``is an element of'', or just ``is in'' for short.)
So to fully understand a discrete random variable \(X\), we need only understand the probabilities \(\mathbb P(X = x)\). These are captured by the probability mass function.

\begin{definition}
For a discrete random variable \(X\), its \textbf{probability mass function} (or \textbf{PMF}) is the function \(p_X\) where
\[ p_X(x) = \mathbb P(X = x)  \qquad \text{for $x \in \operatorname{Range}(X)$.} \]
(When the random variable is obvious from context, we'll just write \(p(x)\) without the subscript.)
\end{definition}

The key is that, once we are thinking in terms of a random variable via its PMF, we can (usually) stop worrying to much about what the underlying sample space is and how the random variable acts on that space.

\begin{example}
Let \(X\) being the sum of two dice rolls. As this is a classical probability problem, the probability \(p(x)\) of rolling a total of \(x\) is \(n(x) / 36\), where \(n(x)\) is the number of ways of rolling a total of \(x\). So, for example, there is only one way \((1,1)\) of rolling a total of 2, so \(p(2) = \frac1{36}\), but there are 5 ways of rolling a 6: \((1,5), (2,4), (3, 3), (4, 2), (5, 1)\); so \(p(5) = \frac5{36}\).

The PMF \(p\) of \(X\) is given by

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}@{}}
\toprule
\(x\) & \(2\) & \(3\) & \(4\) & \(5\) & \(6\) & \(7\) \\
\midrule
\endhead
\(p(x)\) & \(\frac{1}{36}\) & \(\frac{2}{36}\) & \(\frac{3}{36}\) & \(\frac{4}{36}\) & \(\frac{5}{36}\) & \(\frac{6}{36}\) \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
\(x\) & \(\cdots\) & \(8\) & \(9\) & \(10\) & \(11\) & \(12\) \\
\midrule
\endhead
\(p(x)\) & \(\cdots\) & \(\frac{5}{36}\) & \(\frac{4}{36}\) & \(\frac{3}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) \\
\bottomrule
\end{longtable}

\includegraphics{math1710_files/figure-latex/dice-pmf-1.pdf}
\end{example}

\begin{example}
Consider tossing a biased coin, that is Heads with probability \(p\) and Tails with probability \(1-p\). Let \(X = 1\) if the coin lands Heads, and \(X = 0\) if the coin lands Tails. The PMF \(p_X\) of this random variable is given by
\[ p_X(0) = 1 - p \qquad p_X(1) = p . \]

We could alternatively think of the same random variable as representing the result of an experiment, where \(X = 1\) represents a success, with probability \(p_X(1) = p\), and \(X = 0\) represents a failure, with probability \(p_X(0) = 1 - p\).

A random variable \(X\) with this PMF is called a \textbf{Bernoulli trial} (or a ``Bernoulli random variable'', or is said to ``follow the Bernoulli distribution'' -- after the seventeenth-century Swiss mathematician \href{https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/}{Jacob Bernoulli}). We use the notation \(X \sim \text{Bern}(p)\) for short.
\end{example}

Note that since \(p(x) = \mathbb P(X = x)\) is a probability, we must have \(p(x) \geq 0\) for all \(x \in \operatorname{Range}(X)\). Further, we have
\[ 1 = \mathbb P(\Omega) = \mathbb P\big(X \in \operatorname{Range}(X)\big) = \sum_{x \in \operatorname{Range}(X)} \mathbb P(X = x) = \sum_{x \in \operatorname{Range}(X)} p(x) . \]
Hence we have the following:

\begin{theorem}

Let \(X\) be a discrete random variable, and let \(p_X\) be its PMF. Then

\begin{itemize}
\tightlist
\item
  \(p_X(x) \geq 0\) for all \(x\);
\item
  \({\displaystyle \sum_x p_X(x) = 1}\).
\end{itemize}

\end{theorem}

Sometimes it is useful to know the probability a random variable \(X\) is less than some value \(x\). This is captured by the \textbf{cumulative distribution function} (or \textbf{CDF}) \(F_X\), where
\[ F_X(x) = \mathbb P(X \leq x) = \sum_{y \leq x} p_X(y) \qquad \text{for $x \in \mathbb R$.}  \]

\begin{example}
Let \(X \sim \text{Bern}(p)\) be a Bernoulli random variable with success probability \(p\). Then its CDF \(F\) is
\[ F(x) = \begin{cases} 0 & \text{for $x < 0$} \\
                      1-p & \text{for $0 \leq x < 1$} \\
                      1   & \text{for $x \geq 1$} . \end{cases} \]
\end{example}

\begin{example}
If \(X\) is the sum of two dice rolls, then the CDF \(F\) is given by

\begin{longtable}[]{@{}cccccccc@{}}
\toprule
\(x \in {}\) & \((-\infty, 2)\) & \([2,3)\) & \([3,4)\) & \([4,5)\) & \(\cdots\) & \([11,12)\) & \([12, \infty)\) \\
\midrule
\endhead
\(F(x)\) & \(0\) & \(\frac{1}{36}\) & \(\frac{3}{36}\) & \(\frac{5}{36}\) & \(\cdots\) & \(\frac{35}{36}\) & \(1\) \\
\bottomrule
\end{longtable}

\includegraphics{math1710_files/figure-latex/dice-cdf-1.pdf}

Note that the CDF is a ``step function'' that starts at 0, then jumps up suddenly at each of the values \(2, 3, \dots, 12\), ending up at 1.
\end{example}

For any random variable \(X\) with CDF \(F\),

\begin{itemize}
\tightlist
\item
  if \(x\) is smaller than everything in the range of \(X\), then \(F(x) = 0\);
\item
  if \(x\) is greater than everything in the range of \(X\), then \(F(x) = 1\);
\item
  \(F(x)\) is increasing in \(x\).
\end{itemize}

\hypertarget{expectation}{%
\section{Expectation}\label{expectation}}

Often, we will be interested in the ``average'' value of a random variable -- for example, the average total from two dice rolls -- which represents what ``central'' values of the random variable. This average is called the ``expectation''.

\begin{definition}
Let \(\Omega\) be a finite or countably infinite sample space, \(\mathbb P\) be a probability measure on \(\Omega\), and \(X\) be a discrete random variable on \(\Omega\). Then the \textbf{expectation} (or \textbf{expected value}) of \(X\) is
\[ \mathbb EX = \sum_{\omega \in \Omega} X(\omega) \mathbb P(\{\omega\}) . \]
If \(p\) is the PMF of \(X\), then a more convenient formula is
\[ \mathbb EX = \sum_{x \in \operatorname{Range}(X)} x\,p_X(x) . \]
\end{definition}

We get the second formula from the first by grouping together all outcomes \(\omega\) that lead to the same value \(x = X(\omega)\) of \(X\).

Note that ``expectation'' is simply the name that mathematicians give to the value \(\mathbb EX = \sum_x x\, p(x)\). We don't necessarily ``expect'' to get the value \(\mathbb EX\) as the outcome in the normal English-language sense of the word ``expect''. (Indeed, you might like to check that the expectation of a single dice roll is 3.5, but you certainly don't ``expect'' to get the number 3.5 in a single roll of the dice!) We will see later that the the expectation can be interpreted as a sort of ``long-run mean outcome''.

\begin{example}
\emph{Let \(X \sim \text{Bern}(p)\) be a Bernoulli trial with success probability \(p\). What is the expectation \(\mathbb EX\)?}

Using the second formula in the definition, we have
\[ \mathbb EX = \sum_{x} x\,p(x) = 0\times (1-p) + 1\times p = p. \]
\end{example}

\begin{example}
When \(X\) is the total of two dice rolls, the expectation is
\begin{align*}
  \mathbb EX &= \sum_{x \in \operatorname{Range}(X)} x\,p(x)  \\
    &= 2 \times \tfrac{1}{36} + 3 \times \tfrac{2}{36} + \cdots + 12 \times \tfrac{1}{36} \\
    &= \tfrac{252}{36} \\
    &= 7 .
\end{align*}
\end{example}

\hypertarget{functions}{%
\section{Functions of random variables}\label{functions}}

In previous examples, we looked at \(X\) being the total of the dice rolls. But we could equally well chosen to have looked at a different random variable that is a function of that total \(X\), like ``double the total and add 1'' \(Y = 2X + 1\), or ``the total minus 4, all squared'' \(Z = (X-4)^2\). (I'm not sure \emph{why} you'd care about these, but you could study them if you wanted to\ldots)

\begin{example}

Let \(Y = 2X + 1\). Then for each potential outcome \(x\) of \(X\), there is a matching outcome \(y = 2x +1 1\) of \(Y\). So we can find the PMF for \(Y\) by keeping the same probabilities as for \(X\), but changing the values \(x\) to the values \(y = 2x +11\).

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
\(y\) & \(5\) & \(7\) & \(9\) & \(\cdots\) & \(23\) & \(25\) \\
\midrule
\endhead
\(p_Y(y)\) & \(\frac{1}{36}\) & \(\frac{2}{36}\) & \(\frac{3}{36}\) & \(\cdots\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) \\
\bottomrule
\end{longtable}

\end{example}

\begin{example}

What about \(Z = (X - 4)^2\)? This is a bit trickier, because more than one outcome \(x\) can lead to the same outcome \(z = (x - 4)^2\): for example, \((2 - 4)^2 = (6 - 4)^2 = 4\). So, more precisely, we have
\[ \mathbb P(Z = z) = \mathbb P\big(X \in \{x : (x - 4)^2 = z\}\big) , \]
or, in terms of PMFs,
\[   p_Z(z) = \sum_{x : (x - 4)^2 = z} p_X(x) . \]

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}@{}}
\toprule
\(z\) & \(0\) & \(1\) & \(4\) & \(\cdots\) & \(49\) & \(64\) \\
\midrule
\endhead
\(p_Z(z)\) & \(\frac{3}{36}\) & \(\frac{2}{36} + \frac{4}{36} = \frac{6}{36}\) & \(\frac{1}{36} + \frac{5}{36} = \frac{6}{36}\) & \(\cdots\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) \\
\bottomrule
\end{longtable}

\end{example}

So if we wanted to find the expectation of a function of a random variable, we could first find the PMF, like in the above examples, and then use that PMF to find the expectation. But there is a quicker way.

\begin{theorem}[Law of the unconscious statistician]
\protect\hypertarget{thm:unconscious}{}\label{thm:unconscious}Let \(X\) be a random variable, and let \(Y = g(X)\) be another random variable that is a function of \(X\). Then
\[  \mathbb EY = \mathbb Eg(X) = \sum_{x} g(x) \, p_X(x) . \]
\end{theorem}

(The rather cruel name of this theorem is, I think, because this is the formula you might carelessly write down for \(\mathbb Eg(X)\) if you weren't thinking carefully -- but it turns out it's correct!)

\begin{proof}
As in the previous example, the idea is to group together \(x\)s that give the same \(y\).

From the definition of expectation, we have
\[ \mathbb EY = \sum_y y\, p_Y(y) . \]
Then using
\[ p_Y(y) = \sum_{x : g(x) = y} p_X(x) , \]
we get
\begin{align*}
  \mathbb EY &= \sum_y y \sum_{x : g(x) = y} p_X(x) \\
    &= \sum_y \sum_{x : g(x) = y} y\,p_X(x) \\
    &= \sum_y \sum_{x : g(x) = y} g(x) \, p_X(x) ,
\end{align*}
since \(y = g(x)\) inside the second sum. But those two sums together are summing over all \(x\), just partitioned by which value of \(y\) they lead to, so can be replaced by a single sum over \(x\). That gives the theorem.
\end{proof}

There are some functions for which this expression becomes particularly simple.

\begin{theorem}[Linearity of expectation, 1]
\protect\hypertarget{thm:linearity1}{}\label{thm:linearity1}

Let \(X\) be a random variable. Then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbb E(aX) = a\mathbb EX\);
\item
  \(\mathbb E(X + b) = \mathbb EX + b\).
\end{enumerate}

\end{theorem}

\begin{proof}
We use the law of the unconscious statistician.

For part 1, we can take the \(a\) outside the sum, to get
\[ \mathbb E(aX) = \sum_x ax\, p_X(x) = a\sum_x x\, p_X(x) = a\mathbb EX . \]

For part 2, we have
\begin{align*}
  \mathbb E(X+b) &= \sum_x (x + b)\, p_X(x) \\
    &= \sum_x \big( x\, p_X(x) + b\,p_X(x) \big) \\
    &= \sum_x x\, p_X(x) + \sum_x b\,p_X(x) \\
    &= \mathbb E(X) + b \sum_x p_X(x) \\
    &= \mathbb E(X) + b .
\end{align*}
The last line was because PMFs always add up to 1, so \(\sum_x p_X(x) = 1\).
\end{proof}

So for our ``double the dice total and add 1'' random variable \(Y = 2X + 1\), we have
\[ \mathbb EY = \mathbb E(2X+1) = 2\mathbb EX + 1 = 2\times 7 + 1 = 15. \]

\hypertarget{variance}{%
\section{Variance}\label{variance}}

\newcommand{\Var}{\operatorname{Var}}

In the same way as the expectation of a random variable tells us about central typical values of it, the variance of a random variable tells us about the spread of typical values.

\begin{definition}
Let \(X\) be a random variable with expectation \(\mathbb EX = \mu\). Then the \textbf{variance} of \(X\) is
\[ \operatorname{Var}(X) = \mathbb E(X - \mu)^2 . \]
\end{definition}

(To be clear, the notation there means the expectation of \((X-\mu)^2\); and \emph{not} \(\mathbb E(X - \mu)\) squared, which would be \(0^2 = 0\).)

Note that \((X - \mu)^2\) is a square, so always non-negative, and hence the variance is always non-negative also.

It may not surprise you, if you remember \protect\hyperlink{S01-eda}{Section 1} that to go along with that definitional formula for the variance, we also have a computational formula.

\begin{theorem}
Let \(X\) be a random variable with expectation \(\mathbb EX = \mu\). Then the variance \(\operatorname{Var}(X) = \mathbb E(X - \mu)^2\) can also be calculated as
\[ \operatorname{Var}(X) = \mathbb EX^2 - \mu^2 . \]
\end{theorem}

(Again, \(\mathbb EX^2\) means the expectation of \(X^2\).)

\begin{proof}
As previously we expand out the brackets, and use linearity of expectation (in the same way we ``brought the sum inside'' previously). We get
\begin{align*}
  \operatorname{Var}(X) &= \mathbb E(X - \mu)^2 \\
    &= \mathbb E(X^2 - 2\mu X + \mu^2) \\
    &= \mathbb EX^2 - \mathbb E(2\mu X) + \mathbb E \mu^2 \\
    &= \mathbb EX^2 - 2\mu \,\mathbb EX + \mu^2 .
\end{align*}
But we said that \(\mathbb EX\) would be called \(\mu\), so we can substitute in \(\mathbb EX = \mu\), to get
\[ \operatorname{Var}(X) = \mathbb E X^2 - 2\mu^2 + \mu^2 = \mathbb E X^2 - \mu^2 , \]
as required.
\end{proof}

\begin{example}
Let \(X \sim \text{Bern}(p)\) be a Bernoulli trial, and recall that \(\mathbb EX = p\).

Using the definitional formula, we have
\begin{align*}
\operatorname{Var}(X) &= \mathbb E(X-p)^2 \\
        &= (0 - p)^2 \,p_X(0) + (1-p)^2\, p_X(1) \\
        &= p^2\times(1-p) + (1-p)^2 \times p \\
        &= p(1-p)\big(p + (1-p)\big) \\
        &= p(1-p) .
\end{align*}

Alternatively, using the computational formula, we have
\begin{align*}
\operatorname{Var}(X) &= \mathbb EX^2 - p^2 \\
        &= \big(0^2\,p_X(0) + 1^2 p_X(1)\big) - p^2 \\
        &= 0\times(1-p) + 1\times p - p^2 \\
        &= p - p^2 \\
        &= p(1-p) .
\end{align*}
\end{example}

\begin{example}
For the total of two dice, using the computational formula, we have
\begin{align*}
\operatorname{Var}(X) &= \mathbb EX^2 - \mu^2 \\
        &= \left(2^2 \times \frac{1}{36} + 3^2 \times \frac{2}{36} + \cdots + 12^2 \times \frac{1}{36}\right) - 7^2 \\
        &= \frac{1974}{36} - 49 \\
        &= \frac{70}{12} \approx 5.8 .
\end{align*}
\end{example}

Finally, a result on what happens to the variance of simple functions of random variables.

\begin{theorem}

Let \(X\) be a random variable. Then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\operatorname{Var}(aX) = a^2\operatorname{Var}(X)\);
\item
  \(\operatorname{Var}(X + b) = \operatorname{Var}(X)\).
\end{enumerate}

\end{theorem}

You will prove this on \protect\hyperlink{P3}{the problem sheet}.

\hypertarget{summary-05}{%
\section*{Summary}\label{summary-05}}
\addcontentsline{toc}{section}{Summary}

\begin{itemize}
\tightlist
\item
  A random variable is a numerical summary of a random experiment.
\item
  The probability mass function (PMF) is \(p_X(x) = \mathbb P(X = x)\), and the cumulative distribution function (CDF) is \(F_X(x) = \mathbb P(X \leq x)\).
\item
  The expectation is \(\mathbb EX = \sum_x x\, p_X(x)\).
\item
  The variance of a random variable with expectation \(\mu\) is \(\operatorname{Var}(X) = \mathbb E(X - \mu)^2\).
\item
  \(\mathbb E(aX+b) = a\mathbb EX + b\) and \(\operatorname{Var}(aX+b) = a^2\operatorname{Var}(X)\).
\end{itemize}

\hypertarget{P3}{%
\chapter*{Problem Sheet 3}\label{P3}}
\addcontentsline{toc}{chapter}{Problem Sheet 3}

\commfalse

This is Problem Sheet 3. This problem sheet covers \protect\hyperlink{S04-conditional}{Sections 4} and \protect\hyperlink{S05-discrete-rv}{5}. You should work through all the questions on this problem sheet during Weeks 4 and 5, in preparation for your tutorial in Week 6. The problem sheet contains two assessed questions, which are due in by \textbf{2pm on Monday 8 November}.

\hypertarget{P3-short}{%
\section*{A: Short questions}\label{P3-short}}
\addcontentsline{toc}{section}{A: Short questions}

\textbf{A1.} Consider dealing two cards (without replacement) from a pack of cards. Which of the following pairs of events are independent?

\textbf{(a)} ``The first card is a Heart'' and ``The first card is Red''.

\textbf{(b)} ``The first card is a Heart'' and ``The first card is a Spade''.

\textbf{(c)} ``The first card is a Heart'' and ``The first card is an Ace''.

\textbf{(d)} ``The first card is a Heart'' and ``The second card is a Heart''.

\textbf{(e)} ``The first card is a Heart'' and ``The second card is an Ace''.

\textbf{A2.} Three events \(A, B, C\) are said to be \emph{pairwise independent} if each pair of events are independent -- that is, if \(A\) and \(B\) are independent, \(B\) and \(C\) are independent, and \(A\) and \(C\) are independent. The three events are said to \emph{mutually independent} if they are pairwise independent and also \(\mathbb P(A \cap B \cap C) = \mathbb P(A)\,\mathbb P(B)\,\mathbb P(C)\).

Consider rolling two dice independently. Let \(A\) be the event that the first roll is even, let \(B\) be the event that the second roll is even, and let \(C\) be the event that the total score is even.

\textbf{(a)} Are \(A\), \(B\), and \(C\) pairwise independent?

\textbf{(b)} Are \(A\), \(B\), and \(C\) mutually independent?

\textbf{A3.} Consider the random variable \(X\) with the following PMF:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\(x\) & \(-1\) & \(0\) & \(0.5\) & \(1\) & \(2\) \\
\midrule
\endhead
\(p(x)\) & \(0.1\) & \(0.3\) & \(0.3\) & \(0.2\) & \(0.1\) \\
\bottomrule
\end{longtable}

Find the expectation and variance of \(X\).

\textbf{A4.} Consider the random variable \(X\) with the following PMF:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\(x\) & \(1\) & \(2\) & \(4\) & \(5\) & \(a\) \\
\midrule
\endhead
\(p(x)\) & \(0.1\) & \(0.2\) & \(0.1\) & \(b\) & \(c\) \\
\bottomrule
\end{longtable}

This random variable has \(\mathbb EX = 4.3\) and \(\operatorname{Var}(X) = 4.61\). Find the values of \(a\), \(b\) and \(c\).

\textbf{A5.} The temperature \(T_C\) measured in degrees Celsius can be converted to a temperature \(T_F\) in degrees Fahrenheit using the formula \(T_F = \frac95 T_C + 32\).

The average daily maximum temperature in Leeds in July is 19.0~°C with a variance of 10.4~°C\textsuperscript{2}.

\textbf{(a)} What is the average daily maximum temperature in degrees Fahrenheit?

\textbf{(b)} What is the variance of the daily maximum temperature in degrees Fahrenheit squared?

\textbf{(c)} The average temperature in March is 1.8~°C higher than in February. What is this average difference in degrees Fahrenheit?

\hypertarget{P3-long}{%
\section*{B: Long questions}\label{P3-long}}
\addcontentsline{toc}{section}{B: Long questions}

\textbf{B1.} Suppose \(A\) and \(B\) are a pair of independent events. Show that \(A\) and \(B^\mathsf{c}\) are also independent events.

\textbf{B2.} Chloe cycles to work with probability \(0.6\) and takes the bus with probability \(0.4\). She has noticed that she is late 20\% of the time when she takes the bus. Her boss notices that, on average, Chloe is late one time per week (that is, once every five days).

\textbf{(a)} What is the probability that Chloe will be late if she cycles to work?

\textbf{(b)} Chloe is late today, and her boss suspects this is because she took the bus. What is the probability that this guess is correct?

\textbf{B3.} Let \(\Omega\) be a sample space, let \(\mathbb P\) be a probability measure on \(\Omega\), and fix an event \(B \subset \Omega\) with \(\mathbb P(B) > 0\). Show that the conditional probability \(\mathbb P( {\cdot} \mid B)\) is also a probability measure on \(\Omega\). That is, show that the conditional probability also satisfies the probability axioms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbb P(A \mid B) \geq 0\) for all events \(A \subset \Omega\);
\item
  \(\mathbb P(\Omega \mid B) = 1\);
\item
  For disjoint events \(A_1, A_2, \dots\), we have
  \[ \mathbb P(A_1 \cup A_2 \cup \cdots \mid B) = \mathbb P(A_1 \mid B) + \mathbb P(A_2 \mid B) + \cdots . \]
\end{enumerate}

\textbf{B4.} Soldiers are asked about their use of illegal drugs, but to protect their privacy and ensure honest answers, a ``randomised survey'' is used. Each soldier is handed a deck of three cards, picks one of the three cards at random, and responds according to what the card says. The three cards say:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``Say `Yes.'\,''
\item
  ``Say `No.'\,''
\item
  ``Truthfully answer the question `Have you taken any illegal drugs in the past 12 months?'\,''
\end{enumerate}

\textbf{(a)} 40\% of soldiers respond ``Yes''. What is the likely proportion of soldiers who have taken illegal drugs in the past 12 months.

\textbf{(b)} If a soldier responds ``Yes'', what is the probability that the soldier has taken illegal drugs in the past 12 months.

\textbf{B5.} Let \(X\) be a random variable. Prove that

\textbf{(a)} \(\operatorname{Var}(aX) = a^2 \operatorname{Var}(X)\);

\textbf{(b)} \(\operatorname{Var}(X+b) = \operatorname{Var}(X)\).

\textbf{B6.} A gambling game works as follows. You keep tossing a fair coin until you first get a Head. If your first Head comes on the \(n\)th coin toss, then you win \(2^n\) pounds.

\textbf{(a)} What is the probability that the first Head is seen on the \(n\)th toss of the coin?

\textbf{(b)} Show that the expected winnings from playing this game are infinite.

\textbf{(c)} The ``St Petersburg paradox'' refers to the fact that, despite the fact that the expected winnings from this game are infinite, hardly anybody would be prepared to play this game for, say, £100, and certainly not for £1000. Discuss a few possible ``resolutions'' to this paradox which could explain why people are unwilling to play this game despite seemingly having infinite expected winnings.

\hypertarget{P3-assessed}{%
\section*{C: Assessed questions}\label{P3-assessed}}
\addcontentsline{toc}{section}{C: Assessed questions}

The last two questions are \textbf{assessed questions}. These two questions count for 3\% of your final mark for this module.

The deadline for submitting your solutions is \textbf{2pm on Monday 8 November} at the beginning of Week 7, although I strongly recommend completing and submitting your work during Week 6. Submission will be via Gradescope; submission will open on Monday 1 November.
Your work will be marked by your tutor and returned on Monday 15 November, when solutions will also be made available.

Both questions are ``long questions'', where the marks are not only for mathematical accuracy but also for the clarity and completeness of your explanations.

You should not collaborate with others on the assessed questions: your answers must represent solely your own work. The University's rules on \href{https://library.leeds.ac.uk/info/1401/academic_skills/46/academic_integrity_and_plagiarism}{academic integrity} -- and the related punishments for violating them -- apply to your work on the assessed questions.

\textbf{C1.} A computer spam filter is 98\% effective at sending spam emails to my junk folder, but will also incorrectly send 1\% of legitimate emails to my junk folder. Suppose that 1 in 10 emails are spam. What proportion of emails in my junk folder are actually legitimate emails?

\textbf{C2.} A random variable \(X\) is said to follow the \emph{discrete uniform distribution} on \(\{1, 2, \dots, n\}\) if each value \(x\) in that set \(\{1,2,\dots,n\}\) is equally likely.

\textbf{(a)} Show that the expectation of \(X\) is \(\mathbb EX = \displaystyle\frac{n+1}{2}\).

\textbf{(b)} Find the variance of \(X\).

\textbf{(c)} Let \(Y\) be a discrete uniform distribution on \(\{a, a+1, a+2, \dots, b-1, b\}\), for integers \(a\) and \(b\) with \(a<b\). Using parts \(a\) and \(b\), but without calculating any sums directly, find the expectation and variance of \(Y\).

You may use without proof the standard results
\[ \sum_{x=1}^n x = \frac{n(n+1)}{2} \qquad  \sum_{x=1}^n x^2 = \frac{n(n+1)(2n+1)}{6} . \]

\hypertarget{P3-short-sols}{%
\section*{Solutions to short questions}\label{P3-short-sols}}
\addcontentsline{toc}{section}{Solutions to short questions}

\textbf{A1.} (c) and (e) are independent. \textbf{A2.} (a) Yes (b) No \textbf{A3.} 0.5 and 0.6 \textbf{A4.} \(a = 9, b = 0.5, c = 0.1\) \textbf{A5} (a) 66.2~°F (b) 33.7~°F\textsuperscript{2} (c) 3.2~°F

\hypertarget{part-other-stuff}{%
\part*{Other stuff}\label{part-other-stuff}}
\addcontentsline{toc}{part}{Other stuff}

\hypertarget{R}{%
\chapter*{R Worksheets}\label{R}}
\addcontentsline{toc}{chapter}{R Worksheets}

\hypertarget{r-work}{%
\section*{R worksheets}\label{r-work}}
\addcontentsline{toc}{section}{R worksheets}

Each week there will be an R worksheet to work through in your own time. We recommend spending about one hour on each worksheet, plus one extra hour for worksheets with assessed questions, for checking through and submitting your solutions.

\begin{longtable}[]{@{}clc@{}}
\toprule
Week & Worksheet & Deadline for assessed work \\
\midrule
\endhead
1 & \href{https://mpaldridge.github.io/math1710/R1.html}{\textbf{R basics}} (\href{https://mpaldridge.github.io/math1710/R1-solutions.html}{Solutions}) & --- \\
2 & \href{https://mpaldridge.github.io/math1710/R2.html}{\textbf{Vectors}} & --- \\
3 & \href{https://mpaldridge.github.io/math1710/R3.html}{\textbf{Data in R}} & Monday 18 October \\
4 & \href{https://mpaldridge.github.io/math1710/R4.html}{\textbf{Plots I:} Making plots} & --- \\
5 & \href{https://mpaldridge.github.io/math1710/R5.html}{\textbf{Plots II:} Making plots better} & Monday 1 November \\
6 & RMarkdown (optional) & --- \\
7 & Discrete random variables & Monday 15 November \\
8 & Discrete distributions & --- \\
9 & Normal distribution & Monday 29 November \\
10 & Law of large numbers & --- \\
11 & Summary & \emph{To be confirmed} \\
\bottomrule
\end{longtable}

\hypertarget{about-r}{%
\section*{About R and RStudio}\label{about-r}}
\addcontentsline{toc}{section}{About R and RStudio}

\begin{itemize}
\tightlist
\item
  \textbf{R} is a \emph{programming language} that is particularly good at working with probability and statistics. R is very widely used in universities and increasingly widely used in industry. Learning to use R is a mandatory part of this module, and exercises requiring use of R make up at least 15\% of your module mark. Many other statistics-related course at the University also use R.
\item
  \textbf{RStudio} is a \emph{program} that gives a convenient way to work with the language R. RStudio is the most common way to use the language R, and learning to use RStudio is strongly recommended.
\end{itemize}

R and RStudio are free/open-source software.

\hypertarget{r-access}{%
\section*{How to access R and RStudio}\label{r-access}}
\addcontentsline{toc}{section}{How to access R and RStudio}

There are a number of ways you can access R and RStudio:

\begin{itemize}
\tightlist
\item
  All \textbf{University computers} have R and RStudio already installed. \href{https://it.leeds.ac.uk/it/?id=kb_article\&sysparm_article=KB0013658}{Here is a directory of the University's computer clusters.}
\item
  You can \textbf{install} R and RStudio on your own computer -- see the instructions below.
\item
  If you want to use R/RStudio on a non-University device for which you don't have admin/installation rights (Chromebook, iPad, friend's laptop, etc), you could try:

  \begin{itemize}
  \tightlist
  \item
    You can use the University's copies of R/RStudio virtually through the \href{https://it.leeds.ac.uk/it?id=kb_article\&sysparm_article=KB0014379}{Windows Virtual Desktop} or \href{https://it.leeds.ac.uk/it?id=kb_article\&sysparm_article=KB0014827}{AppsAnywhere} client.
  \item
    The \href{https://rstudio.cloud/}{RStudio Cloud} is a cloud-hosted ``Google Docs for R'' that you can use through your web browser -- you can get 25 hours per month for free (or pay for more).
  \end{itemize}
\end{itemize}

\hypertarget{r-install}{%
\section*{Installing R and RStudio}\label{r-install}}
\addcontentsline{toc}{section}{Installing R and RStudio}

Students who have their own computer usually find it most convenient to install R and RStudio on that computer. To do this, it's important that you install R (the programming language) first, and only install RStudio (the program to use R) once R has already been installed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{First}, install \textbf{R}. Go to the \href{https://cran.r-project.org/}{Comprehensive R Archive Network} and follow the instructions:

  \begin{itemize}
  \tightlist
  \item
    Windows: Click \href{https://cran.r-project.org/bin/windows/}{``Download R for Windows''}, then \href{https://cran.r-project.org/bin/windows/base/}{``Install R for the first time''}. The main link at the top should be to download the most recent version of R.
  \item
    Mac: Click \href{https://cran.r-project.org/bin/macosx/}{Download R for macOS}, and then download the relevant PKG file. (For pre-November 2020 Intel-based Macbooks, you must use the ``Intel 64-bit build''; for post-November 2020 M1-based ``Apple silicon'' Macbooks, the ``Apple silicon arm64 build'' may be faster.)
  \end{itemize}
\item
  \emph{After} R is installed, \emph{then} install \textbf{RStudio}. Go to \href{https://www.rstudio.com/products/rstudio/download/\#download}{the Download page at RStudio.com} and follow the instructions. You want ``RStudio Desktop'', and you want the free version.
\end{enumerate}

If you have difficulty installing R, come along to the first computational drop-in session in Week 2 and bring your computer with you (if it's sufficiently portable), and we'll do our best to help.

\hypertarget{troubleshooting}{%
\section*{Troubleshooting drop-in sessions}\label{troubleshooting}}
\addcontentsline{toc}{section}{Troubleshooting drop-in sessions}

You will learn to use R by working through the R Worksheets. Learning to use a programming language is different from learning mathematics: you should expect to regularly get frustrated and annoyed when the computer seems to refuse to do what you want it to (but also occasionally experience the joy of getting it right!). This is a normal part of learning.

However, many students find getting with started with R in the first few weeks particularly frustrating. Also, sometimes students have problems installing R and RStudio on their own computers. To help with this, we have organised optional troubleshooting drop-in sessions in Weeks 2 and 3. Check your timetable for details -- they are probably listed as ``computer practicals''.

\commtrue

\hypertarget{solutions}{%
\chapter*{Solutions and group feedback}\label{solutions}}
\addcontentsline{toc}{chapter}{Solutions and group feedback}

There are many ways you get feedback on this module, both group feedback (feedback that is generally relevant to many people) and individual feedback (feedback based specifically on your own approach to the work).

\begin{itemize}
\tightlist
\item
  You will have received both individual and group spoken feedback in your tutorial (the more you speak up in your tutorial, the more individualised the feedback you get in return).
\item
  These solutions include group written feedback on common issues for the class.
\item
  Most importantly, when your work on assessed questions is marked, individual written feedback will be given via the Gradescope site. It is very important that you read that feedback.
\item
  Finally, students who would like even more feedback can discuss their work in the ``office hours'' drop-in sessions.
\end{itemize}

\hypertarget{P1-solutions}{%
\section*{Problem Sheet 1}\label{P1-solutions}}
\addcontentsline{toc}{section}{Problem Sheet 1}

\textbf{A1.} Consider again the ``number of Skittles in each packet'' data from Example 1.1.
\[ 59, \ 59, \ 59, \ 59, \ 60, \ 60, \ 60, \ 61, \ 62, \ 62, \ 62, \ 63, \ 63 .\]

\textbf{(a)} Calculate the mean number of Skittles in each packet.

\begin{myanswers}
\emph{Solution.}
This was in the notes:
\[ \bar x = \frac{1}{13} (59 + 59 + \cdots + 63) =  \frac{789}{13} = 60.7 .\]

\end{myanswers}

\textbf{(b)} Calculate the sample variance using the computational formula.

\begin{myanswers}
\emph{Solution.}
\begin{align*}
  s_x^2 &= \frac{1}{13 - 1} \left( (59^2 + 59^2 + \cdots + 63^2) - 13 \times 60.6923^2)\right) \\
        &= \frac{1}{12} (47915 - 47886.2) \\
        &= 2.40
\end{align*}

\textbf{Group feedback:} With the computational formula, the value \(\sum_i x_i^2 - n \bar{x}^2\) is typically a fairly small number given as the difference between two very big numbers \(\sum_i x_i^2\) and \(n \bar x^2\). This means you have to get the two big numbers very precise, to ensure the cancellation happens correctly; in particular, make sure you use plenty of decimal places of accuracy in \(\bar x\).

\end{myanswers}

\textbf{(c)} Calculate the sample variance using the definitional formula.

\begin{myanswers}
\emph{Solution.}
\begin{align*}
  s_x^2 &= \frac{1}{13 - 1} \left( (59 - 60.7)^2 + (59 - 60.7)^2 + \cdots + (63 - 60.7)^2 \right) \\
        &= \frac{1}{12} (2.86 + 2.86 + \cdots + 5.33) \\
        &= \frac{1}{12} \times 28.77 \\
        &= 2.40
\end{align*}

\end{myanswers}

\textbf{(d)} Out of (b) and (c), which calculation did you find easier, and why?

\begin{myanswers}
\emph{Solution.}
The computational formula required fewer presses of the calculator buttons, because \(\sum_i x_i^2\) is fewer button-presses than \(\sum_i (x_i - \bar x)^2\), where you have to subtract the means before squaring.

On the other hand, the expression inside the brackets of the computational formula is a fairly small number given as the difference of two very large numbers, so it was necessary to use lots of decimal places of accuracy in \(\bar x\) to make sure the second large number was accurate and therefore that the subtraction cancelled correctly.

\end{myanswers}

\textbf{A2.} Consider the following data sets of the age of elected politicians on a local council. (The ``18--30'' consists of people older than and including 18, and younger than but \emph{not} including 30.)

\begin{longtable}[]{@{}cccc@{}}
\toprule
Age (years) & Frequency & Relative frequency & Frequency density \\
\midrule
\endhead
18--30 & 1 & & \\
30--40 & 3 & & \\
40--45 & 4 & & \\
45--50 & 5 & & \\
50--55 & 3 & & \\
55--60 & 1 & & \\
60--70 & 3 & & \\
\textbf{Total} & 20 & 1 & --- \\
\bottomrule
\end{longtable}

\textbf{(a)} Complete the table by filling in the relative frequency and frequency densities.

\begin{myanswers}

\emph{Solution.}

\begin{longtable}[]{@{}cccc@{}}
\toprule
Age (years) & Frequency & Relative frequency & Frequency density \\
\midrule
\endhead
18--30 & 1 & 0.05 & 0.0041 \\
30--40 & 3 & 0.15 & 0.015 \\
40--45 & 4 & 0.2 & 0.04 \\
45--50 & 5 & 0.25 & 0.05 \\
50--55 & 3 & 0.15 & 0.03 \\
55--60 & 1 & 0.05 & 0.01 \\
60--70 & 3 & 0.15 & 0.015 \\
\textbf{Total} & 20 & 1 & --- \\
\bottomrule
\end{longtable}

\end{myanswers}

\textbf{(b)} What is the median age bin?

\begin{myanswers}
\emph{Solution.} The 10th- and 11th-largest observations are both in the 45--50 bin, which is therefore the median bin.

\end{myanswers}

\textbf{(c)} Calculate (an approximation of) the mean age of the politicians.

\begin{myanswers}
\emph{Solution.}
Pretending that each person is in the centre of their bin, we have
\[ \bar x = \frac{1}{20} (1\times24 + 3\times 35 + \cdots + 3 \times 65) = \frac{946.5}{20} = 47.3 . \]

\end{myanswers}

\textbf{B1.} For each of the two datasets below, calculate the following summary statistics, or explain why it is not possible to do so: mode; median; mean; number of distinct outcomes; inter-quartile range; and sample variance.

\textbf{(a)} Six packets of Skittles are opened together, and the total number of sweets of each colour is:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\textbf{Colour} & Red & Orange & Yellow & Green & Purple \\
\midrule
\endhead
\textbf{Number of Skittles} & 67 & 71 & 87 & 74 & 62 \\
\bottomrule
\end{longtable}

\begin{myanswers}
\emph{Solution.}
The modal colour is Yellow. The number of distinct outcomes is 5.

It's not possible to calculate the median or the quartiles, because, unlike numerical data, the colours can't be put ``in order'' from smallest to largest.

It's not possible to calculate the mean or sample variance, as these require us to have numerical data that can be ``added up'', but this can't be done with colours.

\end{myanswers}

\textbf{(b)} Shirt sizes for a university football squad:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
\textbf{Colour} & Xtra Small & Small & Medium & Large & Xtra Large \\
\midrule
\endhead
\textbf{Number of shirts} & 0 & 1 & 6 & 4 & 5 \\
\bottomrule
\end{longtable}

\emph{{[}\textbf{Note:} This has been corrected from an earlier version, where the 4 Large and 5 Xtra Large were the wrong way round.{]}}

\begin{myanswers}
\emph{Solution.}
The modal shirt size is medium. The number of distinct outcomes is 4 (we don't quite ``Xtra Small'', which was not observed in the data).

This time, we can order the data from smallest to largest, even though the data is not numerical. Since \((16 + 1)/2 - 8.5\), the median datapoint is the 8th or 9th datapoints, which are Large.

Since \(1 + 0.25(16 - 1) = 4.75\) the lower quartile is the 4th or 5th datapoints, which are Medium. Since \(1 + 0.75(16-1) = 12.25\), the upper quartile is the 12th or 13th datapoints, which are Xtra Large. So we can certainly say that the inner quartiles range from Medium to Xtra Large. We could probably also say that the interquartile range is 3 shirt sizes (Medium, Large, Xtra Large).

Again, because the data is not numerical, we can't add it up, so can't calculate a mean or sample variance.

\textbf{Group feedback:} Make sure your explanation is clear for why we can't calculate a median for the Skittles data but can for the shirts: they key is whether or not the data can be \emph{ordered}.

\end{myanswers}

\textbf{B2.} A summary statistic is informally said to be ``robust'' if it typically doesn't change much if a small number of outliers are introduced to a large dataset, or ``sensitive'' if it often changes a lot when a small number of outliers are introduced. Briefly discuss the robustness or sensitivity of the following summary statistics: \textbf{(a)} mode; \textbf{(b)} median; \textbf{(c)} mean; \textbf{(d)} number of distinct outcomes; \textbf{(e)} inter-quartile range; and \textbf{(f)} sample variance.

\begin{myanswers}
\emph{Solutions.}

\textbf{(a)} The mode will typically not change at all if a small number of outliers are introduced, so is robust. (The exception is for data where every observation is likely to be different, so the outliers become ``joint modes'' along with everything else; but in this case the mode is not a useful statistic in the first place.)

\textbf{(b)} The introduction of outliers will typically only change the median a little bit, by shifting it between different nearby values in the ``central mass'' of the data. In particular, the size of the outliers won't make any difference at all (only whether they are ``high outliers'' or ``low outliers''). So the median is robust.

\textbf{(c)} The mean can change a lot is outliers are introduced. (Think about the mean net worth of people in you tutorial group, and how it would change if Jeff Bezos or Elon Musk joined your tutorial group.) So the mean is sensitive.

\textbf{(d)} The number of distinct outcomes will only increase by (at most) 1 for each outlier introduced, so is robust.

\textbf{(e)} The interquartile range is robust, for the same reason as the median.

\textbf{(f)} The sample variance is sensitive, for the same reason as the mean.

(You might like to think about situations where it's better to use a robust statistic or better to use a sensitive statistic.)

\textbf{Group feedback:} Remember that ``robust'' and ``sensitive'' are general descriptions rather than precise mathematical definitions. So it doesn't matter if you disagree with my opinions provided that you give clear and detailed explanations to back up your opinion.

\end{myanswers}

\textbf{B3.} Let \(\mathbf a = (a_1, a_2, \dots a_n)\) and \(\mathbf b = (b_1, b_2, \dots, b_n)\) be two real-valued vectors of the same length. Then the \emph{Cauchy--Schwarz inequality} says that
\[ \left( \sum_{i=1}^n a_i b_i \right)^2 \leq \left( \sum_{i=1}^n a_i^2 \right) \left(\sum_{i=1}^n b_i^2 \right) . \]
Use the Cauchy--Schwarz inequality to show that the correlation \(r_{xy}\) satisfies \(-1 \leq r_{xy} \leq 1\).

(\emph{Hint:} Try to prove that \(s_{xy}^2 \leq s_x^2 s_y^2\). How does this help?)

\begin{myanswers}
\emph{Solutions.}
The first thing we want do is get from the Cauchy--Schwarz inequality
\[ \left( \sum_{i=1}^n a_i b_i \right)^2 \leq \left( \sum_{i=1}^n a_i^2 \right) \left(\sum_{i=1}^n b_i^2 \right) . \]
to the hint \(s_{xy}^2 \leq s_x^2 s_y^2\). We'll do this by making a clever choice for \((a_i)\) and \((b_i)\) in Cauchy--Schwarz that tells us something useful about \(s_{xy}\), \(s_x^2\), and \(s_y^2\).

Recalling the formulas for \(s_{xy}\), \(s_x^2\), and \(s_y^2\),
\begin{align*}
s_{xy} &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) ,\\
s_{x}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 ,\\
s_{y}^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar y)^2 ,
\end{align*}
and comparing them with the Cauchy--Schwarz inequality, it looks like taking \(a_i = x_i - \bar x\) and \(b_i = y_i - \bar y\) might be useful. Making the substitution, we get
\[ \left( \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) \right)^2 \leq \left( \sum_{i=1}^n (x_i - \bar x)^2 \right) \left(\sum_{i=1}^n (y_i - \bar y)^2 \right) . \]
These are very close to the formulas for \(s_{xy}\), \(s_x^2\), and \(s_y^2\), but are just missing the ``\(1/(n-1)\)''s; what we in fact have is
\[ \left( (n-1) s_{xy} \right)^2 \leq (n-1)s_x^2 \cdot (n-1) s_y^2 .\]
Cancelling \((n-1)^2\) from each side, we have \(s_{xy}^2 \leq s_x^2 s_y^2\), and we've proved the hint.

We now want to get from the hint to the desired statement \(-1 \leq r_{xy} \leq 1\). Recall the formula for the correlation is
\[ r_{xy} = \frac{s_{xy}}{s_xs_y} . \]
We can make the hint look a bit like this dividing both sides by \(s_x^2 s_y^2\), to get
\[\frac{s_{xy}^2}{s_x^2 s_y^2} \leq 1.   \]
In fact that's the square of the correlation on the left-hand side, so we've shown that \(r_{xy}^2 \leq 1\).

Finally, we note that if a number squared is less than or equal to 1, then the number must be between -1 and +1 inclusive. (Numbers bigger than 1 get bigger still when squared; number smalles than -1 become bigger than +1 when squared.) Hence we have shown that \(-1 \leq r_{xy} \leq 1\), as required.

\textbf{Group feedback:} There are two halves to this question: first get from the Cauchy--Schwarz inequality to the hint, and second get from the hint to the answer \(-1 \leq r_{xy} \leq 1\). Many students struggled with the first half -- but you can still try to do the second half. Especially in an exam, look for places where you can get marks for doing part of the question -- and it doesn't need to be the \emph{first} part!

\end{myanswers}

\textbf{B4.} A researcher wishes to study the effect of mental health on academic achievement. The researcher will collect data on the mental health of a cohort of students by asking them to fill in a questionnaire, and will measure academic achievement via the students' scores on their university exams. Discuss some of the ethical issues associated with the collection, storage, and analysis of this data, and with the publication of the results of the analysis. Are there ways to mitigate these issues?

(It's not necessary to write an essay for this question -- a few short bulletpoints will suffice. There may be an opportunity to discuss these issues in more detail in your tutorial.)

\begin{myanswers}
\textbf{Group feedback:} There are no ``correct'' or ``incorrect'' answers here, but here are a few things that students in my own tutorials brought up, which may act as a prompt for your own discussions.

\begin{itemize}
\tightlist
\item
  It's important the students/subjects have given their consent for their data to be used this way. It must be ``informed consent'', where they understand for what purpose the data will be used, how it will be stored, and so on. It must be possible and painless for students to decline to take part.
\item
  Consideration should be given on how to anonymise the data as much as possible -- it's not necessary for those analysing the data to know which questionnaire or which exam result belongs to which student, only that the questionnaire and results can be paired up.
\item
  Even if after data is anonymised, care should be taken about whether the students could be worked out from the data. For example, if only one student did a certain combination of modules, their identity could ``leak'' that way. Perhaps imprecise data, such as classes rather than exact marks, might help while only slightly reducing the usefulness of the data?
\item
  On one hand, it seems like this data should perhaps be deleted once analysis has been carried out, for the privacy of the students. On the other hand, principles of ``open science'' suggest that the data should be kept -- and even publically made available -- for other researchers to check the work. There are competing ethical considerations here.
\item
  If correlations are found in the data, care should be taken when publishing the analysis not to wrongly suggest a causation. (Just because X and Y are positively correlated, it doesn't mean that X \emph{causes} Y -- or that Y causes X.)
\end{itemize}

You can probably think of many other things.

\end{myanswers}

\hypertarget{P2-solutions}{%
\section*{Problem Sheet 2}\label{P2-solutions}}
\addcontentsline{toc}{section}{Problem Sheet 2}

\textbf{A1.} Suppose you toss a coin 10 times. What would you suggest for a sample space \textbf{(a)} if you only care about the total number of heads; \textbf{(b)} if you care about the result of every coin toss?

\begin{myanswers}
\emph{Solution.}

\textbf{(a)} The number of heads can be any number from 0 to 10, so we should take \(\Omega = \{0,1,2,\dots, 10\}\). (This sample space contains 11 sample outcomes that are not equally likely.)

\textbf{(b)} The vector of coin outcomes will be something like \((\text{H}, \text{H}, \text{T}, \text{T}, \text{H}, \text{T}, \text{T}, \text{T}, \text{H}, \text{T})\). So our sample space \(\Omega\) should be the set of all vectors of length 10 whose entries are either H, T; the notation \(\Omega = \{\text{H}, \text{T}\}^{10}\) is sometimes used for this. (Note that, by the multiplication principle, this sample space contains \(2^{10} = 1024\) sample outcomes that are equally likely.)

\end{myanswers}

\textbf{A2.} Let \(A\), \(B\) and \(C\) be events in a sample space \(\Omega\). Write the following events using only \(A\), \(B\), \(C\) and the complement, intersection, and union operations.

\textbf{(a)} \(C\) happens but \(A\) doesn't.

\begin{myanswers}
\emph{Solution.} This is ``\(C\) and not \(A\)'': \(C\cap A^{\mathsf{c}}\).

\end{myanswers}

\textbf{(b)} At least one of \(A\), \(B\) and \(C\) happens.

\begin{myanswers}
\emph{Solution.} This is simply the union \(A \cup B\cup C\).

\end{myanswers}

\textbf{(c)} Exactly one of \(B\) or \(C\) happens.

\begin{myanswers}
\emph{Solution.} One way to write this is to split it up as ``\,`\(B\) but not \(C\)' or `\(C\) but not \(B\)'\,'', which is \((B \cap C^{\mathsf{c}}) \cup (B^{\mathsf{c}} \cap C)\).

An alternative is to split it up as ``\,`\(B\) or \(C\)' but not `both \(B\) and \(C\)'\,'', which is \((B \cup C) \cap (B\cap C)^{\mathsf{c}}\).

You can check these are equal by (for example) using De Morgan's law and the distributive law to expand out the second version.

\end{myanswers}

\textbf{(d)} Exactly two of \(A\), \(B\) and \(C\) happens.

\begin{myanswers}
\emph{Solution.} I would split this up into ``\(A\) and \(B\) but not \(C\)'', ``\(A\) and \(C\) but not \(B\)'', and ``\(B\) and \(C\) but not \(A\)'' and take the union. This gives
\[  (A \cap B \cap C^{\mathsf{c}}) \cup (A \cap B^{\mathsf{c}} \cap C) \cup (A^{\mathsf{c}} \cap B \cap C) . \]
There are other equivalent formulations.

\end{myanswers}

\textbf{A3.} Let \(\Omega\) be a sample space with a probability measure \(\mathbb P\), and let \(A, B \subset \Omega\) be events. State, with brief explanations, whether the following statements are true or false:

\textbf{(a)} If \(\mathbb P(A) \leq \mathbb P(B)\), then \(A \subset B\).

\begin{myanswers}
\emph{Solution.} False. It is true that \emph{if} \(A \subset B\) \emph{then} \(\mathbb P(A) \leq \mathbb P(B)\), but here the implication is the wrong way around.

For a counterexample, consider rolling a dice, and let \(A = \{1\}\) and \(B = \{2,3\}\). Then \(\mathbb P(A) = \frac16 \leq \frac26 = \mathbb P(B)\), but it's not true that \(A \subset B\).

\end{myanswers}

\textbf{(b)} \(\mathbb P(A \cap B) + \mathbb P(A \cap B^{\mathsf{c}}) = \mathbb P(A)\).

\begin{myanswers}
\emph{Solution.} True. Note that \((A \cap B) \cup (A \cap B^{\mathsf{c}}) = A\) and that the union is disjoint. (Try drawing a Venn diagram, if this isn't obvious.) The result follows from applying Axiom 3.

\end{myanswers}

\textbf{(c)} \(\mathbb P(A \cup B) \leq \mathbb P(A)\)

\begin{myanswers}
\emph{Solution.} False. On the contrary, \(A \subset A \cup B\), so the inequality should be the other way round. The same \(A\) and \(B\) as in part (a) gives a concrete counterexample. The statement would be true with a reversed inequality, or with the union replaced by an intersection.

\end{myanswers}

\textbf{(d)} If \(A\) and \(B\) are disjoint, then \(\mathbb P((A \cup B)^{\mathsf{c}}) = 1 - \mathbb P(A) - \mathbb P(B)\).

\begin{myanswers}
\emph{Solution.} True. From the complement rule, we have \(\mathbb P((A \cup B)^{\mathsf{c}}) = 1 - \mathbb P(A \cup B)\); then from the addition rule for disjoint unions we have \(\mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B)\). Putting these together gives the result.

\end{myanswers}

\textbf{A4.} What is the value of the following expressions?

\textbf{(a)} \(6!\)

\begin{myanswers}
\emph{Solution.}
\[ 6! = 6 \times 5 \times 4 \times 3 \times 2 \times 1 = 720. \]

\end{myanswers}

\textbf{(b)} \({8}^{\underline{4}}\)

\begin{myanswers}
\emph{Solution.}
\[ {8}^{\underline{4}} = 8 \times 7 \times 6 \times 5 = 1680 \]

\end{myanswers}

\textbf{(c)} \({\displaystyle \binom{10}{4}}\)

\begin{myanswers}
\emph{Solution.}
\[ \binom{10}{4} = \frac{10 \times 9 \times 8 \times 7}{4\times 3\times 2\times 1} = 210 \]

\end{myanswers}

\textbf{A5.} An urn contains 5 red balls and 7 blue balls. Four balls are drawn from the urn. What is the probability that at least one of the balls is red, if the balls are drawn \textbf{(a)} with replacement; \textbf{(b)} without replacement?

\begin{myanswers}
\emph{Solution.} This is an ``at least one'' question, so it will be better to look at the complementary event \(A^\mathsf{c}\) that none of the four balls drawn are red -- that is, that they are all blue.

\textbf{(a)} There are \(|\Omega| = 12^4 = 20736\) ways to draw four balls with replacement. There are \(|A^\mathsf{c}| = 2401\) to draw all blue balls. So
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{|A^\mathsf{c}|}{|\Omega|} = 1 - \frac{2041}{20736} = 0.884 . \]

\textbf{(b)} There are \(|\Omega| = {12}^{\underline{4}} = 11880\) ways to draw four balls without replacement. There are \(|A^\mathsf{c}| = {7}^{\underline{4}} = 840\) to draw all blue balls. So
\[ \mathbb P(A) = 1 - \mathbb P(A^\mathsf{c}) = 1 - \frac{|A^\mathsf{c}|}{|\Omega|} = 1 - \frac{840}{11880} = 0.929 . \]

\end{myanswers}

\textbf{B1} Starting from just the three probability axioms, prove the following statements:

\textbf{(a)} \(\mathbb P(\varnothing) = 0\).

\begin{myanswers}
\emph{Solution.} As always, we seek a disjoint union, to allow us to use Axiom 3.

Let \(A\) be any event (such as \(A = \varnothing\) or \(A = \Omega\), for example). Then \(A \cup \varnothing = A\), and the union is disjoint -- since \(\varnothing\) contains no sample points, it certainly can't contain any sample points that are also in \(A\). Then applying Axiom 3, we get \(\mathbb P(A) + \mathbb P(\varnothing) = \mathbb P(A)\). Subtracting \(\mathbb P(A)\) from both sides gives the result.

\emph{Alternatively}, if you prove part (b) first, you can apply that with \(A = \Omega\). Since \(\Omega^\mathsf{c}= \varnothing\) and Axiom 2 tells us that \(\mathbb P(\Omega) = 1\), the result follows.

\end{myanswers}

\textbf{(b)} \(\mathbb P(A^\mathsf{c}) = 1 - \mathbb P(A)\).

\begin{myanswers}
\emph{Solution.} A very useful and relevant disjoint union is \(A \cup A^\mathsf{c}= \Omega\). Applying Axiom 3 gives us \(\mathbb P(A) + \mathbb P(A^\mathsf{c}) = \mathbb P(\Omega)\). But Axiom 2 tells us that \(\mathbb P(\Omega) = 1\), so \(\mathbb P(A) + \mathbb P(A^\mathsf{c}) = 1\). Rearranging gives the result.

\end{myanswers}

\textbf{B2.} Suppose we pick a number at random from the set \(\{1, 2, \dots, 2021\}\).

\textbf{(a)} What is the probability that the number is divisible by 5?

\begin{myanswers}
\emph{Solution.} The sample space is \(\Omega = \{1, 2, \dots, 2021\}\), and \(A\) is the set of numbers up to 2021 that are divisible by 5. Clearly \(|\Omega| = 2021\). Further, \(|A|\) is the largest integer no bigger than \(\frac{2021}{5} = 404.2\), which is 404, as this is how many times 5 ``goes into'' 2021. Hence
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{404}{2021} = 0.1999 , \]
just a tiny bit smaller than \(\frac{1}{5}\).

\end{myanswers}

\textbf{(b)} What is the probability the number is divisible by 5 or by 7?

\begin{myanswers}
\emph{Solution.} With the same \(\Omega\) and \(A\), we now have \(B\) being the numbers up to 2021 divisible by \(7\); so we're looking for \(\mathbb P(A \cup B)\). As before, \(|B|\) is the largest integer no bigger that \(\frac{2021}{7} = 288.7\), which is \(288\). So
\[ \mathbb P(A \cup B) = \frac{404}{2021} + \frac{288}{2021} - \mathbb P(A\cap B) . \]

Here, \(A \cap B\) is the numbers divisible by both 5 and 7, which is precisely the numbers divisible by \(5 \times 7 = 35\). Then \(|A \cap B|\) is \(\frac{2021}{35} = 57.7\) rounded down. So finally, we have
\[ \mathbb P(A \cup B) = \frac{404}{2021} + \frac{288}{2021} - \frac{57}{2021} = \frac{635}{2021} = 0.314. \]

\end{myanswers}

\textbf{B3.} In this question, you will have to use the standard two-event form of the addition rule for unions
\[ \mathbb P(A \cup B) = \mathbb P(A) + \mathbb P(B) - \mathbb P(A \cap B) . \]

\textbf{(a)} Using the two-event addition rule, show that
\[ \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D \cup E) - \mathbb P\big(C \cap (D \cup E)\big).  \]

\begin{myanswers}
\emph{Solution.} As with the Cauchy--Schwarz question from Problem Sheet 1, the key is to make a good choice for what \(A\) and \(B\) should be. This time, \(A = C\) and \(D \cup E\) will work well, since \(C \cup (D \cup E) = C \cup D \cup E\). (You can call this ``associativity'', if you like.) Making that substitution immediately gives us
\[ \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D \cup E) - \mathbb P\big(C \cap (D \cup E)\big) ,  \]
as required.

\end{myanswers}

\textbf{(b)} Using the two-event addition rule and the distributive law, or otherwise, prove the three-event form of the addition rule for unions:
\[
  \mathbb P(C \cup D \cup E) = \mathbb P(C) + \mathbb P(D) + \mathbb P(E) 
  - \mathbb P(C \cap D) - \mathbb P(C \cap E) - \mathbb P(D \cap E) + \mathbb P(C \cap D \cap E) .
\]

\begin{myanswers}
\emph{Solution.}
Let's take the three terms on the right of the equation from part (a) separately.

The first term is \(\mathbb P(C)\), which is fine as it is.

The second term is \(\mathbb P(D \cup E)\). This is the probability of the union of two events, so we can use addition rule for the union of two events to get
\[ \mathbb P(D \cup E) = \mathbb P(D) + \mathbb P(E) - \mathbb P(D \cap E) . \]

The third term is \(\mathbb P\big(C \cap (D \cup E)\big)\). If we use the distributive law, as suggested in the question, we get \(C \cap (D \cup E) = (C \cap D) \cup (C\cap E)\), so we want to find \(\mathbb P\big((C \cap D) \cup (C\cap E)\big)\). But this is another union of two events again, this time with \(A = C \cap D\) and \(B = C \cap E\). So the addition rule gives
\[ \mathbb P\big((C \cap D) \cup (C\cap E)\big) = \mathbb P(C \cap D) + \mathbb P(C \cap E) - \mathbb P(C \cap D \cap E) , \]
since \((C \cap D) \cap (C \cap E) = C \cap D \cap E\).

Finally, we put this all together, and get
\begin{align*}
  \mathbb P(C \cup D &\cup E) \\
  &= \mathbb P(C) + \big(\mathbb P(D) + \mathbb P(E) - \mathbb P(D \cap E)\big) - \big(\mathbb P(C \cap D) + \mathbb P(C \cap E) - \mathbb P(C \cap D \cap E)\big) \\
  &= \mathbb P(C) + \mathbb P(D) + \mathbb P(E) - \mathbb P(C \cap D) - \mathbb P(C \cap E) - \mathbb P(D \cap E) + \mathbb P(C \cap D \cap E) . 
\end{align*}
which is what we wanted.

\end{myanswers}

\textbf{B4.} Eight friends are about to sit down at random at a round table. Find the probability that

\textbf{(a)} Ashley and Brook sit next to each other, with Chris directly opposite Brook;

\begin{myanswers}
\emph{Solution.}
Let \(\Omega\) be the number of ways the friends can sit around the table. This is an ordering problem, so \(\Omega = 8!\).

Let \(A\) be the event in the question. What is \(|A|\)? Well,

\begin{itemize}
\tightlist
\item
  Ashley can sit anywhere, so has 8 choices of seat.
\item
  Brook can sit either directly to Ashley's left or directly to Ashley's right, so has 2 choices of seat.
\item
  Chris must sit directly opposite Brook, so only has 1 choice of seat.
\item
  The remaining five friends can fill up the remaining seats however they like, so have 5, 4, 3, 2, and 1 choices respectively.
\end{itemize}

Hence \(|A| = 8 \times 2 \times 1 \times 5 \times 4 \times 3 \times 2 \times 1\). Thus we get
\[ \mathbb P(A) = \frac{|A|}{|\Omega|} = \frac{8 \times 2 \times 1 \times 5 \times 4 \times 3 \times 2 \times 1}{8 \times 7 \times 6 \times 5 \times 4 \times 3 \times 2 \times 1} = \frac{2 \times 1}{7 \times 6} = \frac{1}{21} . \]

\end{myanswers}

\textbf{(b)} neither Ashley, Brook nor Chris sit next to each other.

\begin{myanswers}
\emph{Solution.}
The sample space \(\Omega\) is as before. Let's count the outcomes in \(B\), the event in the question.

\begin{itemize}
\tightlist
\item
  Ashley can sit anywhere, so has 8 choices of seat.
\item
  Chris's number of choices will depend on where Brook sits, so we'll have to count their choices together.

  \begin{itemize}
  \tightlist
  \item
    Brook cannot sit next to Ashley.
  \item
    If Brook sits next-but-one to Ashley, of which there are 2 choices, then Chris has 3 choices: Chris cannot sit on the seat directly between Ashley and Brook, nor directly next to Ashley on the other side, nor directly next to Brook on the other side.
  \item
    If Brook does sits neither next nor next-but-one to Ashley, of which there are 3 choices, then Chris has 2 choices: he cannot sit to the right or left of Ashley, nor to the right or left of Brook.
  \end{itemize}
\item
  The remaining friends have 5, 4, 3, 2, and 1 choices again.
\end{itemize}

Hence, \(|B| = 8 \times (2\times 3 + 3 \times 2) \times 5 \times 4 \times 3 \times 2 \times 1\). So
\[ \mathbb P(B) = \frac{|B|}{|\Omega|} = \frac{8 \times (2\times 3 + 3 \times 2) \times 5 \times 4 \times 3 \times 2 \times 1}{8 \times 7 \times 6 \times 5 \times 4 \times 3 \times 2 \times 1} = \frac{2\times 3 + 3 \times 2}{7 \times 6} = \frac{12}{42} = \frac{2}{7} .  \]

\emph{Alternatively}, last year a MATH1710 student suggested to me the following rather elegant solution. Suppose the five other friends are already sat at a round table with five chairs. Ashley, then Brook, then Chris will each bring along their own chair, and push into one of the gaps between the friends.

Ashley has 5 gaps to choose from, the Brook will have 6 (Ashley joining the table will have increased the number of gaps by 1), then Chris will have 7, so the total number of ways they can push in is \(|\Omega| = 5 \times 6 \times 7\).

To not sit next to each other, Ashley can push in any of the 5 gaps, Brook only has \(6 - 2 = 4\) choices (not in the gap directly to the left or right of Ashley), and Chris only has \(7 - 4 = 3\) choices (not in the gaps directly to the left or right of Ashley nor the gaps directly to the left or right of Brook -- these four gaps are distinct). Hence \(|B| = 5 \times 4 \times 3\), and we have
\[ \mathbb P(B) = \frac{5 \times 4 \times 3}{5 \times 6 \times 7} = \frac{4 \times 3}{6 \times 7} = \frac{12}{42} = \frac{2}{7}.  \]

\end{myanswers}

\textbf{B5.} Suppose your tutorial group contains 12 students -- you and 11 others. The tutor wishes to choose 4 members of the group to present their work.

\textbf{(a)} How many ways can the tutor choose the presentation group?

\begin{myanswers}
\emph{Solution.}
The tutor is sampling 4 items from 12, without replacement (the same person can't be picked twice) and where the order doesn't matter. So this is
\[ \binom{12}{4} = 495 . \]

\end{myanswers}

\textbf{(b)} How many ways can the tutor choose the presentation group if you are one of the presenters?

\begin{myanswers}
\emph{Solution.}
Once the tutor has chosen you, she must pick 3 other students to complete the presentation group out of the other 11 students. So this is
\[ \binom{11}{3} = 165 . \]

\end{myanswers}

\textbf{(c)} How many ways can the tutor choose the presentation group if you are \emph{not} one of the presenters?

\begin{myanswers}
\emph{Solution.}
If the tutor doesn't chose you, she must pick all 4 presenters out of the other 11 students. So this is
\[ \binom{11}{4} = 330 . \]

\end{myanswers}

\textbf{(d)} Pascal's formula says that
\[ \binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k} . \]
Prove Pascal's formula.

\begin{myanswers}
\emph{Solution.}
We'd like a ``double-counting'' argument, where we count the same number in two different ways. Let's follow the breadcrumbs from the first three parts of the question.

Suppose the tutorial group has \(n\) students and there will be \(k\) presenters. How many ways can the presentation group be chosen?

One way is simply to say that this is \(\displaystyle\binom{n}{k}\).

Another way is to separately count the presentation groups that do include you and the presentation groups that don't include you, and add them together. The presentation groups including you require another \(k-1\) presenters from the other \(n -1\) students, which makes \(\binom{n-1}{k-1}\). The presentation groups not including you require all \(k\) presenters from the other \(n-1\), which makes \(\binom{n-1}{k}\). In total, the number of presentation groups is
\[ \binom{n-1}{k-1} + \binom{n-1}{k} . \]

Since we've counted the number of presentation groups in two different ways, these expressions must be equal.

\emph{Alternatively}, you can prove algebraically that
\[  \frac{n(n-1)\cdots(n-k+1)}{k(k-1)\cdots2\cdot1} = \frac{(n-1)(n-2)\cdots(n-k+1)}{(k-1)(k-2)\cdots2\cdot1} + \frac{(n-1)(n-2)\cdots(n-k)}{k(k-1)\cdots2\cdot1} \]
(start by making \(k!\) the common denominator on the right), but I feel that's not in the spirit of the question.

\end{myanswers}

\end{document}
