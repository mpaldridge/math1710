[["index.html", "MATH1710 Probability and Statistics I Schedule", " MATH1710 Probability and Statistics I Matthew Aldridge University of Leeds, 2021–22 Schedule Week 1 (27 September – 1 October): Section 1: Exploratory data analysis Problem Sheet 1: all questions – deadline for assessed questions: Monday 11 October R Worksheet 1 "],["about.html", "About MATH1710 Organisation of MATH1710 Content of MATH1710 About these notes", " About MATH1710 Organisation of MATH1710 This module is MATH1710 Probability and Statistics I. A few students will be taking this module as half of MATH2700 Probability and Statistics for Scientists. This module lasts for 11 weeks from 27 September to 10 December 2021. The exam will take place between 10 and 21 January 2022. The core teaching team are: Dr Matthew Aldridge (you can call me “Matt” or “Dr Aldridge”): I am the module leader, the main lecturer, and the main author of these notes. A module assistant TBC. The shared email address for the core teaching team is math1710@leeds.ac.uk; please use this address, rather than emailing our personal addresses; this will ensure your email is seen as soon as possible. Notes and videos The main way you will learn new material for this module is by reading these notes and by watching the accompanying pre-recorded videos. There will be one section of notes each week, for a total of 11 sections, with the final section being a summary and revision. Reading mathematics is a slow process. Each section should take one and a half to two hours to work through; we recommend you split this into two or more sessions. If you find yourself regularly getting through sections in much less than that amount of time, you’re probably not reading carefully enough through each sentence of explanation and each line of mathematics, including understanding the motivation, checking the accuracy, and making your own notes. You are probably reading the web version of the notes. If you want a PDF or ebook copy (to read offline or to print out), they can be downloaded via the top ribbon of the page. (Warning: I have not made as much effort to make the PDF and ebook as neat and tidy as I have the web version, and there may be formatting errors.) We are very keen to hear about errors in the notes mathematical, typographical or otherwise. Please, please email us if think you may have found any. Problem sheets There will be 5 problem sheets. Each problem sheet has a number of short and long questions for you to cover in your own time to help you learn the material, and two assessed questions, which you should submit for marking. The assessed questions on each problem sheet make up 3% of your mark on this module, for a total of 15%. Although the deadlines are on Mondays, you are advised to complete and submit the work in the previous week. Problem Sheet Sections covered Assessed work due 1 1 Monday 11 October (Week 3) 2 2 and 3 Monday 25 October (Week 5) 3 4 and 5 Monday 8 November (Week 7) 4 6 and 7 Monday 22 November (Week 9) 5 8, 9 and 10 Monday 6 December (Week 11) Assessed questions should be submitted in PDF format through Gradescope. (Further Gradescope details will follow.) Most students choose to hand-write their solutions and then scan them to PDF using their phone; you should use a proper scanning app – we recommend Microsoft Office Lens or Adobe Scan – and not just submit photographs. Lectures You will have one online synchronous (that is, live, not recorded) “lecture” session each week, with me, run through Zoom. Because this is a large cohort, we will split into two groups: Group 1: Mondays at 1200 Group 2: Mondays at 1500 You should check your timetable to see which lecture group you are in. This will not be a “lecture” in the traditional sense of the term, but will be an opportunity to re-emphasise material you have already learned from notes and videos, to give extra examples, and to answer common student questions, with some degree of interactivity via quizzes, polls, and the chat box. We will assume you have completed all the work for the previous week by the time of the lecture. We are very keen to hear about things you’d like to go through in the lectures; please email us with your suggestions. Tutorials Tutorials are small groups of about a dozen students. You have been assigned to one of 38 tutorial groups, each with a member of staff as the tutor. Your tutorial group will meet five times, in Weeks 2, 4, 6, 8, and 10. Tutorial groups will meet in person on campus; you should check your timetable to see when and where your tutorial group meets. (For those not yet on campus, due to travel restrictions or health conditions, there will be an extra online tutorial group for the first few tutorials.) The main goal of the tutorials will be to go over your answers to the non-assessed questions on the problems sheets in an interactive session. In this smaller group, you will be able to ask detailed questions of your tutor, and have the chance to discuss your answers to the problem sheet. Your tutor may ask you to present some of your work to your fellow students, or may give you the opportunity to work together with others during the tutorial. Your tutor may be willing to give you a hint on the assessed questions if you’ve made a first attempt but have got stuck. My recommended approach to problem sheets and tutorials is the following: Work through the problem sheet before the tutorial, spending plenty of time on it, and making multiple efforts at questions you get stuck on. I recommend spending at least 3 hours per week on the problem sheets, which will usually mean a total of at least 6 hours per problem sheet (as most problem sheets cover two weeks). Collaboration is encouraged when working through the non-assessed problems, but I recommend writing up your work on your own; answers to assessed questions must be solely your own work. Take advantage of the small group setting of the tutorial to ask for help or clarification on questions you weren’t able to complete. After the tutorial, attempt again the questions you were previously stuck on. If you’re still unable to complete a question after this second round of attempts, then consult the solutions. Your tutor will also be the marker of your answers to the assessed questions on the problem sheets. R worksheets R is a programming language that is particularly good at working with probability and statistics. Learning to use R is an important part of this module, and is used in many other modules in the University, particularly in MATH1712 Probability and Statistics II. R is used by statisticians throughout academic and increasingly in industry too. Learning to program is a valuable skill for all students, and learning to use R is particularly valuable for students interested in statistics and related topics like actuarial science. You will learn R by working through one R worksheet each week in your own time. Worksheets 3, 5, 7, 9 and 11 will also contain a couple of questions for assessment. Each of these is worth 3% of your mark for a total of 15%. I recommend spending one hour per week on the week’s R worksheet, plus one extra hour if there are assessed questions that week. You can read more about the language R, and about the program RStudio that we recommend you use to interact with R, in the R section of these notes. To help you if you have problems with R, we have organised optional R troubleshooting drop-in sessions, where you can discuss any problems you have with an R expert, in Weeks 2 and 3. Check your timetable for details – these will be listed on your timetable as “practicals”. Office hours If you there is something in the module you wish to discuss in detail with the module core teaching team, the place for the is the optional weekly “office hours”, which will operate as drop-in sessions. These sessions are an optional opportunity for you to ask questions you have to a member of staff; these are particularly useful if there’s something on the module that you are stuck on or confused about, but we’re happy to discuss any statistics-related issues or questions you have. There will be two office hours per week: Wednesdays at 1000 and at 1200. (For boring reasons, the 1000 sessions appear on the timetable for MATH2700 students and the 1200 sessions appear on the timetable for MATH1710 students, but I’m happy for anyone to attend either hour.) The sessions will happen, until further notice, at least, in my office, PRD 9.320 on the the ninth floor of the Physics Research Deck. Time management It is, of course, up to you how you choose to spend your time on this module. But my recommendations for your weekly work would be something like this: Notes and videos: 2 hours per week/section Problem sheet: 3 hours per week (so 6 hours for most problem sheets) plus 1 extra hour for writing up and submitting answers to assessed questions R worksheet: 1 hour per week/worksheet, plus 1 extra hour if there are assessed questions Lecture: 1 hour per week Tutorial: 1 hour every other week Revision: 13 hours total at the end of the module That’s roughly 8 hours a week, and makes 100 hours in total. (MATH1710 is a 10 credit module, so is supposed to represent 100 hours work. MATH2700 students are expected to be able to use their greater experience to get through the material in just 75 hours, so should scale these recommendations accordingly.) Exam There will be an exam in January, which makes up the remaining 70% of your mark. The exam will consist of 20 short and 2 long questions, and will be time-limited to 2 hours. We’ll talk more about the exam format near the end of the module. Who should I ask about…? Remember that the email address for the core module teaching team is math1710@leeds.ac.uk. Please don’t email our personal addresses; it will take longer for us to reply, and we may miss your email all together. I don’t understand something in the notes or on a problem sheet: Come to office hours, or (if the timing works) ask your tutor in your next tutorial. I’m having difficulties with R: In Weeks 2 or 3, you should attend the R trouble-shooting drop-in session; at other times, come to office hours. I have an admin question about arrangements for the module: Come to office hours or email the core module teaching team. I have an admin question about arrangements for my tutorial: Contact your tutor. I have an admin question about general arrangements for my course as a whole: Email the Maths Taught Students Office (Maths.Taught.Students@leeds.ac.uk) or speak to your personal academic tutor. I have a question about the marking of my assessed work on the problem sheets: First, check your feedback on Gradescope; if you still have questions, contact your tutor. I have a question about the marking of my assessed work on the R worksheets: Come to office hours or email the core module teaching team. I have suggestion for something to cover in the lectures: Email the core module teaching team. Due to exceptional personal circumstances I require an extension on or exemption from assessed work: Email the Maths Taught Students Office; neither the core module teaching team nor your tutor are able to offer extensions or exemptions. (Only exemptions, not extensions, are available for R worksheets.) Content of MATH1710 Prerequisites The formal prerequisite for MATH1710 is “Grade B in A-level Mathematics or equivalent”. We’ll assume you have some basic school-level maths knowledge, but we don’t assume you’ve studied probability or statistics in detail before (although we recognise that many of you will have). If you have studied probability and/or statistics at A-level (or post-16 equivalent) level, you’ll recognise some of the material in this module; however you should find that we go deeper in some areas, and that we treat the material through with a greater deal of mathematical formality and rigour. “Rigour” here means precisely stating our assumptions, and carefully proving how other statements follow from those assumptions. Syllabus The module has three parts: a short first part on “exploratory data analysis”, a long middle part on probability theory, and a short final part on a statistical framework called “Bayesian statistics”. There’s also the weekly R worksheets, which you could count as a fourth part running in parallel, but which will connect with the other parts too. An outline plan of the topics covered is the following. (Remember that one section is one week’s work.) Exploratory data analysis [1 section] Summary statistics, data visualisation Probability [8 sections] Probability with events: Probability spaces, probability axioms, examples and properties of probability, “classical probability” of equally likely events, independence, conditional probability, Bayes’ theorem [3 sections] Probability with random variables: Discrete random variables, expectation and variance, binomial distribution, geometric distribution, Poisson distribution, multiple random variables, law of large numbers, continuous random variables, exponential distribution, normal distribution, central limit theorem [5 sections] Bayesian statistics [1 section]: Bayesian framework, Beta prior, normal–normal model Summary and revision [1 section] Books You can do well on this module by reading the notes and watching the videos, attending the lectures and tutorials, and working on the problem sheets and R worksheets, without needing to do any further reading beyond this. However, students can benefit from optional extra background reading or an alternative view on the material, especially in the parts of the module on probability. For exploratory data analysis, you can stick to Wikipedia, but if you really want a book, I’d recommend: GM Clarke and D Cooke, A Basic Course in Statistics, 5th edition, Edward Arnold, 2004. For the probability section, any book with a title like “Introduction to Probability” would do. Some of my favourites are: JK Blitzstein and J Hwang, Introduction to Probability, 2nd edition, CRC Press, 2019. G Grimmett and D Welsh, Probability: An Introduction, 2nd edition, Oxford University Press, 2014. (The library has online access.) SM Ross, A First Course in Probability, 10th edition, Pearson, 2020. RL Scheaffer and LJ Young, Introduction to Probability and Its Applications, 3rd edition, Cengage, 2010. D Stirzaker, Elementary Probability, 2nd edition, Cambridge University Press, 2003. (The library has online access.) On Bayesian statistics, I recommend: JV Stone, Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis, Sebtel Press, 2013. For R, there are many excellent resources online, and Google is your friend for finding them. (For all these books I’ve listed the newest editions, but older editions are usually fine too.) About these notes These notes were written by Matthew Aldridge in 2021. Editing help was provided by XXX. They are based in part on previous notes by Dr Robert G Aykroyd and Prof Wally Gilks. Dr Jason Anquandah and Dr Aykroyd advised on the R worksheets. Dr Aykroyd’s help and advice on many aspects of the module was particularly valuable. These notes (in the web format) should be accessible by screenreaders. The videos have (highly imperfect) automated subtitles. If you have accessibility difficulties with these notes, contact maths1710@leeds.ac.uk. "],["S01-eda.html", "Section 1 Exploratory data analysis 1.1 What is EDA? 1.2 What is R? 1.3 Summary statistics and boxplots 1.4 Binned data and histograms 1.5 Multiple variables and scatterplots Summary", " Section 1 Exploratory data analysis 1.1 What is EDA? Statistics is the study of data. Exploratory data analysis (or EDA, for short) is the part of statistics concerned with taking a “first look” at some data. Later, toward the end of this course, we will see more detailed and complex ways of building models for data, and in MATH1712 Probability and Statistics II (for those who take it) you will see many other statistical techniques – in particular, ways of testing formal hypotheses for data. But here we’re just interested in first impressions and brief summaries. In this section, we will concentrate on two aspects of EDA: Summary statistics: That is, calculating numbers that briefly summarise the data. A summary statistic might tell us what “central” or “typical” values of the data are, how spread out the data is, or about the relationship between two different variables. Data visualisation: Drawing a picture based on the data is an another way to show the shape (centrality and spread) of data, or the relationship between different variables. Even before calculating summary statistics or drawing a plot, however, there are other questions it is important to ask about the data: What is the data? What variables have been measured? How were they measured? How many datapoints are there? What is the possible range of responses? How was the data collected? Was data collected on the whole population or just a smaller sample? (If a sample: How was that sample chosen? Is that sample representative of the population?) How were there variable measured? Are there any outliers? “Outliers” are datapoints that seem to be very different from the other datapoints – for example, are much larger or much smaller than the others. Each outlier should be investigated to seek the reason for it. Perhaps it is a genuine-but-unusual datapoint (which is useful for understanding the extremes of the data), or perhaps there is an extraordinary explanation (a measurement or recording error, for example) meaning the data is not relevant. Once the reason for an outlier is understood, it then might be appropriate to exclude it from analysis (for example, the incorrectly recorded measurement). It’s usually bad practice to exclude an outlier merely for being an outlier before understanding what caused it. Ethical questions: Was the data collected ethically and, where necessary, with the informed consent of the subjects? Has it been stored properly? Are their privacy issues with the collection and storage of the data? What ethical issues should be considered before publishing (or not publishing) results of the analysis? Should the data be kept confidential, or should it be openly shared with other researchers for the betterment of science? 1.2 What is R? R is a programming language that is particularly good at working with probability and statistics. A convenient way to use the language R is through the program RStudio. An important part of this module is learning to use R, by completing weekly worksheets – you can read more in the R section of these notes. R can easily and quickly perform all the calculations and draw all the plots in this section of notes on exploratory data analysis. In this text, we’ll show the relevant R code. Code will appear like this: data &lt;- c(4, 7, 6, 7, 4, 5, 5) mean(data) ## [1] 5.428571 Here, the code in the first shaded box is the R commands that are typed into RStudio, which you can type in next to the &gt; arrow in the RStudio “console”. The numerical answers that R returns are shown here in the second unshaded box next to a double hashsign ##. The [1] can be ignored (this is just R’s way of saying that this is the first part of the answer – but all the answers here only have one part anyway). Plots produced by R are displayed here as pictures. Most importantly for now, you are not expected to understand the R code in this section yet. The code is included so that, in the future, as you work through the R worksheets week by week, you can look back at the code in the section, and it will start to make sense. By the time you have finished R Worksheet 5 in week 5, you should be able understand most of the R code in this section. 1.3 Summary statistics and boxplots Suppose we have collected some data on a certain variable. We will assume here that we have \\(n\\) datapoints, each of which is a single real number. We can write this data as a vector \\[ \\mathbf x = (x_1, x_2, \\dots, x_n) . \\] A statistic is a calculation from the data \\(\\mathbf x\\), which is (usually) also a real number. In this section we will look at two types of “summary statistics”, which are statistics that we feel will give us useful information about the data. We’ll look here at two types of summary statistic: Measures of centrality, which tell us where the “middle” of the data is. Measures of spread, which tell us how far the data typically spreads out from that middle. Some measures of centrality are the following. Definition 1.1 Consider some real-valued data \\(\\mathbf x = (x_1, x_2, \\dots, x_n)\\). The mode is the most common value of \\(x_i\\). (If there are multiple joint-most common values, they are all modes.) Suppose the data is ordered as \\(x_1 \\leq x_2 \\leq \\cdots \\leq x_n\\). Then the median is the central value in the ordered list. If \\(n\\) is odd, this is \\(x_{(n+1)/2}\\); if \\(n\\) is even, we normally take halfway between the two central points, \\(\\frac12(x_{n/2}+x_{(n+1)/2})\\). The mean \\(\\bar x\\) is \\[ \\bar x = \\frac{1}{n}(x_1 + x_2 + \\cdots + x_n) = \\frac1n \\sum_{i=1}^n x_i . \\] (In that last expression, we’ve made use of Sigma notation to write down the sum.) Example 1.1 Some packets of Skittles (a small fruit-flavoured sweet) were opened, and the number of Skittles in each packet counted. There were 13 packets, and the number of sweets (sorted from smallest to largest) were: \\[ 59, \\ 59, \\ 59, \\ 59, \\ 60, \\ 60, \\ 60, \\ 61, \\ 62, \\ 62, \\ 62, \\ 63, \\ 63 .\\] The mode is 59, because there were 4 packets containing 59 sweets; more than any other number. Since there are \\(n = 13\\) packets, the middle packet is number \\(i = 7\\), so the median is \\(x_7 = 60\\). The mean is \\[ \\bar x = \\frac{1}{13} (59 + 59 + \\cdots + 63) = \\frac{789}{13} = 60.7 .\\] The median is one example of a “quantile” of the data. Suppose our data is increasing order again. For \\(0 \\leq \\alpha \\leq 1\\), the \\(\\alpha\\)-quantile \\(q(\\alpha)\\) of the data is the datapoint \\(\\alpha\\) of the way along the list. So the median is the \\(\\frac12\\)-quantile \\(q(\\frac12)\\), the minimum is the 0-quantile \\(q(0)\\), and the maximum is the 1-quantile \\(q(1)\\). Generally, \\(q(\\alpha)\\) is equal to \\(x_{1+\\alpha(n-1)}\\) when \\(1+\\alpha(n-1)\\) is an integer. (If \\(1+\\alpha(n-1)\\) isn’t an integer, there are various conventions of how to choose that we won’t go into here. R has seven different settings for choosing quantiles! – we will always just use R’s default choice.) Two other common terms: \\(q(\\frac34)\\) is called the upper quartile and \\(q(\\frac14)\\) is called the lower quartile (note “quartile” – as in “quarter” – not “quantile”, here). The upper and lower quartiles of the \\(n = 13\\) Skittles packets are the \\(q(\\frac14) = x_4 = 59\\) and \\(x_{10} = 62\\). Some measures of spread are: Definition 1.2 The number of distinct observations is precisely that: the number of different datapoints you have after removing any repeats. The interquartile range is the difference between the upper and lower quartiles \\(\\text{IQR} = q(\\frac34) - q(\\frac14)\\). The sample variance is \\[ s^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2 , \\] where \\(\\bar x\\) is the sample mean from before. The standard deviation \\(s_x = \\sqrt{s^2_x}\\) is the square-root of the sample variance. The formula we’ve given for sample variance is sometimes called the “definitional formula”, as it’s the formula used to define the sample variance. We can rearrange that formula as follows: \\[\\begin{align*} s^2_x &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^2 - 2x_i\\bar x + \\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\left(\\sum_{i=1}^nx_i^2 - \\sum_{i=1}^n 2x_i\\bar x + \\sum_{i=1}^n\\bar x^2 \\right) \\\\ &amp;= \\frac{1}{n-1} \\left(\\sum_{i=1}^n x_i^2 - 2\\bar x \\sum_{i=1}^n x_i + \\bar x^2 \\sum_{i=1}^n 1 \\right) \\\\ &amp;= \\frac{1}{n-1} \\left(\\sum_{i=1}^n x_i^2 - 2n\\bar x^2 + n\\bar x^2 \\right) \\\\ &amp;= \\frac{1}{n-1} \\left(\\sum_{i=1}^n x_i^2 - n\\bar x^2 \\right) . \\end{align*}\\] Here, the first line is the definitional formula; the second line is from expanding out the bracket; the third line is taking the sum term-by-term; the fourth line takes any constants (things not involving \\(i\\)) outside the sums; the fifth line uses \\(\\sum_{i=1}^n x_i = n\\bar x\\), from the definition of the mean, and \\(\\sum_{i=1}^n 1 = 1 + 1 + \\cdots 1 = n\\); and the sixth line simplifies the final two terms. This has left us with \\[ s^2_x = \\frac{1}{n-1} \\left(\\sum_{i=1}^n x_i^2 - n\\bar x^2 \\right) . \\] This is sometimes called the “computational formula”; this is because it’s usually more convenient to calculate the sample variance using this formula rather than the definitional formula. The following R code reads in some data which has the daily average temperature in Leeds in 2020, divided into months. We can find, for example, the mean October temperature or the sample variance of the July temperature. temperature &lt;- read.csv(&quot;https://mpaldridge.github.io/math1710/data/temperature.csv&quot;) jul &lt;- temperature[temperature$month == &quot;jul&quot;, ] oct &lt;- temperature[temperature$month == &quot;oct&quot;, ] mean(oct$temp) ## [1] 11.93548 var(jul$temp) ## [1] 12.03226 A boxplot is a useful way to illustrate data. It can be easier to tell the difference between different data sets “by eye” when looking at a boxplot, rather than examining raw summary statistics. A boxplot is drawn as follows: The vertical axis represents the data values. Draw a box from the lower quartile \\(q(\\frac14)\\) to the median \\(q(\\frac12)\\). Draw another box on top of this from the median \\(q(\\frac12)\\) to the upper quartile \\(q(\\frac34)\\). Note that size of these two boxes put together is the interquartile range. Decide which datapoints are outliers, and plot these with circles. (The R default is that any data point less than \\(q(\\frac14) - 1.5 \\times \\text{IQR}\\) or greater than \\(q(\\frac34) + 1.5 \\times \\text{IQR}\\) is an outlier.) Out from the two previous boxes, draw “whiskers” to the smallest and largest non-outlier datapoints. Here are two boxplots from the July and October temperature data. What do you conclude about the data from these boxplots? boxplot(jul$temp, oct$temp, names = c(&quot;July&quot;, &quot;October&quot;), ylab = &quot;Daily maximum temperature (degrees C) in Leeds&quot;) (And yes, I did check the outlier to make sure it was a genuine datapoint.) 1.4 Binned data and histograms Often when collecting data, we don’t collect exact data, but rather collect data clumped into “bins”. For example, suppose a student wished to use a questionnaire to collect data on how long it takes people to reach campus from home; they might not ask “Exactly how long does it take?”, but rather give a choice of tick boxes: “0–5 minutes”, “5–10 minutes”, and so on. Consider the following binned data, from \\(n = 100\\) students: Time Frequency Relative frequency 0–5 minutes 4 0.04 5–10 minutes 8 0.08 10–15 minutes 21 0.21 15–30 minutes 42 0.42 30–45 minutes 15 0.15 45–60 minutes 8 0.08 60–120 minutes 2 0.02 Total 100 1 Here the frequency \\(f_j\\) of bin \\(j\\) is simply the number of observations in that bin; so, for example, 42 students had journey lengths of between 15 and 30 minutes. The relative frequency of bin \\(j\\) is \\(f_j/n\\); that is, the proportion of the observations in that bin. What is the median journey length? Well, we don’t know exactly, but \\(0.04 + 0.08 + 0.21\\) is less than 0.5, while \\(0.04 + 0.08 + 0.21 + 0.42\\) is greater than 0.5. So we know that the median student is in the “10–15 minute” bin, and we can say that the median journey length is between 10 and 15 minutes. What about the mode? The bin with the most observations in it is the “15–30 minute” bin. But this bin covers 15 minutes, while some of the other bins only cover 5 minutes. It would be a fairer comparison to look at the frequency density: the relative frequency divided by the size of the bin. Time Frequency Relative frequency Frequency density 0–5 minutes 4 0.04 0.008 5–10 minutes 8 0.08 0.016 10–15 minutes 21 0.21 0.042 15–30 minutes 42 0.42 0.028 30–45 minutes 15 0.15 0.010 45–60 minutes 8 0.08 0.005 60–120 minutes 2 0.02 0.0003 Total 100 1 In the first row, for example, the relative frequency is 0.04 and the size of the bin is 5 minutes, so the frequency density is \\(0.04/5 = 0.008\\). So the modal bin – the bin with the highest frequency density – is in fact the “10–15 minutes” bin. Since we don’t have the exact data, it’s not possible to exactly calculate the mean and variance. However, we can often get a good estimate by assuming that each observation was in fact right in the centre of its bin. So, for example, we can assume that all 4 observations in the “0–5 minutes” bin were journeys of exactly 2.5 minutes. Of course, this isn’t true (or is highly unlikely to be true), but we can often get a good approximation this way. For our journey-time data, our approximation of the mean would be \\[ \\bar x = \\frac{1}{100} \\big(4\\times 2.5 + 8 \\times 7.5 + \\cdots + 2\\times90) = 24.4 . \\] More generally, if \\(m_j\\) is the midpoint of bin \\(j\\) and \\(f_j\\) its frequency, then we can calculate the binned mean and binned variance by \\[\\begin{align*} \\bar x &amp;= \\frac{1}{n} \\sum_j f_j m_j \\\\ s^2_x &amp;= \\frac{1}{n-1} \\sum_j f_j (m_j - \\bar x)^2 \\end{align*}\\] Data in bins can be illustrated with a histogram. A histogram has the measurement on the x-axis, with one bar across the width of each bin, with bars drawn up to the height of the corresponding frequency density. Note that this means that the area of the bar is exactly the relative frequency of the corresponding bin. (If all the bins are the same width, frequency density is directly proportional to frequency and to relative frequency, so it can be clearer use one of those as the y-axis instead.) Here is a histogram for our journey-time data: journeys &lt;- read.csv(&quot;https://mpaldridge.github.io/math1710/data/journeys.csv&quot;) bins &lt;- c(0, 5, 10, 15, 30, 45, 60, 120) hist(journeys$midpoint, breaks = bins, xlab = &quot;Journey length (min)&quot;, main = &quot;&quot;) Often we draw histograms because the data was collected in bins. But even when we have exact data, we might choose to divide it into bins for the purposes of drawing a histogram. In this case we have to decide where to put the “breaks” between the bins. Too many breaks too close together, and the small number of observations in each bin will give “noisy” results (see left); too few breaks too far apart, and the histogram will lose detail (see right). hist_data &lt;- c(rnorm(30, 8, 2), rnorm(40, 12, 3)) # Some fake data hist(hist_data, breaks = 40, main = &quot;Too many bins&quot;) hist(hist_data, breaks = 3, main = &quot;Too few bins&quot;) 1.5 Multiple variables and scatterplots Often, more than one piece of data is collected from each subject, and we wish to compare that data, to see if there is a relationship between the variables. For example, we could take \\(n\\) second-year maths students, and for each student \\(i\\), collect their mark \\(x_i\\) in MATH1710 and their mark \\(y_i\\) in MATH1712. This gives is two “paired” datasets, \\(\\mathbf x = (x_1, x_2, \\dots, x_n)\\) and \\(\\mathbf y = (y_1, y_2, \\dots, y_n)\\). We can calculate sample statistics of draw plots for \\(\\mathbf x\\) and for \\(\\mathbf y\\) individually. But we might also want to see if there is a relationship between \\(\\mathbf x\\) and \\(\\mathbf y\\): Do students with high marks in MATH1710 also get high marks in MATH1712? A good way to visualise the relationship between two variables is to use a scatterplot. In a scatterplot, the \\(i\\)th data pair \\((x_i, y_i)\\) is illustrated with a mark (such as a circle or cross) whose x-coordinate has the value \\(x_i\\) and whose y-coordinate has the value \\(y_i\\). In the following scatterplot, we have \\(n = 50\\) datapoints for the 50 US states; for each state \\(i\\), \\(x_i\\) is the Republican share of the vote in that state in the 2016 Trump–Clinton presidential election, and \\(y_i\\) is the Republican share of the vote in that state in the 2020 Trump–Biden election. elections &lt;- read.csv(&quot;https://mpaldridge.github.io/math1710/data/elections.csv&quot;) plot(elections$X2016, elections$X2020, col = &quot;blue&quot;, xlab = &quot;Republican share of the two-party vote, 2016 (%)&quot;, ylab = &quot;Republican share of the two-party vote, 2020 (%)&quot;) abline(h = 50, col = &quot;grey&quot;) abline(v = 50, col = &quot;grey&quot;) abline(0.195, 0.963, col = &quot;red&quot;) We see that there is a strong relationship between \\(\\mathbf x\\) and \\(\\mathbf y\\), with high values of \\(x\\) corresponding to high values of \\(y\\) and vice versa. Further, the points on the scatterplot lie very close to a straight line. A useful summary statistic here is the correlation \\[ r_{xy} = \\frac{s_{xy}}{s_x s_y} , \\] where \\(s_{xy}\\) is the sample covariance \\[ s_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) , \\] and \\(s_x\\) and \\(s_y\\) are the standard deviations. The correlation \\(r_{xy}\\) is always between \\(-1\\) and \\(+1\\). Values of \\(r_{xy}\\) near \\(+1\\) indicate that the scatterpoints are close to a straight line with an upward slope (big \\(x\\) = big \\(y\\)); values of \\(r_{xy}\\) near \\(-1\\) indicate that the scatterpoints are close to a straight line with a downward slope (big \\(x\\) = small \\(y\\)); and values of \\(r_{xy}\\) near 0 indicate that there is a weak linear relationship between \\(x\\) and \\(y\\). For the elections data, the correlation is cor(elections$X2016, elections$X2020) ## [1] 0.9919659 which, as we expected, is extremely high. Summary Exploratory data analysis is about taking a first look at data. Summary statistics are numbers calculated from data that give us useful information about the data. Summary statistics that measure the centre of the data include the mode, median, and mean. Summary statistics that measure the spread of the data include the number of distinct outcomes, the interquartile range, and the sample variance. A summary statistic that measures the linear relationship between two variables is the correlation. Boxplots, histograms, and scatterplots are useful ways of visualising data. "],["P1.html", "Problem Sheet 1 A: Short questions B: Long questions C: Assessed questions Solutions to short questions", " Problem Sheet 1 This is Problem Sheet 1, which covers material from Section 1 of the notes. You should work through all the questions on this problem sheet during Week 1, in preparation for your tutorial in Week 2. Questions C1 and C2 are assessed questions, and are due in by 2pm on Monday 11 October. I recommend spending about 3 hours on this problem sheet in Week 1, plus 1 extra hour in Week 2 to neatly write up and submit your answers to the assessed questions. A: Short questions The first two questions are short questions, which are intended to be mostly not too difficult. Short questions usually follow directly from the material in the notes. Here, your should clearly state your final answer, and give enough working-out (or a short written explanation) for it to be clear how you reached that answer. You can check your answers with the solutions-without-working at the bottom of this sheet; solutions-with-working will be available later. If you get stuck on any of these questions, you might want to ask for guidance in your tutorial. A1. Consider again the “number of Skittles in each packet” data from Example 1.1. \\[ 59, \\ 59, \\ 59, \\ 59, \\ 60, \\ 60, \\ 60, \\ 61, \\ 62, \\ 62, \\ 62, \\ 63, \\ 63 .\\] (a) Calculate the mean number of Skittles in each packet. (b) Calculate the sample variance using the computational formula. (c) Calculate the sample variance using the definitional formula. (d) Out of (b) and (c), which calculation did you find easier, and why? A2. Consider the following data sets of the age of elected politicians on a local council. (The “18–30” consists of people older than and including 18, and younger than but not including 30.) Age (years) Frequency Relative frequency Frequency density 18–30 1 30–40 3 40–45 4 45–50 5 50–55 3 55–60 1 60–70 3 Total 20 1 — (a) Complete the table by filling in the relative frequency and frequency densities. (b) What is the median age bin? (c) Calculate (an approximation of) the mean age of the politicians. B: Long questions The next four questions are long questions, which are intended to be harder. Long questions often require you to think originally for yourself, not just directly follow procedures from the notes. Here, your answers should be written in complete sentences, and you should carefully explain in words each step of your working. Your answers to these questions – not only their mathematical content, but also how to clearly write good solutions – are likely to be the main topic for discussion in your tutorial. B1. For each of the two datasets below, calculate the following summary statistics, or explain why it is not possible to do so: mode; median; mean; number of distinct outcomes; inter-quartile range; and sample variance. (a) Six packets of Skittles are opened together, and the total number of sweets of each colour is: Colour Red Orange Yellow Green Purple Number of Skittles 67 71 87 74 62 (b) Shirt sizes for a university football squad: Colour Xtra Small Small Medium Large Xtra Large Number of Skittles 0 1 6 5 4 B2. A summary statistic is informally said to be “robust” if it typically doesn’t change much if a small number of outliers are introduced to a large dataset, or “sensitive” if it often changes a lot when a small number of outliers are introduced. Briefly discuss the robustness or sensitivity of the following summary statistics: (a) mode; (b) median; (c) mean; (d) number of distinct outcomes; (e) inter-quartile range; and (f) sample variance. B3. Let \\(\\mathbf a = (a_1, a_2, \\dots a_n)\\) and \\(\\mathbf b = (b_1, b_2, \\dots, b_n)\\) be two real-valued vectors of the same length. Then the Cauchy–Schwarz inequality says that \\[ \\left( \\sum_{i=1}^n a_i b_i \\right)^2 \\leq \\left( \\sum_{i=1}^n a_i^2 \\right) \\left(\\sum_{i=1}^n b_i^2 \\right) . \\] Use the Cauchy–Schwarz inequality to show that the correlation \\(r_{xy}\\) satisfies \\(-1 \\leq r_{xy} \\leq 1\\). (Hint: Try to prove that \\(s_{xy}^2 \\leq s_x^2 s_y^2\\). How does this help?) B4. A researcher wishes to study the effect of mental health on academic achievement. The researcher will collect data on the mental health of a cohort of students by asking them to fill in a questionnaire, and will measure academic achievement via the students’ scores on their university exams. Discuss some of the ethical issues associated with the collection, storage, and analysis of this data, and with the publication of the results of the analysis. Are there ways to mitigate these issues? (It’s not necessary to write an essay for this question – a few short bulletpoints will suffice. There may be an opportunity to discuss these issues in more detail in your tutorial.) C: Assessed questions The last two questions are assessed questions. This means you will submit your answers, and your answers will be marked by your tutor. These two questions count for 3% of your final mark for this module. If you get stuck, your tutor may be willing to give you a hint in your tutorial. The deadline for submitting your solutions is 2pm on Monday 11 October at the beginning of Week 3, although I strongly recommend completing and submitting your work during Week 2. Submission will be via Gradescope; submission will open on Monday 4 October. You should submit your answers as a single PDF file. Most students choose to hand-write their work, then scan it to PDF using their phone; if you do this, you should use a proper scanning app (like Microsoft Lens or Adobe Scan) – please do not just submit photographs. We will discuss Gradescope submission further in the Week 2 lectures. Your work will be marked by your tutor and returned on Monday 18 September, when solutions will also be made available. Question C1 is a “short question”, where brief explanations or working are sufficient; Question C2 is a “long question”, where the marks are not only for mathematical accuracy but also for the clarity and completeness of your explanations. You should not collaborate with others on the assessed questions: your answers must represent solely your own work. The University’s rules on academic integrity – and the related punishments for violating them – apply to your work on the assessed questions. C1. The monthly average exchange rate for US dollars into British pounds over a 12-month period was: \\[\\begin{gather*} 1.306, \\ 1.301, \\ 1.290, \\ 1.266, \\ 1.290, \\ 1.302,\\\\ 1.317, \\ 1.304, \\ 1.284, \\ 1.268, \\ 1.247, \\ 1.215. \\end{gather*}\\] (a) Calculate the median for this data. (b) Calculate the mean for this data. (c) Calculate the sample variance for this data. (d) Is the mode an appropriate summary statistic for this data? Why/why not? C2.  (a) Prove the following computational formula for the sample covariance: \\[ s_{xy} = \\frac{1}{n-1} \\left( \\sum_{i=1}^n x_iy_i - n\\bar x \\bar y \\right). \\] (b) Suppose that a dataset \\(\\mathbf x = (x_1, x_2, \\dots, x_n)\\) (with \\(n \\geq 2\\)) has sample variance \\(s_x^2 = 0\\). Show that all the datapoints are in fact equal. Solutions to short questions A1. (a) 60.7 (b) &amp; (c) 2.40 (d) — A2. (a) — (b) 45–50 (c) 47.3 "],["R.html", "R Worksheets R worksheets About R and RStudio How to access R and RStudio Installing R and RStudio Troubleshooting drop-in sessions", " R Worksheets R worksheets Each week there will be an R worksheet to work through in your own time. We recommend spending about one hour on each worksheet, plus one extra hour for worksheets with assessed questions, for checking through and submitting your solutions. Week Worksheet Deadline for assessed work 1 R basics — 2 Working with vectors — 3 Importing data into R Monday 18 October 4 Plots I: Making plots — 5 Plots II: Making plots nicer Monday 1 November 6 RMarkdown (optional) — 7 Discrete random variables Monday 15 November 8 Discrete distributions — 9 Normal distribution Monday 29 November 10 Law of large numbers — 11 Summary To be confirmed About R and RStudio R is a programming language that is particularly good at working with probability and statistics. R is very widely used in universities and increasingly widely used in industry. Learning to use R is a mandatory part of this module, and exercises requiring use of R make up at least 15% of your module mark. Many other statistics-related course at the University also use R. RStudio is a program that gives a convenient way to work with the language R. RStudio is the most common way to use the language R, and learning to use RStudio is strongly recommended. R and RStudio are free/open-source software. How to access R and RStudio There are a number of ways you can access R and RStudio: All University computers have R and RStudio already installed. Here is a directory of the University’s computer clusters. You can install R and RStudio on your own computer – see the instructions below. If you want to use R/RStudio on a non-University device for which you don’t have admin/installation rights (Chromebook, iPad, friend’s laptop, etc), you could try: You can use the University’s copies of R/RStudio virtually through the Windows Virtual Desktop or AppsAnywhere client. The RStudio Cloud is a cloud-hosted “Google Docs for R” that you can use through your web browser – you can get 25 hours per month for free (or pay for more). Installing R and RStudio Students who have their own computer usually find it most convenient to install R and RStudio on that computer. To do this, it’s important that you install R (the programming language) first, and only install RStudio (the program to use R) once R has already been installed. First, install R. Go to the Comprehensive R Archive Network and follow the instructions: Windows: Click “Download R for Windows”, then “Install R for the first time”. The main link at the top should be to download the most recent version of R. Mac: Click Download R for macOS, and then download the relevant PKG file. (For pre-November 2020 Intel-based Macbooks, you must use the “Intel 64-bit build”; for post-November 2020 M1-based “Apple silicon” Macbooks, the “Apple silicon arm64 build” may be faster.) After R is installed, then install RStudio. Go to the Download page at RStudio.com and follow the instructions. You want “RStudio Desktop”, and you want the free version. If you have difficulty installing R, come along to the first computational drop-in session in Week 2 and bring your computer with you (if it’s sufficiently portable), and we’ll do our best to help. Troubleshooting drop-in sessions You will learn to use R by working through the R Worksheets. Learning to use a programming language is different from learning mathematics: you should expect to regularly get frustrated and annoyed when the computer seems to refuse to do what you want it to (but also occasionally experience the joy of getting it right!). This is a normal part of learning. However, many students find getting with started with R in the first few weeks particularly frustrating. Also, sometimes students have problems installing R and RStudio on their own computers. To help with this, we have organised optional troubleshooting drop-in sessions in Weeks 2 and 3. Check your timetable for details – they are probably listed as “computer practicals”. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
