::::: {.myq}
**B1.** Calculate the CDF $F(x) = \mathbb P(X \leq x)$ of the geometric distribution...

:::: {.subq}
**(a)** ...by summing the PMF;

::: {.myanswers data-latex=""}
*Solution.*
We have,using the standard formula for the sum of a finite geometric progression, 
\begin{align*}
F(x) &= \sum_{y = 1}^x p(y) \\
&= \sum_{y = 1}^x (1-p)^{y-1} p \\
&= \frac{p\big(1 - (1-p)^x\big)}{1 - (1-p)} \\
&= \frac{p\big(1 - (1-p)^x\big)}{p} \\
&= 1 - (1 - p)^x .
\end{align*}
:::
::::

:::: {.subq}
**(b)** ...by explaining how the "number of trials until success" definition tells us what $1 - F(x) = \mathbb P(X > x)$ must be.

::: {.myanswers data-latex=""}
*Solution.*
Note that $1 - F(x) = \mathbb P(X > x)$ is precisely the probability that the first $x$ trials are failures, and hence that the first success comes strictly after the $x$th trial. The probability that the first $x$ trials are failures is $(1-p)^x$. So $F(x) = 1 - (1-p)^x$.
:::
::::

:::: {.subq}
**(c)** A gambler rolls a pair of dice until he gets a double-six. What is the probability that this takes between 20 and 40 double-rolls?

::: {.myanswers data-latex=""}
*Solution.*
Let $X \sim \text{Geom}(\frac{1}{36})$. Then
\begin{align*}
\mathbb P(20 \leq X \leq 40) &= \mathbb P(X \leq 40) - \mathbb P(X \leq 19) \\
  &= F(40) - F(19) \\
  &= \bigg(1 - \big(1 - \tfrac{1}{36})^{40}\bigg) - \bigg(1 - \big(1 - \tfrac{1}{36})^{19}\bigg) \\
  &= 0.676 - 0.414 \\
  &= 0.261.
\end{align*}
:::
::::
:::::

::::: {.myq}
**B2.** Let $Y$ be a geometric distribution with parameter $p$ according to the alternative "number of failures *before* the first success" definition.

:::: {.subq}
**(a)** Write down the PMF for $Y$.

::: {.myanswers data-latex=""}
*Solution.* Having $Y = y$ requires $y$ consecutive failures immediately followed by a success. So $p_Y(y) = (1-p)^y p$.
:::
::::

:::: {.subq}
**(b)** Calculate the expectation and variance of $Y$. You may use without proof the fact that for a standard "number of trials up to and including the first success" geometric distribution we have $\mathbb EX = 1/p$ and $\Var(X) = (1-p)/p^2$.

::: {.myanswers data-latex=""}
*Solution.* If $X \sim \text{Geom}(p)$ under the standard definition, then (as we saw in the notes) $Y$ has the same distribution as $X -1$. Therefore,
\[ \mathbb EY = \mathbb E(X-1) = \mathbb EX - 1 = \frac{1}{p} - 1 = \frac{1-p}{p} \]
and
\[ \Var(Y) = \Var(X -1) = \Var(X) = \frac{1-p}{p^2} .  \]
:::
::::
:::::

::::: {.myq}
**B3** Let $X \sim \text{Po}(\lambda)$.

:::: {.subq}
**(a)** Show that $\mathbb EX(X-1) = \lambda^2$. You may use the Taylor series for the exponential,
\[ \mathrm{e}^\lambda = \sum_{y=0}^\infty \frac{\lambda^y}{y!} . \]

::: {.myanswers data-latex=""}
*Solution.*
We follow exactly the method used to calculate $\mathbb EX$ in the notes. We have
\begin{align*}
\mathbb EX(X-1) &= \sum_{x=0}^\infty x(x-1)\, \mathrm e^{-\lambda} \frac{\lambda^x}{x!} \\
  &= \lambda^2 \mathrm e^{-\lambda} \sum_{x=2}^\infty \frac{\lambda^{x-2}}{(x - 2)!} \\
  &= \lambda^2 \mathrm e^{-\lambda}\sum_{y=0}^\infty  \frac{\lambda^y}{y!} \\
  &= \lambda^2 \mathrm e^{-\lambda} \, \mathrm e^{\lambda} \\
  &= \lambda^2  .
\end{align*}
In the second line, we took a $\lambda^2$ and a $\mathrm e^{-\lambda}$ outside the brackets; cancelled the $x$ and $x-1$ out of the $x!$; and removed the $x = 0$ and $x = 1$ terms from the sum, since they were 0 anyway. In the third line, we re-indexed the sum by setting $y = x - 2$. In the fourth line, we used the Taylor series for the exponential
:::
::::

:::: {.subq}
**(b)** Hence show that $\Var(X) = \lambda$. You may use the fact, proved in the notes, that $\mathbb EX = \lambda$.

::: {.myanswers data-latex=""}
*Solution.* 
We know from part (a) that 
\[ \mathbb EX(X-1) = \mathbb E(X^2 - X) = \mathbb EX^2 - \mathbb EX = \mathbb EX^2 - \lambda = \lambda^2 ,\]
which gives $\mathbb EX^2 = \lambda^2 + \lambda$. We can then use the computational formula for the variance to get
\[ \Var(X) = \mathbb EX^2 - \lambda^2 = \lambda^2 + \lambda - \lambda^2 = \lambda .\]
:::
::::
:::::

:::: {.myq}
**B4.** Each week in the UK about 15 million Lotto tickets are sold. As we saw in Section 3, the probability of each ticket winning is about 1 in 45 million. Estimate the proportion of weeks when there is **(a)** a roll-over (no jackpot winners), **(b)** a unique jackpot winner, or **(c)** when multiple winners share the jackpot. State any modelling assumptions you make and the approximation that you use.

::: {.myanswers data-latex=""}
*Solution.*
We assume that each ticket is uniformly randomly chosen from all possible tickets, independent of all other tickets. Then the number of winners is $X \sim \text{Bin}(15 \text{ million}, 1/(45 \text{ million}))$.
It will be convenient to use a Poisson approximation with rate
\[ \lambda = 15 \text{ million} \times \frac{1}{45 \text{ million}} = \tfrac13 .  \]

The probability there is a roll-over is 
\[ \mathbb P(X = 0) \approx \mathrm e^{-1/3} = 0.72 . \]
The probability there is a unique jackpot winner is
\[ \mathbb P(X = 1) \approx \tfrac13 \mathrm e^{-1/3} = 0.24 . \]
The probability there are multiple winners is
\[ \mathbb P(X \geq 2) = 1 - \mathbb P(X = 0) - \mathbb P(X = 1) = 0.04  . \]
:::
::::

::::: {.myq}
**B5.**  Let $X_1, X_2, \dots, X_n$ be IID random variable with common expectation $\mu$ and common variance $\sigma^2$, and let $\overline X = (X_1 + \cdots + X_n)/n$ be the mean of these random variables. We will be considering the random variable $S^2$ given by
\[ S^2 = \sum_{i=1}^n (X_i - \overline X)^2 . \]

:::: {.subq}
**(a)** By writing 
\[ X_i - \overline X = (X_i - \mu) - (\overline X - \mu)  \]
or otherwise, show that
\[ S^2 = \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2 . \]

::: {.myanswers data-latex=""}
*Solution.*
Using the suggestion in the question, we have
\begin{align*}
S^2 &= \sum_{i=1}^n (X_i - \overline X)^2 \\
  &= \sum_{i=1}^n \big( (X_i - \mu) - (\overline X - \mu)  \big)^2 \\
  &= \sum_{i=1}^n \big( (X_i - \mu)^2 - 2(X_i - \mu)(\overline X - \mu) + (\overline X - \mu)^2\big) \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - \sum_{i=1}^n 2(X_i - \mu)(\overline X - \mu) + \sum_{i=1}^n (\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2\left(\sum_{i=1}^n X_i - \sum_{i=1}^n \mu\right)(\overline X - \mu)  + (\overline X - \mu)^2 \sum_{i=1}^n 1 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2(n\overline X - n\mu) (\overline X - \mu) + n (\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2n(\overline X - \mu)^2 + n(\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2 .
\end{align*}
This is mostly manipulation of sums as we have seen before, although note that going from the fifth to sixth lines we use the definition of $\overline X$ to write $\sum_{i=1}^n X_i$ as $n \overline X$.
:::
::::

:::: {.subq}
**(b)** Hence or otherwise, show that
\[ \mathbb E S^2 = (n - 1)\sigma^2 . \]
You may use facts about $\overline X$ from the notes provided you state them clearly. (You may find it helpful to recognise some expectations as definitional formulas for variances, where appropriate.) <!--You might also find it useful to recall that if a random variable $Y$ has expectation $\mathbb EY = 0$, then $\mathbb EY^2 = \Var(Y)$.-->

::: {.myanswers data-latex=""}
*Solution.* Starting with the linearity of expectation, we have
\begin{align*}
\mathbb ES^2 &= \mathbb E \left( \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2  \right) \\
  &= \sum_{i=1}^n \mathbb E (X_i - \mu)^2 - n \mathbb E(\overline X - \mu)^2 \\
  &= \sum_{i=1}^n \Var(X_i) - n \Var(\overline X) .
\end{align*}
The last line follows because $\mathbb EX_i = \mu$ for all $i$ by assumption, and we showed in the notes that $\mathbb E \overline X = \mu$ also; hence, as hinted, the expectations are precisely definitional formulas for the variances. We then also know that $\Var(X_i) = \sigma^2$ by assumption, and we showed in the notes that $\Var(\overline X) = \sigma^2/n$. Hence
\[ \mathbb ES^2 = \sum_{i=1}^n \sigma^2 - n\, \frac{\sigma^2}{n} = n \sigma^2 - \sigma^2 = (n-1)\sigma^2, \]
as required.
:::
::::

:::: {.subq}
**(c)** At the beginning of this course, we defined the sample variance of the values $x_1, x_2, \dots, x_n$ to be
\[ s^2_x = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 . \]
Explain one reason why we might consider it appropriate to use $1/(n-1)$ as the factor at the beginning of this expression, rather than simply $1/n$.

::: {.myanswers data-latex=""}
*Solution.* 
We often model a data set $x_2, x_2, \dots, x_n$ as being realisations of an IID sequence of random variables $X_1, X_2, \dots, X_n$. In this case, we are using the summary statistic of the sample variance $s_x^2$ to "estimate" the variance $\Var(X_1) = \sigma^2$. Using the factor $1/(n-1)$ ensures that this estimator is correct "in expectation", because
\[ \mathbb E s_X^2 = \mathbb E \frac{1}{n-1}S^2 = \frac{1}{n-1} \mathbb ES^2 = \frac{1}{n-1}(n-1)\sigma^2 = \sigma^2 . \]
This property of being correct in expectation is called being an "unbiased" estimator, and its usually considered beneficial for an estimator to be unbiased.

Note that we already know that the sample mean $\bar x$ is an unbiased estimator for the expectation $\mathbb EX = \mu$, as we already know that $\mathbb E\overline X = \mu$.
:::
::::
:::::