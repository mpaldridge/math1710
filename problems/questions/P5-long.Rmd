::::: {.myq}
**B1.** 
**(a)** Let $X \sim \text{Exp}(\lambda)$. Show that
\[ \mathbb P(X > x + y \mid X > y) = \mathbb P(X > x) . \]

:::: {.subq}
::: {.myanswers data-latex=""}
*Solution.*  
Using the definition of conditional probability, we have
\[\mathbb P(X > x + y \mid X > y) = \frac{\mathbb P(X > x + y \text{ and } X > y) }{\mathbb P(X > y)}= \frac{\mathbb P(X > x + y ) }{\mathbb P(X > y)} , \]
since is $X > x + y$ then we automatically have $X > y$.
Note also that, for an exponential distribution we have
\[ \mathbb P(X > x) = 1 - F(x) = 1 - (1 - \mathrm e^{-\lambda x}) = \mathrm e^{-\lambda x} . \]
So the left-hand side of the statement in the question is
\[ \frac{\mathrm{e}^{-\lambda(x + y)}}{\mathrm{e}^{-\lambda y}} = \mathrm e^{-\lambda x - \lambda y + \lambda y} = \mathrm{e}^{-\lambda x} , \]
which equals the right-hand side, by the above.
:::
:::

:::: {.subq}
**(b)** The result proved in part (a) is called the "memoryless property". Why do you think it's called that?

::: {.myanswers data-latex=""}
*Solution.* Think of $X$ as a waiting time. The result tells us that, given that we've already waited $y$ minutes, the probability that we have to wait at least another $x$ minutes is exactly the same as the probability we had to wait at least $x$ minutes starting from the beginning. In other words, *no matter when we start timing from*, the probability we have to wait more than $x$ minutes remains the same.

This is called the "memoryless property" because it's as if the process has no memory of how long we've already been waiting for.

(This property also holds for the geometric distribution. The expected number of rolls of a dice until you get a six is always 6 rolls, no matter how many times you've already rolled the dice.)
:::
::::

:::: {.subq}
**(c)** When you get to certain bus stop, the average amount of time you have to wait for a bus to arrive is 20 minutes. Specifically, the time until the next bus arrives is modelled as an exponential distribution with expectation $1/\lambda = 20$ minutes. Suppose you have already been waiting at the bus stop for 15 minutes. What is the expected further amount of time you still have to wait for a bus to arrive?

::: {.myanswers data-latex=""}
*Solution.*
By the memoryless property, it's irrelevant how long we've been waiting for: the average time until a bus arrives is always $1/\lambda = 20$ minutes.
:::
::::
:::::

::::: {.myq}
**B2.** The main dangerous radioactive material left over after the Chernobyl disaster is Caesium-137. The amount of time it takes a Caesium-137 particle to decay is known to follow an exponential distribution with rate $\lambda = 0.023$ years^-1^.

:::: {.subq}
**(a)** What is the average amount of time it takes a Caesium-137 particle to decay?

::: {.myanswers data-latex=""}
*Solution.* The expectation is $1/\lambda = 43.5$ years.
:::
::::

:::: {.subq}
**(b)** The "half-life" of a radioactive substance is the amount of time it takes for half of the substance to decay. Using the information in the question, calculate the half-life of Caesium-137.

::: {.myanswers data-latex=""}
*Solution.*  The half-life is the median of the distribution; that is, the solution $x$ to
\[ F(x) = 1 - \mathrm{e}^{-0.023x} = \tfrac12 . \]
So
\[ x = \frac{\log\frac12}{-0.023} = \frac{\log 2}{0.023} = 30.1 \text{ years} . \]
:::
::::

:::: {.subq}
**(c)** It is estimated that roughly 24 kg of Caesium-137 was released during the Chernobyl disaster, which happened roughly 35.6 years ago. Estimate the mass of Caesium-137 that has still not decayed? 

::: {.myanswers data-latex=""}
*Solution.* The proportion of Caesium-137 still remaining is
\[ \mathbb P(X > 35.6) = \mathrm e^{-0.023 \times 35.6} = 0.441 , \]
so roughly $24 \times 0.441 = 10.6$ kg of Caesium-137 has still not decayed.

*(The Chernobyl disaster was actually* 36.6 *years ago -- I forgot to update the question this year. The amount of Caesium-137 will have gone down about another 250g in the last year.)*
:::
::::
:::::

::::: {.myq}
**B3.** Consider the pair of random variables $(X,Y)$ with joint PDF
\[ f_{X,Y}(x,y) = 2 \qquad \text{for $0 \leq x \leq y \leq 1$} \]
and $f_{X,Y}(x,y) = 0$ otherwise. (In particular, note that the joint PDF is only nonzero when $x \leq y$.)

:::: {.subq}
**(a)** Draw a picture of the range of $(X,Y)$ in the $xy$-plane.
::::

:::: {.subq}
**(b)** Describe the conditional distribution of $X$ given $Y = y$, for $0 \leq y \leq 1$.

::: {.myanswers data-latex=""}
*Solution.* Fix $y$. The conditional distribution is
\[ f_{X \mid Y}(x \mid y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} \propto f_{X,Y}(x,y) .\]
We know that $f_{X,Y}(x,y) = 2$ when $0 \leq x \leq y$ and is $0$ otherwise. So the conditional distribution of $X$ given $Y = y$ is continuous uniform on the interval $[0, y]$.

If we want to check the denominator $f_Y(y)$ formally, we can check that
\[ f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \mathrm dx = \int_0^y 2\, \mathrm dy = 2y ,\]
so the conditional PDF is indeed $f_{X \mid Y}(x \mid y) = 2/2y = 1/y$ for $0 \leq x \leq y$ and 0 otherwise.

<!--(In the online/video tutorial, I tried to explain this by drawing a graph in the $(x,y)$ plain of where $f_{X,Y}(x,y)$ is 2 or 0, then drawing a horizontal line across it at $y$ to demonstrate the conditional distribution. I'm not sure how clear this made things...)-->
:::
::::

:::: {.subq}
**(c)** What is the marginal PDF $f_X$ of $X$?

::: {.myanswers data-latex=""}
*Solution.* Again the key is that the joint PDF is only nonzero when $y \geq x$ but $y \leq 1$. So
\[ f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y) \, \mathrm dy = \int_x^1 2 \, \mathrm dy = 2(1 - x)  \]
for $0 \leq x \leq 1$ and 0 otherwise.
:::
::::

:::: {.subq}
**(d)** Are $X$ and $Y$ independent?

::: {.myanswers data-latex=""}
*Solution.* No. Take, for example, $x = \frac34$ and $y = \frac14$. It's clear that this $f_{X,Y}(\frac34,\frac14) = 0$, while $f_X(\frac34)$ and $f_Y(\frac14)$ are nonzero, just by looking at the picture from part (a).

We can check it formally too, if we want. Since $x > y$, this point has joint PDF $f_{X,Y}(\frac34,\frac14) = 0$. We know the marginal PMFs, though are
\begin{align*}
f_X\big(\tfrac34\big) &= 2\big(1 - \tfrac34\big) = \tfrac12 \\
f_Y\big(\tfrac14\big) &= 2 \times \tfrac14 = \tfrac12 .
\end{align*}
(we used $f_Y(y) = 2y)$ based on symmetry with $f_X(1-x)$, or alternatively by calculating it "long-hand".)
So $f_X(x)f_Y(y) = \tfrac12 \times \tfrac12 = \tfrac14 \neq 0$. So $X$ and $Y$ are not independent.
:::
::::
:::::

::::: {.myq}
**B4.** *(Optional)* Engineers and scientists often use the rule of thumb "Only 5% of data is more than two sample standard deviations away from the sample mean." Carefully justify this rule, using concepts from the module.

::: {.myanswers data-latex=""}
*Solution.* By the central limit theorem, and other related approximation arguments, it is reasonable that lots of real life data -- especially that which is affected by the accumulation of numerous small effects -- is approximately normally distributed.

Write $\mu$ for the *true* expectation and $\sigma^2$ for the *true* variance of the population distribution $X$. Then the proportion of data that is within two true-standard-deviations of the true-expectation will, be the law of large numbers, tends to
\[ \mathbb P (\mu - 2\sigma \leq X \leq \mu + 2\sigma) \]
Using standardisation, this is
\[ \mathbb P\left( \frac{(\mu - 2\sigma) - \mu}{\sigma} \leq \frac{X - \mu}{\sigma} \leq \frac{(\mu + 2\sigma) - \mu}{\sigma}\right) = \mathbb P(-2 \leq Z \leq 2) . \]
Using R or statistical tables, this is
\[ \Phi(2) - \Phi(-2) = 2 \, \Phi(2) - 1 = 0.9545 . \]
So only $1 - 0.9545 = 0.0455$, or approximately 5%, of data is more than two true-standard-deviations away from the true-expectation.

Finally, the law of large numbers also tells us that, provided a large number of datapoints $n$ are collected, the sample mean $\bar x$ and the sample standard deviation $s_x$ will be very close to the true expectation $\mu$ and the true standard deviation $\sigma$ respectively, so we can replace the latter with the former in our calculations.
:::
:::::






::::: {.myq}
**B5.**  Let $X_1, X_2, \dots, X_n$ be IID random variable with common expectation $\mu$ and common variance $\sigma^2$, and let $\overline X = (X_1 + \cdots + X_n)/n$ be the mean of these random variables. We will be considering the random variable $S^2$ given by
\[ S^2 = \sum_{i=1}^n (X_i - \overline X)^2 . \]

:::: {.subq}
**(a)** By writing 
\[ X_i - \overline X = (X_i - \mu) - (\overline X - \mu)  \]
or otherwise, show that
\[ S^2 = \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2 . \]

::: {.myanswers data-latex=""}
*Solution.*
Using the suggestion in the question, we have
\begin{align*}
S^2 &= \sum_{i=1}^n (X_i - \overline X)^2 \\
  &= \sum_{i=1}^n \big( (X_i - \mu) - (\overline X - \mu)  \big)^2 \\
  &= \sum_{i=1}^n \big( (X_i - \mu)^2 - 2(X_i - \mu)(\overline X - \mu) + (\overline X - \mu)^2\big) \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - \sum_{i=1}^n 2(X_i - \mu)(\overline X - \mu) + \sum_{i=1}^n (\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2\left(\sum_{i=1}^n X_i - \sum_{i=1}^n \mu\right)(\overline X - \mu)  + (\overline X - \mu)^2 \sum_{i=1}^n 1 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2(n\overline X - n\mu) (\overline X - \mu) + n (\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - 2n(\overline X - \mu)^2 + n(\overline X - \mu)^2 \\
  &= \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2 .
\end{align*}
This is mostly manipulation of sums as we have seen before, although note that going from the fifth to sixth lines we used the definition of $\overline X$ to write $\sum_{i=1}^n X_i$ as $n \overline X$.
:::
::::

:::: {.subq}
**(b)** Hence or otherwise, show that
\[ \mathbb E S^2 = (n - 1)\sigma^2 . \]
You may use facts about $\overline X$ from the notes provided you state them clearly. (You may find it helpful to recognise some expectations as definitional formulas for variances, where appropriate.) <!--You might also find it useful to recall that if a random variable $Y$ has expectation $\mathbb EY = 0$, then $\mathbb EY^2 = \Var(Y)$.-->

::: {.myanswers data-latex=""}
*Solution.* Starting with the linearity of expectation, we have
\begin{align*}
\mathbb ES^2 &= \mathbb E \left( \sum_{i=1}^n (X_i - \mu)^2 - n(\overline X - \mu)^2  \right) \\
  &= \sum_{i=1}^n \mathbb E (X_i - \mu)^2 - n \mathbb E(\overline X - \mu)^2 \\
  &= \sum_{i=1}^n \Var(X_i) - n \Var(\overline X) .
\end{align*}
The last line follows because $\mathbb EX_i = \mu$ for all $i$ by assumption, and we showed in the notes that $\mathbb E \overline X = \mu$ also; hence, as hinted, the expectations are precisely definitional formulas for the variances. We then also know that $\Var(X_i) = \sigma^2$ by assumption, and we showed Lecture 18 that $\Var(\overline X) = \sigma^2/n$. Hence
\[ \mathbb ES^2 = \sum_{i=1}^n \sigma^2 - n\, \frac{\sigma^2}{n} = n \sigma^2 - \sigma^2 = (n-1)\sigma^2, \]
as required.
:::
::::

:::: {.subq}
**(c)** At the beginning of this module, we defined the sample variance of the values $x_1, x_2, \dots, x_n$ to be
\[ s^2_x = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2 . \]
Explain one reason why we might consider it appropriate to use $1/(n-1)$ as the factor at the beginning of this expression, rather than simply $1/n$.

::: {.myanswers data-latex=""}
*Solution.* 
We often model a data set $x_1, x_2, \dots, x_n$ as being realisations of an IID sequence of random variables $X_1, X_2, \dots, X_n$. In this case, we are using the summary statistic of the sample variance $s_x^2$ to "estimate" the variance $\Var(X_1) = \sigma^2$. Using the factor $1/(n-1)$ ensures that this estimator is correct "in expectation", because
\[ \mathbb E s_X^2 = \mathbb E \frac{1}{n-1}S^2 = \frac{1}{n-1} \mathbb ES^2 = \frac{1}{n-1}(n-1)\sigma^2 = \sigma^2 . \]
This property of being correct in expectation is called being an "unbiased" estimator, and its usually considered beneficial for an estimator to be unbiased.

Note that we already know that the sample mean $\bar x$ is an unbiased estimator for the expectation $\mathbb EX = \mu$, as we already know that $\mathbb E\overline X = \mu$.

(You may learn more about estimation and "unbiasedness" in MATH1712 Probability and Statistics II.)
:::
::::
:::::

::::: {.myq}
**B6.** *(New)* Roughly how many times should I toss a coin for there to be a 95% chance that between 49% and 51% of my coin tosses land Heads?

::: {.myanswers data-latex=""}
*Solution.*
The number of Heads in $n$ coin tosses is $X \sim \mathrm{Bin}(n, \frac12)$, which is approximately $Y \sim \mathrm{N}(\frac n2, \frac n4)$. We want to choose $n$ such that
\[ \mathbb P(0.49 n \leq Y \leq 0.51n) = 0.95 .\]

Standardising, this is
\[ \mathbb P \left( \frac{0.49n - 0.5n}{0.5\sqrt{n}} \leq \frac{Y - 0.5n}{0.5\sqrt{n}} \leq \frac{0.51n - 0.5n}{0.5\sqrt{n}}\right) = \mathbb P(-0.02\sqrt{n} \leq Z \leq 0.02\sqrt{n}) \]
Since the normal distribution is symmetric, we want
\[\mathbb P(X \leq 0.02\sqrt{n}) = 0.975 .\]
From Table 2 of the statistical tables, or by the R command `qnorm(0.975)`, this requires
$0.01\sqrt{n} = 1.960$, which is $n \approx 9600$.

So if we toss 10,000 coins, there's about a 95% chance we get between 4900 and 5100 Heads.
:::
:::::

*(I meant to delete Question B4 from this problem sheet, to make it shorter, but I accidentally deleted Question B6 instead. I've now added B6 and marked B4 as "optional".)*