# Discrete random variables  {#L09-discrete-rv}

## What is a random variable?  {#rv}

Let's consider again the case of rolling two dice. We know that the sample space is the set of pairs of numbers between 1 and 6
  \[  \Omega = \big\{ \boldsymbol\omega = (\omega_1, \omega_2) : \omega_1, \omega_2 \in \{1,2,3,4,5,6\} \big\} , \]
which is equipped with the classical probability measure
\[ \mathbb P(A) = \frac{|A|}{36} . \]

But if we are rolling the two dice as part of a board game, we might only care about the total score on the two dice, rather than the two individual dice scores. In this case, we could write $X$ for the score on the first dice plus the score on the second dice -- that is,
\[ X(\boldsymbol\omega) = \omega_1 + \omega_2 . \]

If we want to know the probability we roll a total of 7, say, then we could write this as $\mathbb P(A)$ where
\[ A = \{ \boldsymbol\omega : \omega_1 + \omega_2 = 7\}  \]
is the set of dice rolls with total 7.
But it might just be easier to write this as $\mathbb P(X = 7)$. We might also be interested in other things about the total score $X$, like what the average total score is.

Here, $X$ is an example of what we call a **random variable**. A random variable can be thought of as a numerical summary of an experiment (like the total summarising the two individual dice rolls). But by considering the random variable directly, it often means we don't have to worry so much about exactly what the sample space is, or what probability measure is being used, or which sample outcomes are in a particular event. This often makes our life easier when thinking about probability problems.

Random variables are typically given capital letters from late in the alphabet, like $X$, $Y$, $Z$. Values that those random variables take are often given lower-case letters, like $x$, $y$, $z$.

The formal definition of a random variable is as a function that turns the outcome into this numerical summary.

::: {.definition}
Let $\Omega$ be a sample space. Then a **random variable** is a function $X$ from $\Omega$ to the real numbers $\mathbb R$; that is, to each sample outcome $\omega$ it assigns a real number $X(\omega)$.

Expressions like $\mathbb P(X = x)$ should be understood as representing more formal probability
\[ \mathbb P \big( \{\omega : X(\omega) = x \}\big) . \]
:::

This formal definition of a random variable as a function was summarised by my own first-year probability lecturer as "There's only two things you need to know about the definition of a random variable: first, it's not random; second, it's not a variable."

However, the way we actually *think* about random variables *is* as random and is as variables. In this more informal way of thinking, a random variable is a variable that can take different values with different probabilities -- just as the total of the two dice can be 2 with probability $\frac{1}{36}$, or the value 3 with probability $\frac{2}{36}$, and so on.

\newcommand{\Range}{\operatorname{Range}}

::: {.definition}
The set of values a random variable $X$ can take is called its **range**, $\Range(X) = \{X(\omega) : \omega \in \Omega \}$.
:::

So, for example, the range of the dice sum $X$ is $\Range(X) = \{2, 3, \dots, 12\}$.

Random variables that we will consider in this module will be one of two types:

* **Discrete random variables** have a range that is finite (like the dice total being an integer between 2 and 12) or countably infinite (like the positive integers, for example). Discrete random variables can be used as models for "count data".
* **Continuous random variables** have a range that is uncountably infinite (like the real numbers, the positive real numbers, or the interval $[0,1]$, for example). Continuous random variables can be used as models for "measurement data".

In this section and the next two, we will look at discrete random variables; later in Section 8 and 9 we will look at continuous random variables.


## Probability mass functions  {#pmf}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/ZYYTNk7F4KI"></iframe>
:::
::::


We now consider only discrete random variables $X$, where the range $\Range(X)$ is a finite or countably infinite set. In this case, Axiom 3 tells us that for any set $A$, we have
\[ \mathbb P(X \in A) = \sum_{x \in A} \mathbb P(X = x) . \]
(Recall that the symbol $\in$ means "is an element of", or just "is in" for short.)
So to fully understand a discrete random variable $X$, we need only understand the probabilities $\mathbb P(X = x)$. These are captured by the probability mass function.

::: {.definition}
For a discrete random variable $X$, its **probability mass function** (or **PMF**) is the function $p_X$ where 
\[ p_X(x) = \mathbb P(X = x)  \qquad \text{for $x \in \Range(X)$.} \]
(When the random variable is obvious from context, we'll just write $p(x)$ without the subscript.)
:::

The key is that, once we are thinking in terms of a random variable via its PMF, we can (usually) stop worrying to much about what the underlying sample space is and how the random variable acts on that space.

::: {.example}
Let $X$ being the sum of two dice rolls. As this is a classical probability problem, the probability $p(x)$ of rolling a total of $x$ is $n(x) / 36$, where $n(x)$ is the number of ways of rolling a total of $x$. So, for example, there is only one way $(1,1)$ of rolling a total of 2, so $p(2) = \frac1{36}$, but there are 5 ways of rolling a 6: $(1,5), (2,4), (3, 3), (4, 2), (5, 1)$; so $p(5) = \frac5{36}$.

The PMF $p$ of $X$ is given by

| $x$ | $2$ | $3$ | $4$ | $5$ | $6$ | $7$ |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| $p(x)$ | $\frac{1}{36}$ | $\frac{2}{36}$ | $\frac{3}{36}$ | $\frac{4}{36}$ | $\frac{5}{36}$ | $\frac{6}{36}$ |

| $x$ | $\cdots$ | $8$ | $9$ | $10$ | $11$ | $12$ |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| $p(x)$ | $\cdots$ | $\frac{5}{36}$ | $\frac{4}{36}$ | $\frac{3}{36}$ | $\frac{2}{36}$ | $\frac{1}{36}$ |

```{r dice-pmf, cache = TRUE, echo = FALSE}
x <- 2:12
prob <- c(1:6, 5:1)/36

plot(x, prob, type = "h", xlab = "dice total, x", ylab = "probability mass function, p(x)", xlim = c(0, 15), ylim = c(0,0.2), col = "blue", lwd=2)
```
:::

::: {.example}
Consider tossing a biased coin, that is Heads with probability $p$ and Tails with probability $1-p$. Let $X = 1$ if the coin lands Heads, and $X = 0$ if the coin lands Tails. The PMF $p_X$ of this random variable is given by
\[ p_X(0) = 1 - p \qquad p_X(1) = p . \]

We could alternatively think of the same random variable as representing the result of an experiment, where $X = 1$ represents a success, with probability $p_X(1) = p$, and $X = 0$ represents a failure, with probability $p_X(0) = 1 - p$.

A random variable $X$ with this PMF is called a **Bernoulli trial** (or a "Bernoulli random variable", or is said to "follow the Bernoulli distribution" -- after the seventeenth-century Swiss mathematician [Jacob Bernoulli](https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/)). We use the notation $X \sim \text{Bern}(p)$ for short.
:::

Note that since $p(x) = \mathbb P(X = x)$ is a probability, we must have $p(x) \geq 0$ for all $x \in \Range(X)$. Further, we have
\[ 1 = \mathbb P(\Omega) = \mathbb P\big(X \in \Range(X)\big) = \sum_{x \in \Range(X)} \mathbb P(X = x) = \sum_{x \in \Range(X)} p(x) . \]
Hence we have the following:

::: {.theorem}
Let $X$ be a discrete random variable, and let $p_X$ be its PMF. Then

* $p_X(x) \geq 0$ for all $x$;
* ${\displaystyle \sum_x p_X(x) = 1}$.
:::

Sometimes it is useful to know the probability a random variable $X$ is less than some value $x$. This is captured by the **cumulative distribution function** (or **CDF**) $F_X$, where
\[ F_X(x) = \mathbb P(X \leq x) = \sum_{y \leq x} p_X(y) \qquad \text{for $x \in \mathbb R$.}  \]

::: {.example}
Let $X \sim \text{Bern}(p)$ be a Bernoulli random variable with success probability $p$. Then its CDF $F$ is
\[ F(x) = \begin{cases} 0 & \text{for $x < 0$} \\
                      1-p & \text{for $0 \leq x < 1$} \\
                      1   & \text{for $x \geq 1$} . \end{cases} \]
:::

::: {.example}
If $X$ is the sum of two dice rolls, then the CDF $F$ is given by

| $x \in {}$ | $(-\infty, 2)$ | $[2,3)$ | $[3,4)$ | $[4,5)$ | $\cdots$ | $[11,12)$ | $[12, \infty)$ |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| $F(x)$ | $0$ | $\frac{1}{36}$ | $\frac{3}{36}$ | $\frac{5}{36}$ | $\cdots$ | $\frac{35}{36}$ | $1$ |

```{r dice-cdf, cache = TRUE, echo = FALSE}
x <- 2:12
prob <- c(1:6, 5:1)/36
cdf <- cumsum(prob)

plot(c(-1,x, 16), c(0, cdf, 1), type = "s", xlab = "x", ylab = "cumulative distribution function, F(x)", xlim = c(0, 15), ylim = c(0,1), col = "blue")
```

Note that the CDF is a "step function" that starts at 0, then jumps up suddenly at each of the values $2, 3, \dots, 12$, ending up at 1.
:::

For any random variable $X$ with CDF $F$,

* if $x$ is smaller than everything in the range of $X$, then $F(x) = 0$;
* if $x$ is greater than everything in the range of $X$, then $F(x) = 1$;
* $F(x)$ is increasing in $x$.


