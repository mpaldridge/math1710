# Poisson distribution {#L12-poisson}

## Definition and properties  {#poisson}

<!--
:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/xbWQalpr5h0"></iframe>
:::
::::
-->

Another important distribution is the Poisson distribution. The Poisson distribution (roughly "*pwa*-song") is typically used to model "the number of times something happens in a set period of time". For example, the number of emails you receive in a day; the number of claims at an insurance company each year; or the number of calls to call centre in one hour. (Famously, one of the first historical datasets modelled using a Poisson distribution was "the number of Prussian soldiers in different cavalry units kicked to death by their own horse between 1875 and 1894".) We'll explain why the Poisson distribution is a good model for this in the next subsection.

::: {.definition}
Let $X$ be a discrete random variable with range $\{0,1,2,\dots\}$ and PMF
\[ p(x) = \mathrm e^{-\lambda}  \frac{\lambda^x}{x!} . \]
Then we say that $X$ follows the **Poisson distribution** with **rate** $\lambda$, and write $X \sim \text{Po}(\lambda)$.
:::

I should note that we interpret $0! = 1$, so
\[ p(0) = \mathrm e^{-\lambda}  \frac{\lambda^0}{0!} = \mathrm e^{-\lambda}  \frac{1}{1} = \mathrm e^{-\lambda} . \]

```{r po-pic, cache = TRUE, echo = FALSE}
x <- 0:10

plot(x-0.05,   dpois(x, 1.2), type = "h", lwd = 4, col = "blue", ylim = c(0, 0.4), xlab = "x", ylab = "probability mass function p(x)")
points(x+0.05, dpois(x, 3), type = "h", lwd = 4, col = "red")
legend("topright", c("Po(1.2)", "Po(3)"), col = c("blue", "red"), lwd = 4)
```

The Poisson distribution is named after the French mathematician [Siméon-Denis Poisson](https://mathshistory.st-andrews.ac.uk/Biographies/Poisson/) who wrote about it in 1837, although the origin of the idea is more than 100 years earlier with another French mathematician, [Abraham de Moivre](https://mathshistory.st-andrews.ac.uk/Biographies/De_Moivre/).

::: {.example}
*An insurance company receives large insurance claims of over £100,000 at a rate of $\lambda = 1.2$ per day, modelled as a Poisson distribution and independent between days. What's the probability that in a week (5 days) they get at least one large claim every day?*

Let $X \sim \text{Po}(\lambda)$ be the number of large claims received in a day. Then the probability there is at least one claim in a day -- which is an "at least one" question, suggesting we look at the complement -- is
\[ \mathbb P(X \geq 1) = 1 - \mathbb P(X = 0) = 1 - \mathrm e^{-1.2} \frac{1.2^0}{0!} = 1 - \mathrm e^{-1.2} =0.699 .    \]

Since it's assumed that days are independent, the probability there is at least one large claim all 5 days is
\[  \mathbb P(X \geq 1)^5 = 0.699^5 = 0.167, \]
or about 17%.
:::

The parameter $\lambda$ is called the "rate" because that indeed the number of emails (or insurance claims, or phone calls, or deaths by horse-kicking) that we expect to see.

::: {.theorem}
Let $X \sim \text{Po}(\lambda)$. Then

* $p(x)$ is indeed a PMF, in that $\displaystyle\sum_{x=0}^\infty p(x) = 1$.
* $\mathbb EX = \lambda$,
* $\Var(X) = \lambda$.
:::

::: {.proof}
We'll do the first two here, then you can do the variance in Problem Sheet 4. 

It will be useful to remember the Taylor series for the exponential function,
\[ \mathrm e^\lambda = \sum_{x=0}^\infty \frac{\lambda^k}{x!} . \]

To see that the PMF does indeed sum to one, note that the Taylor series gives us
\[ \sum_{x=0}^\infty p(x) = \sum_{x=0}^\infty \mathrm e^{-\lambda} \frac{\lambda^x}{x!}
= \mathrm e^{-\lambda} \sum_{x=0}^\infty  \frac{\lambda^x}{x!} = \mathrm e^{-\lambda}\,\mathrm e^{\lambda} = 1. \]

For the expectation, we have
\begin{align*}
\mathbb EX &= \sum_{x=0}^\infty x\,\mathrm e^{-\lambda}  \frac{\lambda^x}{x!} \\
  &= \mathrm e^{-\lambda} \sum_{x=1}^\infty x\,\frac{\lambda^x}{x!} \\
  &= \mathrm e^{-\lambda} \sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!} \\
  &= \lambda \mathrm e^{-\lambda} \sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}
\end{align*}
In the second line, we took $\mathrm e^{-\lambda}$ outside the sum, and allowed ourselves to start the sum from 1, since the $x = 0$ term was 0 anyway; in the third line, we cancelled the $x$ from the $x!$ to get $(x-1)!$; and in the fourth line we took one of the $\lambda$s in $\lambda^x$ outside the sum, to give ourselves terms in $x - 1$ inside the sum. We can now "re-index" the sum by putting $y = x - 1$, to get
\[ \mathbb EX = \lambda \mathrm e^{-\lambda} \sum_{y=0}^\infty \frac{\lambda^{y}}{y!}
= \lambda \mathrm e^{-\lambda} \mathrm e^{\lambda} = \lambda , \]
where we used the Taylor series again.
:::

## Poisson approximation to the binomial  {#poisson-approx}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/5Ma4YIF_r7w"></iframe>
:::
::::


Suppose I own a watch shop in Leeds. My watches are very expensive, so I don't need to sell many each day -- in fact, I sell an average of 4.8 watches per day. How should I model the number of watches sold each day as a random variable?

One way could be to say this. There are $n$ people living in Leeds or nearby, and, on any given day, each of them will independently buy a watch from my shop with probability $p$. Thus the total number of watches I sell could be modelled as a binomial distribution $\text{Bin}(n, p)$.

But what should $n$ and $p$ be? To make the average $\mathbb EX = np = 4.8$, I should take $p = 4.8/n$. But what about $n$? We know $n$ is a very big number, because Leeds is a big city, so let's take a limit as $n \to \infty$. It turns out, that this distribution $\text{Bin}(n, 4.8/n)$ becomes a Poisson(4.8) distribution!

:::  {.theorem #po-bint}
Fix $\lambda \geq 0$, and let $X_n \sim \text{Bin}(n, \lambda/n)$ for all integers $n \geq \lambda$. Then $X_n \to \text{Po}(\lambda)$ in distribution as $n \to infty$, by which we mean that if $Y \sim \text{Po}(\lambda)$, then
\[ p_{X_n}(x) \to p_Y(x) \qquad \text{for all $x \in \{0, 1, \dots \}$}. \]
:::

A looser way to state the principle of this theorem would be this: *When $n$ is very large and $p$ very small, in such a way that $np$ is a small-ish number, then $\text{Bin}(n,p)$ is well approximated by $\text{Po}(\lambda)$ where $\lambda = np$.*

This is why a Poisson distribution is a good model for the number of occurrences in a set time period. It applies if there lots of things that could happen (large $n$), each one is individually unlikely (small $p$), and on average a few of them will actually happen ($\lambda = np$ small-ish).

::: {.example}
*A lecturer teaches a module with $n = 100$, and estimate that each student turns up to office hours drop-in sessions independently with probability $p = 0.035$. What is the probability that **(a)** exactly 5, **(b)** 2 or more students turn up to a drop-in session?*

If we let $X$ be the  number of students that turn up to a drop-in session, then the exact distribution of $X$ is $X \sim \text{Bin}(100, 0.035)$.

For part (a), we then have
\[ \mathbb P(X = 5) = \binom{100}{5} 0.035^5 (1 - 0.035)^{100-5} = 0.134 .  \]

For part (b), we use the complement rule to get
\begin{align*}
\mathbb P(X \geq 2)
&= 1 - \mathbb P(X = 0) - \mathbb P(X = 1) \\
&= 1 - \binom{100}{0} 0.035^0 (1 - 0.035)^{100-0} + \binom{100}{1} 0.035^1 (1 - 0.035)^{100 - 1} \\
&= 1 - (1 - 0.035)^{100} + 100 \times 0.035 (1 - 0.035)^{99} \\
&= 1 - 0.028 - 0.103 \\
&= 0.869
\end{align*}

Alternatively, it might be more convenient to approximate $X$ by a Poisson distribution $Y \sim \text{Po}(100 \times 0.035) = \text{Po}(3.5)$.

For part (a), this gives
\[ \mathbb P(Y = 5) = \mathrm e^{-3.5} \frac{3.5^5}{5!} = 0.132 ,  \]
which is very close to the exact answer above of $0.134$.

For part (b), the approximation gives
\begin{align*}
\mathbb P(Y \geq 2)
&= 1 - \mathbb P(Y = 0) - \mathbb P(Y = 1) \\
&= 1 - \mathrm e^{-3.5} \frac{3.5^0}{0!} - \mathrm e^{-3.5} \frac{3.5^1}{1!} \\
&= 1 - \mathrm e^{-3.5} - 3.5 \mathrm e^{-3.5} \\
&= 1 - 0.030 - 0.106 \\
&= 0.864
\end{align*}
which is very close to the exact answer above of $0.869$.

The following graph shows how close the $\text{Po}(3.5)$ distribution is to a $\text{Bin}(100, 0.035)$ distribution -- not exact, but pretty good.

```{r po-binom-pic, cache = TRUE, echo = FALSE}
x <- 0:10

plot(x-0.05,   dpois(x, 3.5), type = "h", lwd = 4, col = "blue", ylim = c(0, 0.25), xlab = "x", ylab = "probability mass function p(x)")
points(x+0.05, dbinom(x, 100, 0.035), type = "h", lwd = 4, col = "red")
legend("topright", c("Po(3.5)", "Bin(100, 0.035)"), col = c("blue", "red"), lwd = 4)
```
:::



For completeness, we include a proof of Theorem \@ref(thm:po-bint) here, although since it discusses use of limits, it's not examinable material for this module.

::: {.proof}
*(Non-examinable)*
We need to show that, as $n \to \infty$, 
\[ \binom nx \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n-x}
\to \mathrm{e}^{-\lambda} \frac{\lambda^x}{x!} . \]
Let's try! The left-hand side is
\begin{align*}
\binom nx &\left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n-x} \\
  &= \frac{n(n-1)\cdots(n-x+1)}{x!} \frac{\lambda^x}{n^x} \left(1 - \frac{\lambda}{n}\right)^{n}\left(1 - \frac{\lambda}{n}\right)^{-x} \\
  &= \frac{\lambda^x}{x!} \frac{n(n-1)\cdots(n-x+1)}{n^x} \left(1 - \frac{\lambda}{n}\right)^{n}\left(1 - \frac{\lambda}{n}\right)^{-x} \\
  &= \frac{\lambda^x}{x!} \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-x+1}{n} \left(1 - \frac{\lambda}{n}\right)^{n}\left(1 - \frac{\lambda}{n}\right)^{-x} \\
  &= \frac{\lambda^x}{x!} 1 \left(1 - \frac{1}{n}\right) \cdots \left(1 - \frac{x-1}{n}\right)  \left(1 - \frac{\lambda}{n}\right)^{n}\left(1 - \frac{\lambda}{n}\right)^{-x} .
\end{align*}

We take each of the terms in turn. First $\lambda^x / x!$ looks very promising, and can stay. Second, each of the terms $1, 1 - 1/n, \dots, 1 - (x-1)/n$ tend to 1 as $n \to \infty$. Third, 
\[ \left(1 - \frac{\lambda}{n}\right)^{n} \to \mathrm{e}^{-\lambda} ; \]
this is from a standard result that
\[ \left(1 + \frac{a}{n}\right)^{n} \to \mathrm{e}^{a} \qquad \text{as $n \to \infty$}. \]
Finally
\[\left(1 - \frac{\lambda}{n}\right)^{-x} \to 1 , \]
as $1 - \lambda/n \to 1$, and $x$ is fixed. Putting all that together gives the result.
:::

## Poisson process  {#poisson-process}

A call centre is open 9 hours a day


## Summary  {#summary-06 .unnumbered}

| Distribution | Range | PMF | Expectation | Variance |
|:----|:-:|:-:|:-:|:-:|
| **Bernoulli:** $\text{Bern}(p)$ | $\{0,1\}$ | $p(0) = 1- p$, $p(1) = p$ | $p$ | $p(1-p)$ |
| **Binomial:** $\text{Bin}(n,p)$ | $\{0,1,\dots,n\}$ | $\displaystyle\binom{n}{x} p^x (1-p)^{n-x}$ | $np$ | $np(1-p)$ |
| **Geometric:** $\text{Geom}(p)$ | $\{1,2,\dots\}$ | $(1-p)^{x-1}p$ | $\displaystyle\frac{1}{p}$ | $\displaystyle\frac{1-p}{p^2}$ |
| **Poisson:** $\text{Po}(\lambda)$ | $\{0,1,\dots\}$ | $\mathrm{e}^{-\lambda} \displaystyle\frac{\lambda^x}{x!}$ | $\lambda$ | $\lambda$ |