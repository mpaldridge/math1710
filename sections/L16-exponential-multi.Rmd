# Exponential distribution and multiple continuous random variables  {#L17-exp-multiple}

This lecture is really two mini-lectures stuck together. In the first mini-lecture, we look at another important continuous distribution, the exponential distribution. In the second mini-lecture, we look at the theory of multiple continuous random variables, and find it's very similar to what we already know about multiple discrete random variables.

## Exponential distribution  {#exponential}

<!--
:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/t11Rsll3jVI"></iframe>
:::
::::
-->


An important continuous distribution is the exponential distribution. The exponential distribution is often used to represent lengths of time: for example, the time between radioactive particles decaying, the time between eruptions of a volcano, or the time between buses arriving at a bus stop.

::: {.definition}
A continuous random variable $X$ is said to have the **exponential distribution with rate $\lambda > 0$** if it has the PDF
\[ f(x) = \lambda \mathrm{e}^{-\lambda x} \qquad \text{for $x \geq 0$}, \]
and 0 otherwise. We write $X \sim \text{Exp}(\lambda)$.
:::

```{r exp-pic, cache = TRUE, echo = FALSE}
curve(dexp(x, 0.5), type = "l", lwd = 2, col = "blue", from = 0, to = 11, xlim = c(0,10),  xlab = "x", ylab = "probability density function f(x)")
curve(dexp(x, 0.2), type = "l", lwd = 2, from = 0, to = 11, col = "red", add = TRUE)
legend("topright", c("Exp(1/2)", "Exp(1/5)"), col = c("blue", "red"), lwd = 2)
```

::: {.example}
*The length of time in years that a lightbulb works before needing to be replaced is modelled as an exponential distribution with rate $\lambda = 2$. What is the probability the lightbulb needs replacing within a year?*

If $X \sim \text{Exp}(2)$ is the lifetime of the lightbulb, we seek $\mathbb P(X \leq 1)$. This is
\[ \int_{-\infty}^1 f(x)\, \mathrm{d}x = \int_0^1 2 \mathrm e^{-2x} \, \mathrm dx = \big[ -\mathrm e^{-2x} \big]_0^1 = -\mathrm e^{-2} -(-1) = 1 - \mathrm e^{-2} = 0.864.  \]
:::

::: {.theorem #exp-prop}
Suppose $X \sim \text{Exp}(\lambda)$. Then:

1. $f$ is indeed a PDF, in that $\displaystyle\int_0^\infty f(x)\,\mathrm{d}x = 1$;
2. the CDF of $X$ is $F(x) = 1 - \mathrm{e}^{-\lambda x}$;
3. the expectation of $X$ is $\mathbb EX = \displaystyle\frac{1}{\lambda}$;
4. the variance of $X$ is $\Var(X) = \displaystyle\frac{1}{\lambda^2}$.
:::

::: {.example}
Returning to the lightbulb example, where $X \sim \text{Exp}(2)$, we see that the average lifetime of a lightbulb is $\mathbb EX = \frac12$ a year with variance $\Var(X) = \frac14$.

If we wanted to calculate $\mathbb P(1 \leq X \leq 3)$, we could do this by integrating the PDF between 1 and 3. Alternatively, we could use the CDF:
\[ \mathbb P(1 \leq X \leq 3) = F(3) - F(1) = (1 - \mathrm{e}^{-2\times 3}) - (1 - \mathrm{e}^{-2\times 1}) = \mathrm{e}^{-2} - \mathrm{e}^{-6} = 0.132 . \]
:::

::: {.proof}
*of Theorem \@ref(thm:exp-prop).*
For part 1,
\[ \int_0^\infty \lambda \mathrm{e}^{-\lambda x}\,\mathrm{d}x
     = \big[-\mathrm{e}^{-\lambda x} \big]_0^\infty = -0 -(-1) = 1 . \]

Similarly for part 2,
\[ F(x) = \int_0^x \lambda \mathrm{e}^{-\lambda y}\,\mathrm{d}y
     = \big[-\mathrm{e}^{-\lambda y} \big]_0^x = -\mathrm{e}^{-\lambda x} -(-1) = 1 - \mathrm{e}^{-\lambda x}. \]
     
For part 3, we use integration by parts with $u = x$ and $v' = \lambda \mathrm{e}^{-\lambda x}$, so $u' = 1$ and $v = -\mathrm{e}^{-\lambda x}$. We get
\begin{align*}
\mathbb EX &= \int_0^\infty x  \lambda \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
  &= \big[-x \mathrm{e}^{-\lambda x}\big]_0^\infty + \int_0^\infty \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
  &= -0 - (-0) + \left[ -\frac{1}{\lambda} \mathrm{e}^{-\lambda x} \right]_0^\infty \\
  &= -0 - \left(- \frac{1}{\lambda}\right) \\
  &= \frac{1}{\lambda}
\end{align*}

For part 4, we use integration by parts with $u = x^2$ and $v' = \lambda \mathrm{e}^{-\lambda x}$, so $u' = 2x$ and $v = -\mathrm{e}^{-\lambda x}$ again. We get
\begin{align*}
\mathbb EX^2 &= \int_0^\infty x^2  \lambda \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
  &= \big[-x^2 \mathrm{e}^{-\lambda x}\big]_0^\infty + \int_0^\infty 2x \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
  &= -0 - (-0) + \frac{2}{\lambda} \int_0^\infty x  \lambda x \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
  &= \frac{2}{\lambda} \mathbb EX \\
  &= \frac{2}{\lambda^2} ,
\end{align*}
where we used a cunning trick on the integral on the right -- spotting that we could turn it into the expectation, which is $1/\lambda$, by part 3 -- to save us the effort of calculating it again.
Hence
\[ \Var(X) = \mathbb EX^2 - \left(\frac{1}{\lambda}\right)^2 =  \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2} .  \]
:::



## Multiple continuous random variables  {#continuous-multiple}

The theory we set up for two or more discrete random variables also works for two or more continuous random variables.

Now, the intensity of probability for $(X,Y)$ being around $(x,y)$ is given by the **joint probability density function** $f_{X,Y}$. In particular for $a \leq b$ and $c \leq d$, we have
\[ \mathbb P(a \leq X \leq b \text{ and } c \leq Y \leq d ) = \int_{x = a}^b \int_{y = c}^d f_{X,Y}(x,y)\, \mathrm dx \,\mathrm dy .\]

|   Discrete random variables   |   Continuous random variables   |
|-----|-----|
| We can get the **marginal PMF** $p_X$ of $X$ by summing over $y$, so \[ p_X(x) = \sum_y p_{X,Y}(x,y) . \] | We can get the **marginal PDF** $f_X$ of $X$ by integrating over $y$, so \[ f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y) \, \mathrm dy. \] |
| Two discrete random variables $X$ and $Y$ are **independent** if their PMFs satisfy \[p_{X,Y}(x,y) = p_X(x)\,p_Y(y) \qquad \text{for all $x, y$}.\] | Two continuous random variables $X$ and $Y$ are **independent** if they have PDFs which satisfy \[f_{X,Y}(x,y) = f_X(x)\,f_Y(y) \qquad \text{for all $x, y$}.\] |
| The **conditional PMF** of $Y$ given $X$ is defined by \[ p_{Y \mid X}(y \mid x) = \frac{p_{X,Y}(x,y)}{p_X(x)} . \] | The **conditional PDF** of $Y$ given $X$ is defined by \[ f_{Y \mid X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)} . \] |
| **Bayes' theorem** states that \[ p_{X \mid Y}(x \mid y) = \frac{p_X(x)\,p_{Y\mid X}(y\mid x)}{p_Y(y)} . \] | **Bayes' theorem** states that \[ f_{X \mid Y}(x \mid y) = \frac{f_X(x)\,f_{Y\mid X}(y\mid x)}{f_Y(y)} . \] |
| The expectation of a function of $X$ and $Y$ is given by the sum \[ \mathbb Eg(X,Y) = \sum_{x,y} g(x,y)\, p_{X,Y}(x,y) . \] | The expectation of a function of $X$ and $Y$ is given by the integral \[ \mathbb Eg(X,Y) = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)\, f_{X,Y}(x,y) \, \mathrm dx \, \mathrm dy . \] |
| The **covariance** of $X$ and $Y$ is given by \[ \Cov(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) , \] and has a computational formula \[ \Cov(X,Y) = \mathbb EXY - \mu_X \mu_Y . \] | The **covariance** of $X$ and $Y$ is given by \[ \Cov(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) , \] and has a computational formula \[ \Cov(X,Y) = \mathbb EXY - \mu_X \mu_Y . \]

::: {.example}
Consider the pair of continuous random variable $(X,Y)$ with joint PDF
\[ f_{X,Y}(x,y) = \tfrac12(1 + x + y) \qquad \text{for $0 \leq x,y\leq 1$} \]
and $f_{X,Y}(x,y) = 0$ otherwise.

We get the marginal distribution for $X$ by integrating over $y$, so
\[ f_X(x) = \int_0^1 \tfrac12(1 + x + y) \, \mathrm dy = \tfrac12 \left[(1 + x)y + \tfrac12y^2\right]_0^1 = \tfrac34 + \tfrac12x . \]

We can find the the conditional PDF for $Y$ given $X = \tfrac14$. It is
\[ f_{Y\mid X}\big(y \mid \tfrac14\big) = \frac{f_{X,Y}\big(\tfrac14,y\big)}{f_X\big(\tfrac14\big)}
    = \frac{\tfrac12\big(1 + \tfrac14 + y\big)}{\tfrac34 + \tfrac12\times\tfrac14 } = \tfrac{5}{7} + \tfrac47 y . \]
    
We can calculate the covariance. First, the expectations are
\[ \mathbb EX = \int_{-\infty}^\infty x\, f_X(x) \,\mathrm dx = \int_0^1 x\big(\tfrac34 + \tfrac12x\big)\, \mathrm dx = \left[\tfrac38 x^2 + \tfrac16 x^3 \right] = \tfrac{13}{24} \]
and $\mathbb EY = \frac{13}{24}$ also, by symmetry. Second, we have
\begin{align*}
\mathbb EXY
&= \int_{-\infty}^\infty \int_{-\infty}^\infty xy\, f_{X,Y}(x,y) \, \mathrm dx\, \mathrm dy \\
&= \int_0^1 \int_0^1 xy \, \tfrac12(1 + x + y)\, \mathrm dx\, \mathrm dy \\
&= \int_0^1 \left[ \tfrac14 x^2y + \tfrac16 x^3y + \tfrac14 x^2y^2  \right]_{x=0}^1 \, \mathrm dy\\
&= \int_0^1 \big( \tfrac14 y + \tfrac16 y + \tfrac14 y^2 \big) \, \mathrm dy\\
&= \left[ \tfrac18 y^2 + \tfrac{1}{12}y^2 + \tfrac{1}{12}y^3  \right]_0^1 \\
&= \tfrac{7}{24} .
\end{align*}
So therefore,
\[ \Cov(X,Y) = \mathbb EXY - \mu_X \mu_Y = \tfrac{7}{24} - \tfrac{13}{24} \times \tfrac{13}{24} = -\tfrac{1}{576} . \]
::::

## Summary  {#summary-L17 .unnumbered}

::: {.mysummary}
* The exponential distribution has PDF $f(x) = \lambda \mathrm e^{-\lambda x}$, expectation $1/\lambda$, and variance $1/\lambda^2$.
* Most properties of multiple discrete random variables carry of to multiple continuous random variables. To get a marginal PDF from a joint PDF, we integrate (rather than sum) over the other variable.
:::