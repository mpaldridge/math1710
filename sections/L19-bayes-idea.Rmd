# (PART\*) Part III: Bayesian statistics  {-}

# The Bayesian idea  {#L19-bayes-idea}

In this final section of the module, we return to statistics, where we will look at an approach to data analysis known as "Bayesian statistics".

**Statistics** concerns how to draw conclusions from data; and **Bayesian statistics** is one particular framework for doing this. The idea of Bayesian statistics is that we start with "prior" ("before") beliefs about the underlying model, then use the data (together with Bayes' theorem) to update our to our "posterior" ("after") beliefs about the model *given* the data we have observed.

## Example: fake coin?  {#fake-coin}

<!--
:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/BUZ_4DqHjIM"></iframe>
:::
::::
-->





We will start by illustrating the main idea with an example.

::: {.example}
*A joke shop sells three types of coins: normal fair coins; Heads-biased coins, which land Heads with probability 0.8; and Tails-biased coins, which land Heads with probability 0.2. I pick up a coin and examine it; since it looks mostly like a normal coin, I believe there's 60% chance it's s fair coin, and a 20% chance it's biased either way. I decide to toss the coin four times, to gather some more evidence. The result is that all three are Heads. How should I update my beliefs?*

So, we start with the "prior" (before) belief
\[ \mathbb P(\text{fair}) = 0.6 \qquad \mathbb P(\text{H-bias}) = 0.2 \qquad \mathbb P(\text{T-bias}) = 0.2 \]

We know how to update our beliefs after seeing the data: we use Bayes' theorem. We have
\begin{align*}
\mathbb P(\text{fair} \mid \text{HHH}) &= \frac{\mathbb P(\text{fair})\, \mathbb P(\text{HHH}\mid \text{fair})}{\mathbb P(\text{HHH})} = \frac{0.6 \times 0.5^3}{\mathbb P(\text{HHH})} = \frac{0.075}{\mathbb P(\text{HHH})} \\
\mathbb P(\text{H-bias} \mid \text{HHH}) &= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHH}\mid \text{H-bias})}{\mathbb P(\text{HHH})} = \frac{0.2 \times 0.8^3}{\mathbb P(\text{HHH})} = \frac{0.1024}{\mathbb P(\text{HHH})} \\
\mathbb P(\text{T-bias} \mid \text{HHH}) &= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHH}\mid \text{T-bias})}{\mathbb P(\text{HHH})} = \frac{0.2 \times 0.2^3}{\mathbb P(\text{HHH})} = \frac{0.0016}{\mathbb P(\text{HHH})}  .
\end{align*}
We also need to find common denominator $\mathbb P(\text{HHH})$. We could do that using the law of total probability. But a convenient short-cut is to notice that the above three probabilities have to add up to 1, and so that common denominator must be $0.075 + 0.1024 + 0.0016 = 0.179, so we have
\begin{align*}
  \mathbb P(\text{fair} \mid \text{data}) &= \frac{0.075}{0.179} = 0.419 \\
  \mathbb P(\text{H-bias}\mid \text{data}) &= \frac{0.1024}{0.179} = 0.572 \\
  \mathbb P(\text{T-bias}\mid \text{data}) &= \frac{0.0016}{0.179} = 0.009 .
\end{align*}

So, after tossing the coin four times, our belief has been updated from the "prior" (before) belief
\[ \mathbb P(\text{fair}) = 0.6 \qquad \mathbb P(\text{H-bias}) = 0.2 \qquad \mathbb P(\text{T-bias}) = 0.2 \]
to the "posterior" (after) belief
\[ \mathbb P(\text{fair} \mid \text{data}) = 0.419 \qquad \mathbb P(\text{H-bias}\mid \text{data}) = 0.572 \qquad \mathbb P(\text{T-bias}\mid \text{data}) = 0.009 . \]
Compared to our prior beliefs, our belief that the coin is fair has decreased a little bit; our belief the coin is biased towards Heads has shot up, and is now our most likely belief; while our belief the coin is biased towards Tails has plummeted to just 1%.
:::


## Bayesian framework  {#bayesian-framework}

<!--
:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/9moh0KYc6fE"></iframe>
:::
::::
-->


Let's think more systematically about what we did in the previous example.

* **Model:** The four coin tosses were modelled as four IID Bernoulli trials $X_1, X_2, X_3, X_4 \sim \text{Bern}(\theta)$ (if we let $X_i = 1$ denote that the $i$th coin was Heads). Here, the probability of Heads is some unknown parameter $\theta$. (Recall we talked about parametric models for data in Subsection \@ref(models).) This model gives a distribution that depends on the parameter. Hre we had a conditional PMF for one trial
\[ p(x \mid \theta) = \theta^{x} (1 - \theta)^{1- x}  \]
(this is a convenient way of writing the PMF for a Bernoulli trial). So the joint PMF for the IID trials is
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^4 \theta^{x_i} (1 - \theta)^{1- x_i} = \theta^{\sum_i x_i} (1 - \theta)^{4- \sum_i x_i} . \]
(Here and throughout, $\prod$, the Greek capital Pi, means a product -- it's the multiplication equivalent of the summation Sigma, $\Sigma$.)
* **Prior:** We started with a prior belief $\pi(\theta)$ on the value of the unknown parameter. In our case, we had the PMF
\[ \pi(0.2) = 0.2 \qquad \pi(0.5) = 0.6 \qquad \pi(0.8) = 0.2 . \]
* **Data:** We collected the data $\mathbf x$, which here had $x_1 = 1$, $x_2 = 1$, $x_3 = 1$ (with 1 denoting Heads and 0 denoting Tails).
* **Posterior:** We calculated the posterior distribution $\pi(\theta \mid \mathbf x)$ for the parameter *given* the data. We did this using Bayes' theorem:
\[ \pi(\theta \mid \mathbf x) = \frac{\pi(\theta) \, p(\mathbf x \mid \theta)}{p(\mathbf x)} \propto \pi(\theta) \, p(\mathbf x \mid \theta) .\]
We recovered the constant of proportionality -- that is, the denominator of Bayes' theorem -- because we knew $\pi(\theta \mid \mathbf x)$ was a conditional PMF so must add up to 1. We ended up with
\[ \pi(0.2 \mid \mathbf x) = 0.009 \qquad \pi(0.5 \mid \mathbf x) = 0.419 \qquad \pi(0.8 \mid \mathbf x) = 0.572 . \]

This is the framework of how Bayesian statistics works: model, prior, data, posterior. To lay it out more generally, the procedure goes like this:

::: {.thpart}
* **Model:** We start with a model for the data $\mathbf x$ that depends on one or more parameters $\theta$, as expressed by a conditional PMF (for discrete data) or PDF (for continuous data) $p(\mathbf x \mid \theta)$. This normally represents $n$ IID experiments, so
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n p(x_i \mid \theta) . \]
This conditional distribution is often called the **likelihood**.
* **Prior:** We have a prior distribution $\pi(\theta)$ for the parameter $\theta$, which can be either a PMF or PDF. The prior distribution represents our beliefs about the parameter before we collect the data; this can be based on previous evidence, expert opinion, personal intuition, etc.
* **Data:** We collect the data $\mathbf x$.
* **Posterior:** We then form the posterior distribution $\pi(\theta \mid \mathbf x)$ for the parameter given the data, using Bayes' theorem:
\begin{align*}
\pi(\theta \mid \mathbf x) &\propto \pi(\theta)\, p(\mathbf x \mid \theta) \\
\text{posterior} &\propto \text{prior} \times \text{likelihood} .
\end{align*}
This can either be a conditional PMF or PDF, but will be the same type as the prior $\pi(\theta)$.
:::

  




## Normal--normal model  {#normal-normal}

In our first example, the "joke coins" was a bit artificial, giving us a prior with only three points in its range. Its often more appropriate to have a prior distribution for a parameter that is continuous over a wide range of possibilities (although concentrated towards the more parameters values we believe are more probable.).

Consider a normal likelihood, where $X_1, X_2, \dots, X_n$ are IID $\text{N}(\theta, \sigma^2)$, and where the expectation $\theta$ is the unknown parameter but the variance $\sigma^2$ is known. This could model trying to measure some quantity $\theta$ with an instrument which is known to have a $\text{N}(0,\sigma^2)$ error. 
So the model has joint PDF
\begin{align*}
p(\mathbf x \mid \theta)
  &\propto \prod_{i=1}^n \exp \left(- \frac{(x_i - \theta)^2}{2\sigma^2}\right)
  &= \exp \left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i - \theta)^2}{\sigma^2} \right) \\
  &= \exp \left( - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \theta)^2 \right).
\end{align*}
(Again, we only worry about distributions up to proportionality, because we work out the multiplicative constant at the end.)

In fact, when doing Bayesian statistics, it's often convenient to write $\tau = 1/\sigma^2$ for the inverse of the known variance; this $\tau$ is called the **precision** and is also known. So with this notation, the model is that $X_1, X_2, \dots, X_n$ are IID $\text{N}(\theta, 1 / \tau^2)$, with joint PDF
\[ p(\mathbf x \mid \theta) \propto \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \theta)^2 \right) . \]

What about a prior for the unknown expectation $\theta$. Often an appropriate choice is a normal $\text{N}(\mu_0, 1/\tau_0)$ prior for the unknown expectation parameter $\theta$. This represents that we expect the quantity we are trying to mention to be around $\mu_0$, with an amount of uncertainty captured by the precision $\tau_0$ on the prior. So the prior PDF is 
\[ \pi(\theta) \propto \exp \left( - \frac{\tau_0}{2} (\theta - \mu_0)^2 \right) \]

Because both the prior distribution and the model for the data are normal, this is known as the **normal--normal model**.

Suppose we collect data $\mathbf x = (x_1, x_2, \dots, x_n)$, and recall that we write $\bar x = (\sum_i x_i)/n$ for the sample mean. 

To get the posterior distribution requires a bit of an algebra slog (see below), but the outcome is that the posterior distribution is 
\[ \pi(\theta \mid \mathbf x) \propto \exp \left( - \frac{\tau_0 + n\tau}{2} \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) , \]
which is (proportional to) the PDF for yet another normal distribution
\[ \theta \mid \mathbf x \sim \mathrm{N} \left( \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x, \ \frac{1}{\tau_0 + n\tau} \right)  . \]
In other words, the posterior expectation 
\[ \mathbb E(\theta \mid \mathbf x) = \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x \]
is a weighted average of the prior expectation $\mu_0$ and the mean of the data $\bar x$, and the more datapoints $n$ you get, the heavier the weighting on the data compared to the prior. Further, the precision has increased from the prior precision $\tau_0$ to the posterior precision $\tau_0 + n\tau$; so the more data we get, the larger the precision gets, so the smaller the variance gets, and the more sure we get about the true value of $\theta$. 


::: {.thpart}
*The algebra slog (non-examinable).* 
Recall that we can ignore multiplicative terms that don't contain $\theta$, thanks to our proportionality trick. But note also that a multiplicative term becomes an additive term inside an exponential. So, within an exponential, we can always ignore any "plus constants" that don't involve $\theta$.

So the prior can be written as
\begin{align*}
\pi(\theta) &\propto \exp\left(-\frac{\tau_0}{2} (\theta - \mu_0)^2 \right) \\
  &= \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta -  \frac{\tau_0}{2}\mu_0^2 \right) \\
  &\propto \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta \right) ,
\end{align*}
where we ignored the final constant term.

Similarly, the likelihood can be written as
\begin{align*}
p(\mathbf x \mid \theta) &= \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \theta)^2 \right) \\
  &= \exp \left( - \frac{\tau}{2} \sum_{i=1}^n x_i^2 + \tau \theta \sum_{i=1}^n x_i - \frac{\tau}{2} n\theta^2 \right) \\
  &\propto \exp \left(n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) ,
\end{align*}
where we ignored the first constant term in the second line, and recognised $\sum_i x_i$ as $n\bar x$, as we have done before.

Then Bayes' theorem gives us
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) \, p(\mathbf x \mid \theta) \\
  &\propto \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta \right) \times \exp \left(n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) \\
  &= \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta + n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) \\
  &= \exp \left( -\frac{\tau_0 + n\tau}{2} \theta^2 + (\tau_0\mu_0 + n\tau\bar x)\theta \right) \\
  &= \exp \left( -\left(\frac{\tau_0 + n\tau}{2}\right) \left(\theta^2 - 2 \frac{\tau_0\mu_0 + n\tau\bar x}{\tau_0 + n\tau}\theta \right) \right) \\
  &\propto \exp \left( - \tfrac{\tau_0 + n\tau}{2} \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) .
\end{align*}
In the final line, the squared term differs from the line above only by some additive constant. But this is exactly (proportional to) the PDF of a normal distribution with expectation
\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]
and precision $\tau_0 + n\tau$.
<!--

Before even getting to Bayes, let's remind ourselves from [Problem Sheet 4 Question B5](#P4-long) that
\[ \sum_{i=1}^n (x_i - \theta)^2 = \sum_{i=1}^n (x_i - \bar x)^2 + n(\theta - \bar x)^2 . \]
 Thus we get
\begin{align*}
 \exp \left( - \frac{\tau}{2} \sum_{i=1}^n  (x_i - \theta)^2 \right)
 &=  \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \bar x)^2 - n\tau(\theta - \bar x)^2 \right)\\
 &\propto \exp \left( -\frac{\tau}{2} n(\theta - \bar x)^2 \right) \\
 &= \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + n\tau \bar{x}^2 ) \right) \\
 &\propto \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta) \right),
\end{align*}
where we indeed ignored the first constant terms in the first line and the last constant term in the third line.

Now we can invoke Bayes' theorem, and continue to ignore "plus constants", to get
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) \, p(\mathbf x \mid \theta) \\
  &\propto \exp \left( -\tfrac{1}{2} \tau_0(\theta - \mu_0)^2  \right) \times \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &= \exp \left( -\tfrac{1}{2} (\tau_0(\theta - \mu_0)^2 + n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &=\exp \left( - \tfrac{1}{2} (\tau_0\theta^2 - 2\tau_0\mu_0\theta + \tau_0 \mu_0^2 + n\tau\theta^2 - 2n\tau\bar x\theta)\right) \\
  &\propto \exp \left( - \tfrac{1}{2} \big( (\tau_0 + n\tau)\theta^2 - 2 (\tau_0 \mu_0 +n\tau \bar x )\theta \big)\right) \\
  &= \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta^2 - 2 \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \theta \right)\right)  \\
  &\propto \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) . 
\end{align*}
This is (proportional to) the PDF for a normal distribution with expectation
\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]
and precision $\tau_0 + n\tau$.
-->
:::


