# (PART\*) Part III: Bayesian statistics  {-}

# The Bayesian idea  {#L19-bayes-idea}

## Example: fake coin?  {#fake-coin}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/BUZ_4DqHjIM"></iframe>
:::
::::



**Statistics** concerns how to draw conclusions from data; and **Bayesian statistics** is one particular framework for doing this. The idea of Bayesian statistics is that we use the data (together with Bayes' theorem) to update our "prior" ("before") beliefs about the underlying model to our "posterior" ("after") beliefs about the model *given* the data we have observed.

We will start by illustrating the main idea with an example.

::: {.example}
*A joke shop sells three types of coins: normal fair coins; Heads-biased coins, which land Heads with probability 0.8; and Tails-biased coins, which land Heads with probability 0.2. I pick up a coin and examine it; since it looks mostly like a normal coin, I believe there's 60% chance it's s fair coin, and a 20% chance it's biased either way. I decide to toss the coin four times, to gather some more evidence. The result is: Heads, Heads, Tails, Heads. How should I update my beliefs?*

We know how to do this: we use Bayes' theorem. We have
\begin{align*}
\mathbb P(\text{fair} \mid \text{HHTH}) &= \frac{\mathbb P(\text{fair})\, \mathbb P(\text{HHTH}\mid \text{fair})}{\mathbb P(\text{HHTH})} = \frac{0.6 \times 0.5^3 \times 0.5}{\mathbb P(\text{HHTH})} = \frac{0.0375}{\mathbb P(\text{HHTH})} \\
\mathbb P(\text{H-bias} \mid \text{HHTH}) &= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHTH}\mid \text{H-bias})}{\mathbb P(\text{HHTH})} = \frac{0.2 \times 0.8^3 \times 0.2}{\mathbb P(\text{HHTH})} = \frac{0.02048}{\mathbb P(\text{HHTH})} \\
\mathbb P(\text{T-bias} \mid \text{HHTH}) &= \frac{\mathbb P(\text{H-bias})\, \mathbb P(\text{HHTH}\mid \text{T-bias})}{\mathbb P(\text{HHTH})} = \frac{0.2 \times 0.2^3 \times 0.8}{\mathbb P(\text{HHTH})} = \frac{0.00128}{\mathbb P(\text{HHTH})}  .
\end{align*}
We also need to find $\mathbb P(\text{HHTH})$. We could do that using the law of total probability. But a convenient short-cut is to notice that the above three probabilities have to add up to 1, and so that common denominator must be $0.0375 + 0.02048 + 0.00128 = 0.05926$.

So, after tossing the coin four times, our belief has been updated from the "prior" (before) belief
\[ \mathbb P(\text{fair}) = 0.6 \qquad \mathbb P(\text{H-bias}) = 0.2 \qquad \mathbb P(\text{T-bias}) = 0.2 \]
to the "posterior" (after) belief
\[ \mathbb P(\text{fair} \mid \text{data}) = 0.633 \qquad \mathbb P(\text{H-bias}\mid \text{data}) = 0.346 \qquad \mathbb P(\text{T-bias}\mid \text{data}) = 0.026 . \]
Compared to our prior beliefs, our belief that the coin is fair has stayed about the same, although has increased a little bit; our belief the coin is biased towards Heads has increased quite a lot, up to about a third; our belief the coin is biased towards Tails has plummeted to a mere 3%.
:::


## Bayesian framework  {#bayesian-framework}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/9moh0KYc6fE"></iframe>
:::
::::


Let's think more systematically about what we did in the previous example.

* **Model:** The four coin tosses were modelled as four IID Bernoulli trials $X_1, X_2, X_3, X_4 \sim \text{Bern}(\theta)$ (if we let $X_i = 1$ denote that the $i$th coin was Heads). Here, the probability of Heads is some unknown parameter $\theta$. (Recall we talked about parametric models for data in Subsection \@ref(models).) This model gives a distribution that depends on the parameter: here we had a conditional PMF for one trial
\[ p(x \mid \theta) = \theta^{x} (1 - \theta)^{1- x}  \]
(this is a convenient way of writing the PMF for a Bernoulli trial), and the joint PMF for the IID trials
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^4 \theta^{x_i} (1 - \theta)^{1- x_i} = \theta^{x_1 + x_2 + x_3 + x_4} (1 - \theta)^{4- (x_1 + x_2 + x_3 + x_4)} . \]
* **Prior:** We started with a prior belief $\pi(\theta)$ on the value of the unknown parameter. In our case, we had the PMF
\[ \pi(0.2) = 0.2 \qquad \pi(0.5) = 0.6 \qquad \pi(0.8) = 0.2 . \]
* **Data:** We collected the data $\mathbf x$, which here had $x_1 = 1$, $x_2 = 1$, $x_3 = 0$, $x_4 = 1$ (with 1 denoting Heads and 0 denoting Tails).
* **Posterior:** We calculated the posterior distribution $\pi(\theta \mid \mathbf x)$ for the parameter *given* the data. We did this using Bayes' theorem:
\[ \pi(\theta \mid \mathbf x) = \frac{\pi(\theta) \, p(\mathbf x \mid \theta)}{p(\mathbf x)} \propto \pi(\theta) \, p(\mathbf x \mid \theta) .\]
We recovered the constant of proportionality -- that is, the denominator of Bayes' theorem -- because we knew $\pi(\theta \mid \mathbf x)$ was a conditional PMF so must add up to 1. We ended up with
\[ \pi(0.2 \mid \mathbf x) = 0.026 \qquad \pi(0.5 \mid \mathbf x) = 0.633 \qquad \pi(0.8 \mid \mathbf x) = 0.346 . \]

This is the framework of how Bayesian statistics works: model, prior, data, posterior. To lay it out more generally, the procedure goes like this:

::: {.thpart}
* **Model:** We start with a model for the data $\mathbf x$ that depends on one or more parameters $\theta$, as expressed by a conditional PMF (for discrete data) or PDF (for continuous data) $p(\mathbf x \mid \theta)$. This normally represents $n$ IID experiments, so
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n p(x_i \mid \theta) . \]
This conditional distribution is often called the **likelihood**.
* **Prior:** We have a prior distribution $\pi(\theta)$ for the parameter $\theta$, which can be either a PMF or PDF. The prior distribution represents our beliefs about the parameter before we collect the data; this can be based on previous evidence, expert opinion, personal intuition, etc.
* **Data:** We collect the data $\mathbf x$.
* **Posterior:** We then form the posterior distribution $\pi(\theta \mid \mathbf x)$ for the parameter given the data, using Bayes' theorem:
\begin{align*}
\pi(\theta \mid \mathbf x) &\propto \pi(\theta)\, p(\mathbf x \mid \theta) \\
\text{posterior} &\propto \text{prior} \times \text{likelihood} .
\end{align*}
This can either be a conditional PMF or PDF, but will be the same type as the prior $\pi(\theta)$.
:::

  


## Modern Bayesian statistics  {#modern-bayes}

In this section, we've given just a brief taster of Bayesian statistics. Bayesian statistics is a deep and complicated subject, and you may have the opportunity to find out a lot more about it later in your university career.

We have seen that in Bayesian statistics, one brings in a subjective "prior" based on previous beliefs and evidence, then updates this prior based on the data. This contrasts with the more traditional **frequentist statistics**. In frequentist one uses only the data -- no prior beliefs! -- and judges to what extent the data is consistent or inconsistent with a hypothesis, without weighing in on how likely such a hypothesis is. (Frequentist statistics is the main subject studied in MATH1712 Probability and Statistics II.)

In the two main examples of Bayesian statistics we have looked at -- the Bernoulli likelihood and the normal likelihood -- we ended up with a posterior in the same parametric family as prior, just with different parameters. Such a prior is called a "conjugate prior". Of course, these are very convenient and easy to work with. However, with more complicated likelihoods and more complicated priors -- especially those not with a single parameter but with many parameters -- calculating the posterior distribution can be very difficult. In particular, working out the constant of proportionality (even just approximately) and/or sampling from the posterior distribution are very hard problems.

For this reason, Bayesian statistics was for a long time a minor area of statistics. However, increases in computer power in the 1980s made some of these problems more tractable, and Bayesian statistics has increased in importance and popularity since then.

For a while, there was an occasionally fierce debate between "Bayesians" and "frequentists". Frequentists thought that bringing subjective personal beliefs into things was unmathematical, while Bayesians thought that ignoring how plausible a hypothesis is before testing it is unscientific. The debate has now largely dissipated, and it is largely accepted that modern statisticians need to know about both frequentist and Bayesian methods.

There are still plenty of open problems in Bayesian statistics, and lots of these involve the computational side: finding algorithms that can efficiently calculate the normalising constants in posterior distributions or sample from those posterior distributions, especially when the parameter(s) have very high dimension.


## Summary  {#summary-10 .unnumbered}

::: {.mysummary}
* In Bayesian statistics, we start with a prior distribution for a parameter $\theta$, and update to a posterior distribution given the data $\mathbf x$, through $\pi(\theta \mid \mathbf x) \propto \pi(\theta)p(\mathbf x \mid \theta)$, or $\text{posterior} \propto \text{prior} \times \text{likelihood}$.
* The Beta distribution is a useful family of distributions to use as priors for probability parameters.
* A Beta prior for a Bernoulli likelihood leads to a Beta posterior with different parameters.
* A normal prior for the expectation of a normal likelihood wioth known variance leads to a normal posterior with different parameters.
:::