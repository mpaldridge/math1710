# Bayesian statistics II {#L20-bayes-ii}


## Beta--Bernoulli model  {#beta-bern}

Consider a Bernoulli likelihood, where $X_1, X_2, \dots, X_n$ are IID $\text{Bern}(\theta)$, so have joint PMF
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n \theta^{x_i} (1-\theta)^{1 - x_i} = \theta^{\sum_i x_i} (1 - \theta)^{n-\sum_i x_i} = \theta^y (1 - \theta)^{n-y}, \]
where we have written $y = \sum_i x_i$ for the total number of successes.
Consider further using a $\text{Beta}(\alpha, \beta)$ prior for $\theta$, so that
\[ \pi(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta - 1} \propto \theta^{\alpha-1} (1-\theta)^{\beta - 1} \]
(Because we're going to use the "posterior has to add up to 1" trick at the end, we're free to drop constants whenever we want.) This is known as the **Beta--Bernoulli model**.

Suppose we collect data $\mathbf x = (x_1, x_2, \dots, x_i)$, with $y = \sum_i x_i$ successes. What now is the posterior distribution for $\theta$ given this data?

Using Bayes' theorem, we have
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) p(\mathbf x \mid \theta) \\
  &= \theta^{\alpha-1} (1-\theta)^{\beta - 1} \times \theta^y (1 - \theta)^{n-y} \\
  &= \theta^{\alpha + y - 1} (1 - \theta)^{\beta + n - y - 1} .
\end{align*}
We can recognise immediately that this is proportional to the PDF for a $\text{Beta}(\alpha + y, \beta + n - y)$ distribution, so in particular, the constant of proportionality must be $1/B(\alpha + y, \beta + n - y)$.

So we see that, like the prior, the posterior is also a Beta distribution, where the first parameter has gone from $\alpha$ to $\alpha + y$ and the second parameter has gone from $\beta$ to $\beta + (n-y)$. In other words, $\alpha$ has increased by the number of successes, and $\beta$ has increased by the number of failures.
The expectation has gone from the prior expectation
\[ \frac{\alpha}{\alpha + \beta} \]
to the posterior expectation
\[ \frac{\alpha + y}{\alpha + \beta + n} .\]
This can be thought of as a sort of average between the prior expectation $\alpha/(\alpha + \beta)$ and the mean of the data $y/n$.

## Normal--normal model  {#normal-normal}

In our first example, the "joke coins" was a bit artificial, giving us a prior with only three points in its range. Its often more appropriate to have a prior distribution for a parameter that is continuous over a wide range of possibilities (although concentrated towards the more parameters values we believe are more probable.).

Consider a normal likelihood, where $X_1, X_2, \dots, X_n$ are IID $\text{N}(\theta, \sigma^2)$, and where the expectation $\theta$ is the unknown parameter but the variance $\sigma^2$ is known. This could model trying to measure some quantity $\theta$ with an instrument which is known to have a $\text{N}(0,\sigma^2)$ error. 
So the model has joint PDF
\begin{align*}
p(\mathbf x \mid \theta)
  &\propto \prod_{i=1}^n \exp \left(- \frac{(x_i - \theta)^2}{2\sigma^2}\right)
  &= \exp \left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i - \theta)^2}{\sigma^2} \right) \\
  &= \exp \left( - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \theta)^2 \right).
\end{align*}
(Again, we only worry about distributions up to proportionality, because we work out the multiplicative constant at the end.)

In fact, when doing Bayesian statistics, it's often convenient to write $\tau = 1/\sigma^2$ for the inverse of the known variance; this $\tau$ is called the **precision** and is also known. So with this notation, the model is that $X_1, X_2, \dots, X_n$ are IID $\text{N}(\theta, 1 / \tau^2)$, with joint PDF
\[ p(\mathbf x \mid \theta) \propto \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \theta)^2 \right) . \]

What about a prior for the unknown expectation $\theta$. Often an appropriate choice is a normal $\text{N}(\mu_0, 1/\tau_0)$ prior for the unknown expectation parameter $\theta$. This represents that we expect the quantity we are trying to mention to be around $\mu_0$, with an amount of uncertainty captured by the precision $\tau_0$ on the prior. So the prior PDF is 
\[ \pi(\theta) \propto \exp \left( - \frac{\tau_0}{2} (\theta - \mu_0)^2 \right) \]

Because both the prior distribution and the model for the data are normal, this is known as the **normal--normal model**.

Suppose we collect data $\mathbf x = (x_1, x_2, \dots, x_n)$, and recall that we write $\bar x = (\sum_i x_i)/n$ for the sample mean. 

To get the posterior distribution requires a bit of an algebra slog (see below), but the outcome is that the posterior distribution is 
\[ \pi(\theta \mid \mathbf x) \propto \exp \left( - \frac{\tau_0 + n\tau}{2} \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) , \]
which is (proportional to) the PDF for yet another normal distribution
\[ \theta \mid \mathbf x \sim \mathrm{N} \left( \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x, \ \frac{1}{\tau_0 + n\tau} \right)  . \]
In other words, the posterior expectation 
\[ \mathbb E(\theta \mid \mathbf x) = \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x \]
is a weighted average of the prior expectation $\mu_0$ and the mean of the data $\bar x$, and the more datapoints $n$ you get, the heavier the weighting on the data compared to the prior. Further, the precision has increased from the prior precision $\tau_0$ to the posterior precision $\tau_0 + n\tau$; so the more data we get, the larger the precision gets, so the smaller the variance gets, and the more sure we get about the true value of $\theta$. 


::: {.thpart}
*The algebra slog (non-examinable).* 
Recall that we can ignore multiplicative terms that don't contain $\theta$, thanks to our proportionality trick. But note also that a multiplicative term becomes an additive term inside an exponential. So, within an exponential, we can always ignore any "plus constants" that don't involve $\theta$.

So the prior can be written as
\begin{align*}
\pi(\theta) &\propto \exp\left(-\frac{\tau_0}{2} (\theta - \mu_0)^2 \right) \\
  &= \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta -  \frac{\tau_0}{2}\mu_0^2 \right) \\
  &\propto \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta \right) ,
\end{align*}
where we ignored the final constant term.

Similarly, the likelihood can be written as
\begin{align*}
p(\mathbf x \mid \theta) &= \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \theta)^2 \right) \\
  &= \exp \left( - \frac{\tau}{2} \sum_{i=1}^n x_i^2 + \tau \theta \sum_{i=1}^n x_i - \frac{\tau}{2} n\theta^2 \right) \\
  &\propto \exp \left(n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) ,
\end{align*}
where we ignored the first constant term in the second line, and recognised $\sum_i x_i$ as $n\bar x$, as we have done before.

Then Bayes' theorem gives us
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) \, p(\mathbf x \mid \theta) \\
  &\propto \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta \right) \times \exp \left(n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) \\
  &= \exp\left(-\frac{\tau_0}{2}\theta^2 + \tau_0\mu_0\theta + n \tau \theta \bar x - \frac{\tau}{2} n\theta^2 \right) \\
  &= \exp \left( -\frac{\tau_0 + n\tau}{2} \theta^2 + (\tau_0\mu_0 + n\tau\bar x)\theta \right) \\
  &= \exp \left( -\left(\frac{\tau_0 + n\tau}{2}\right) \left(\theta^2 - 2 \frac{\tau_0\mu_0 + n\tau\bar x}{\tau_0 + n\tau}\theta \right) \right) \\
  &\propto \exp \left( - \tfrac{\tau_0 + n\tau}{2} \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) .
\end{align*}
In the final line, the squared term differs from the line above only by some additive constant. But this is exactly (proportional to) the PDF of a normal distribution with expectation
\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]
and precision $\tau_0 + n\tau$.
<!--

Before even getting to Bayes, let's remind ourselves from [Problem Sheet 4 Question B5](#P4-long) that
\[ \sum_{i=1}^n (x_i - \theta)^2 = \sum_{i=1}^n (x_i - \bar x)^2 + n(\theta - \bar x)^2 . \]
 Thus we get
\begin{align*}
 \exp \left( - \frac{\tau}{2} \sum_{i=1}^n  (x_i - \theta)^2 \right)
 &=  \exp \left( - \frac{\tau}{2} \sum_{i=1}^n (x_i - \bar x)^2 - n\tau(\theta - \bar x)^2 \right)\\
 &\propto \exp \left( -\frac{\tau}{2} n(\theta - \bar x)^2 \right) \\
 &= \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + n\tau \bar{x}^2 ) \right) \\
 &\propto \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta) \right),
\end{align*}
where we indeed ignored the first constant terms in the first line and the last constant term in the third line.

Now we can invoke Bayes' theorem, and continue to ignore "plus constants", to get
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) \, p(\mathbf x \mid \theta) \\
  &\propto \exp \left( -\tfrac{1}{2} \tau_0(\theta - \mu_0)^2  \right) \times \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &= \exp \left( -\tfrac{1}{2} (\tau_0(\theta - \mu_0)^2 + n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &=\exp \left( - \tfrac{1}{2} (\tau_0\theta^2 - 2\tau_0\mu_0\theta + \tau_0 \mu_0^2 + n\tau\theta^2 - 2n\tau\bar x\theta)\right) \\
  &\propto \exp \left( - \tfrac{1}{2} \big( (\tau_0 + n\tau)\theta^2 - 2 (\tau_0 \mu_0 +n\tau \bar x )\theta \big)\right) \\
  &= \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta^2 - 2 \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \theta \right)\right)  \\
  &\propto \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) . 
\end{align*}
This is (proportional to) the PDF for a normal distribution with expectation
\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]
and precision $\tau_0 + n\tau$.
-->
:::

## Modern Bayesian statistics  {#modern-bayes}

In this section, we've given just a brief taster of Bayesian statistics. Bayesian statistics is a deep and complicated subject, and you may have the opportunity to find out a lot more about it later in your university career.

We have seen that in Bayesian statistics, one brings in a subjective "prior" based on previous beliefs and evidence, then updates this prior based on the data. This contrasts with the more traditional **frequentist statistics**. In frequentist one uses only the data -- no prior beliefs! -- and judges to what extent the data is consistent or inconsistent with a hypothesis, without weighing in on how likely such a hypothesis is. (Frequentist statistics is the main subject studied in MATH1712 Probability and Statistics II.)

In the two main examples of Bayesian statistics we have looked at -- the Bernoulli likelihood and the normal likelihood -- we ended up with a posterior in the same parametric family as prior, just with different parameters. Such a prior is called a "conjugate prior". Of course, these are very convenient and easy to work with. However, with more complicated likelihoods and more complicated priors -- especially those not with a single parameter but with many parameters -- calculating the posterior distribution can be very difficult. In particular, working out the constant of proportionality (even just approximately) and/or sampling from the posterior distribution are very hard problems.

For this reason, Bayesian statistics was for a long time a minor area of statistics. However, increases in computer power in the 1980s made some of these problems more tractable, and Bayesian statistics has increased in importance and popularity since then.

For a while, there was an occasionally fierce debate between "Bayesians" and "frequentists". Frequentists thought that bringing subjective personal beliefs into things was unmathematical, while Bayesians thought that ignoring how plausible a hypothesis is before testing it is unscientific. The debate has now largely dissipated, and it is largely accepted that modern statisticians need to know about both frequentist and Bayesian methods.

There are still plenty of open problems in Bayesian statistics, and lots of these involve the computational side: finding algorithms that can efficiently calculate the normalising constants in posterior distributions or sample from those posterior distributions, especially when the parameter(s) have very high dimension.


## Summary  {#summary-10 .unnumbered}

::: {.mysummary}
* In Bayesian statistics, we start with a prior distribution for a parameter $\theta$, and update to a posterior distribution given the data $\mathbf x$, through $\pi(\theta \mid \mathbf x) \propto \pi(\theta)p(\mathbf x \mid \theta)$, or $\text{posterior} \propto \text{prior} \times \text{likelihood}$.
* The Beta distribution is a useful family of distributions to use as priors for probability parameters.
* A Beta prior for a Bernoulli likelihood leads to a Beta posterior with different parameters.
* A normal prior for the expectation of a normal likelihood wioth known variance leads to a normal posterior with different parameters.
:::