# Bayesian models {#L20-bayes-models}

## Beta distribution  {#beta}

In our fake-coin example, we had a prior PMF for the parameter $\theta = p$ that could only take one of three possible values. But when doing Bayesian statistics with a parameter that represents a probability, it makes more sense to have a prior PDF that covers the whole interval $[0,1]$. After all, any parameter value that is given a probability of 0 in the prior always has a probability 0 in the posterior as well, no matter how strong the evidence in its favour; it's considered good practice to only put 0 prior probability on parameter values that are *literally impossible*, such as probabilities below 0 or above 1. (This is sometimes called ["Cromwell's rule"](https://en.wikipedia.org/wiki/Cromwell%27s_rule).)

One useful family of distributions to use as a prior distribution for a probability parameter is the Beta distribution, whose range is the whole interval $[0,1]$.

::: {.definition}
A continuous random variable $X$ is said to have the **Beta distribution** with parameters $\alpha$ and $\beta$ if it has the PDF
\[ f(x) = \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^{\beta - 1} \qquad \text{for $0 \leq x \leq 1$}  \]
and 0 otherwise. Here, the constant
\[ B(\alpha, \beta) = \int_0^1 x^{\alpha-1} (1-x)^{\beta - 1} \, \mathrm dx , \]
known as the "Beta function", ensures that the PDF integrates to 1. We write $X \sim \text{Beta}(\alpha, \beta)$.
:::

::: {.theorem}
Let $X \sim \text{Beta}(\alpha,\beta)$. Then

1. $\mathbb EX = \displaystyle\frac{\alpha}{\alpha + \beta}$
2. $\Var(X) = \displaystyle\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} = \displaystyle\frac{\mu(1-\mu)}{\alpha+\beta + 1}$, where $\mu = \mathbb EX$.
:::

(Proving this requires some awkward messing around with Gamma functions, which we won't bother with here.)

So the idea is that the expectation of $X$ is decided on by the *relative* values of $\alpha$ and $\beta$, while the variance is decided by the *total* value of $\alpha$ and $\beta$. The following two pictures illustrate this:

```{r beta-pic-1, cache = TRUE, echo = FALSE}
curve(dbeta(x, 2, 8), type = "l", n = 1001, lwd = 2, col = "blue", from = 0, to = 1, ylab = "probability density function f(x)", ylim = c(0,4))
curve(dbeta(x, 5, 5), type = "l", n = 1001, lwd = 2, col = "red", add = TRUE)
curve(dbeta(x, 7, 3), type = "l", n = 1001, lwd = 2, col = "green", add = TRUE)
legend("topright", c("Beta(2, 8)", "Beta(5, 5)", "Beta(7, 3)"), col = c("blue", "red", "green"), lwd = 2)
```

```{r beta-pic-2, cache = TRUE, echo = FALSE}
curve(dbeta(x, 64, 128), type = "l", n = 1001, lwd = 2, col = "green", from = 0, to = 1, ylab = "probability density function f(x)")
curve(dbeta(x, 16, 32), type = "l", n = 1001, lwd = 2, col = "red", add = TRUE)
curve(dbeta(x, 4, 8), type = "l", n = 1001, lwd = 2, col = "blue", add = TRUE)
legend("topright", c("Beta(4, 8)", "Beta(16, 32)", "Beta(64, 128)"), col = c("blue", "red", "green"), lwd = 2)
```

Note also that $\text{Beta}(1,1)$ is the continuous uniform distribution from Example \@ref(exm:unifex).

::: {.example}
*A statistician is studying the probability $\theta$ that ordinary coins land Heads. She would like to use a prior distribution for $\theta$ with prior expectation $0.5$ and prior standard deviation $0.01$. What Beta distribution would be appropriate to use?*

To get $\mathbb E\theta = 0.5$, we need $\alpha = \beta$. Then the variance, which needs to be $0.01^2 = 0.0001$, is
\[ \Var(\theta) = \frac{\mu(1-\mu)}{\alpha+\beta+1} = \frac{0.25}{\alpha + \beta + 1} . \]
This requires $\alpha = \beta = 1250$. (Well, actually $1249.5$.)
:::

## Beta--Bernoulli model  {#beta-bern}

Consider a Bernoulli likelihood, where $X_1, X_2, \dots, X_n$ are IID $\text{Bern}(\theta)$, so have joint PMF
\[ p(\mathbf x \mid \theta) = \prod_{i=1}^n \theta^{x_i} (1-\theta)^{1 - x_i} = \theta^{\sum_i x_i} (1 - \theta)^{n-\sum_i x_i} = \theta^y (1 - \theta)^{n-y}, \]
where we have written $y = \sum_i x_i$ for the total number of successes.
Consider further using a $\text{Beta}(\alpha, \beta)$ prior for $\theta$, so that
\[ \pi(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta - 1} \propto \theta^{\alpha-1} (1-\theta)^{\beta - 1} \]
(Because we're going to use the "posterior has to add up to 1" trick at the end, we're free to drop constants whenever we want.) This is known as the **Beta--Bernoulli model**.

Suppose we collect data $\mathbf x = (x_1, x_2, \dots, x_i)$, with $y = \sum_i x_i$ successes. What now is the posterior distribution for $\theta$ given this data?

Using Bayes' theorem, we have
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) p(\mathbf x \mid \theta) \\
  &= \theta^{\alpha-1} (1-\theta)^{\beta - 1} \times \theta^y (1 - \theta)^{n-y} \\
  &= \theta^{\alpha + y - 1} (1 - \theta)^{\beta + n - y - 1} .
\end{align*}
We can recognise immediately that this is proportional to the PDF for a $\text{Beta}(\alpha + y, \beta + n - y)$ distribution, so in particular, the constant of proportionality must be $1/B(\alpha + y, \beta + n - y)$.

So we see that, like the prior, the posterior is also a Beta distribution, where the first parameter has gone from $\alpha$ to $\alpha + y$ and the second parameter has gone from $\beta$ to $\beta + (n-y)$. In other words, $\alpha$ has increased by the number of successes, and $\beta$ has increased by the number of failures.
The expectation has gone from the prior expectation
\[ \frac{\alpha}{\alpha + \beta} \]
to the posterior expectation
\[ \frac{\alpha + y}{\alpha + \beta + n} .\]
This can be thought of as a sort of average between the prior expectation $\alpha/(\alpha + \beta)$ and the mean of the data $y/n$.

## Normal--normal model  {#normal-normal}

Consider a normal likelihood, where $X_1, X_2, \dots, X_n$ are IID $\text{N}(\theta, \sigma^2)$, and where the expectation $\theta$ is the unknown parameter but the variance $\sigma^2$ is known. So the model has joint PDF
\[ p(\mathbf x \mid \theta) \propto \prod_{i=1}^n \exp \left(- \frac{(x_i - \theta)^2}{2\sigma^2}\right) = \exp \left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i - \theta)^2}{\sigma^2} \right) . \]
(Again, we only worry about distributions up to proportionality, because we work out the multiplicative constant at the end.)
In fact, when doing Bayesian statistics, it's often convenient to write $\tau = 1/\sigma^2$ for the inverse of the known variance; this $\tau$ is called the **precision** and is also known. So with this notation, the model is
\[ p(\mathbf x \mid \theta) \propto \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau(x_i - \theta)^2 \right) . \]


Consider further using a normal $\text{N}(\mu_0, 1/\tau_0)$ prior for the unknown expectation parameter $\theta$. So the prior PDF is 
\[ \pi(\theta) \propto \exp \left( - \tfrac{1}{2} \tau_0(\theta - \mu_0)^2 \right) \]
This is known as the **normal--normal model**.

Suppose we collect data $\mathbf x = (x_1, x_2, \dots, x_n)$, and recall that we write $\bar x = (\sum_i x_i)/n$ for the sample mean. 

To get the posterior distribution requires a bit of an algebra slog (see below), but the outcome is that the posterior distribution is
\[ \theta \mid \mathbf x \sim \mathrm{N} \left( \frac{\tau_0}{\tau_0 + n\tau} \mu_0 + \frac{n\tau}{\tau_0 + n\tau} \bar x, \frac{1}{\tau_0 + n\tau} \right)  . \]
In other words, the posterior expectation is a weighted average of the prior expectation $\mu_0$ and the mean of the data $\bar x$, and the more datapoints $n$ you get, the heavier the weighting on the data compared to the prior. Further, the precision has increased from the prior precision $\tau_0$ to the posterior precision $\tau_0 + n\tau$; so the more data we get, the larger the precision gets, so the smaller the variance gets, and the more sure we get about the true value of $\theta$. 


::: {.thpart}
*The algebra slog.* Before even getting to Bayes, let's remind ourselves from [Problem Sheet 4 Question B5](#P4-long) that
\[ \sum_{i=1}^n \tau(x_i - \theta)^2 = \sum_{i=1}^n \tau(x_i - \bar x)^2 + n\tau(\theta - \bar x)^2 . \]
Recalling that we can ignore multiplicative terms that don't contain $\theta$, thanks to our proportionality trick, and note also that a multiplicative term becomes an additive term inside an exponential. So we can always ignore any "plus constants" that don't involve $\theta$ that are inside an exponential. Thus we get
\begin{align*}
 \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau (x_i - \theta)^2 \right)
 &=  \exp \left( - \frac{1}{2} \sum_{i=1}^n \tau(x_i - \bar x)^2 - n\tau(\theta - \bar x)^2 \right)\\
 &\propto \exp \left( -\tfrac{1}{2} n\tau(\theta - \bar x)^2 \right) \\
 &= \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta + n\tau \bar{x}^2 ) \right) \\
 &\propto \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta) \right).
\end{align*}

Now we can invoke Bayes' theorem, and continue to ignore "plus constants", to get
\begin{align*}
\pi(\mathbf x \mid \theta)
  &\propto \pi(\theta) \, p(\mathbf x \mid \theta) \\
  &\propto \exp \left( -\tfrac{1}{2} \tau_0(\theta - \mu_0)^2  \right) \times \exp \left( -\tfrac{1}{2} (n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &= \exp \left( -\tfrac{1}{2} (\tau_0(\theta - \mu_0)^2 + n\tau\theta^2 - 2n\tau\bar x\theta ) \right) \\
  &=\exp \left( - \tfrac{1}{2} (\tau_0\theta^2 - 2\tau_0\mu_0\theta + \tau_0 \mu_0^2 + n\tau\theta^2 - 2n\tau\bar x\theta)\right) \\
  &\propto \exp \left( - \tfrac{1}{2} \big( (\tau_0 + n\tau)\theta^2 - 2 (\tau_0 \mu_0 +n\tau \bar x )\theta \big)\right) \\
  &= \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta^2 - 2 \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \theta \right)\right)  \\
  &\propto \exp \left( - \tfrac{1}{2}(\tau_0 + n\tau) \left( \theta - \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \right)^{\!2} \right) . 
\end{align*}
This is (proportional to) the PDF for a normal distribution with expectation
\[ \frac{\tau_0 \mu_0 +n\tau \bar x }{\tau_0 + n\tau} \]
and precision $\tau_0 + n\tau$.
:::