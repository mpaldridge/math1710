# Independence and conditional probability  {#S04-conditional}

## Independent events  {#independent-events}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/9lhF4WEoJxk"></iframe>
:::
::::

*Suppose 40% of people have blond hair, and 20% of people have blue eyes. What proportion of people have both blond hair and blue eyes?*

The answer to this question is: we don't know. The question doesn't give us enough information to tell. However, *if* it were the case that having blond hair didn't effect your chance of having blue eyes, then we could work out the answer. If that were true, we would think that the 20% of people with blue eyes equally made up both 20% of the blonds and also 20% of the non-blonds. Thus the proportion of people with blond hair and blue eyes would be this 20% of the 40% of people with blond hair, and 20% of 40% is $0.2 \times 0.4 = 0.08$, or 8%.

To put it in probability language, *if* blond hair and blue eyes were unrelated, then we would expect that
  \[ \mathbb P(\text{blond hair and blue eyes}) = \mathbb P(\text{blond hair}) \times \mathbb P(\text{blue eyes}) . \]
This is an important property known as "independence".

::: {.definition}
Two events $A$ and $B$ are said to be **independent** if
\[ \mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B) .  \]
:::

There are two ways we can use this definition.

* If we know $\mathbb P(A)$, $\mathbb P(B)$, and $\mathbb P(A \cap B)$, then we can find out whether or not $A$ and $B$ are independent by checking whether or not $\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)$.
* If we know $\mathbb P(A)$ and $\mathbb P(B)$ and we know that $A$ and $B$ are independent, then we can find $\mathbb P(A \cap B)$ by calculating $\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)$.

In this second case, we might know $A$ and $B$ are independent because we are specifically told they are. But we might reason that $A$ and $B$ because the related experiments are not physically related -- for example if we roll a dice then toss a coin, we might reason that $\{\text{roll a 5}\}$ and $\{\text{the coin lands Heads}\}$ must be independent because the dice roll doesn't effect the coin toss, and use the independence assumption in calculations.

::: {.example}
*Consider rolling a dice. Let $A = \{\text{even number}\} = \{2,4,6\}$, and let $B = \{\text{roll at least 4}\} = \{4,5,6\}$. Are $A$ and $B$ independent?*

Clearly we have $\mathbb P(A) = \frac36 = \frac12$ and $\mathbb P(B) = \frac 36 = \frac12$. The intersection is $A \cap B = \{4,6\}$, so $\mathbb P(A \cap B) = \frac26 = \frac13$. So we see that
\[ \mathbb P(A\cap B) = \frac13  \qquad \text{and} \qquad  \mathbb P(A)\, \mathbb P(B) = \frac12 \times \frac12 = \frac14 . \]
So $\mathbb P(A \cap B) \neq \mathbb P(A)\, \mathbb P(B)$, and the two events are not independent.
:::

::: {.example}
*A biased coin has probability $p$ of landing Heads and probability $1-p$ of landing Tails. You toss the coin 3 times. Assuming tosses of the coin are independent, calculate the probability of getting exactly 2 Heads.*

There are three ways we could get exactly 2 Heads: HHT, HTH, or THH. For the first of these,
\[ \mathbb P(\text{HHT}) = \mathbb P(\text{first coin H} \cap \text{second coin H} \cap \text{third coin T}) . \]
Since tosses of the coin are independent, we therefore have
\begin{align*}
\mathbb P(\text{HHT})
  &= \mathbb P(\text{first coin H}) \times \mathbb P ( \text{second coin H} )\times \mathbb P(\text{third coin T}) \\
  &=p \times p \times (1-p) \\
  &= p^2(1-p).
\end{align*}

Similarly,
\[ \mathbb P(\text{HTH}) = \mathbb P(\text{THH}) = p^2(1-p) \]
also.

Finally, because the events are disjoint, we have
\[ \mathbb P(\text{HHT} \cup\text{HTH} \cup \text{THH}) = \mathbb P(\text{HHT} ) + \mathbb P(\text{HTH}) + \mathbb P(\text{THH}) = 3p^2(1-p) . \]
:::


## Conditional probability  {#conditional}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/x3wISv0RWic"></iframe>
:::
::::

Let us to return to the example of blond hair and blue eyes. Suppose the population statistics are like this:

| | **Brown hair** | **Blond hair** | **Total** |
|:--------------:|:---:|:---:|:----:|
| **Brown eyes** | 50% | 30% |  80% |
| **Blue eyes**  | 10% | 10% |  20% |
| **Total**      | 60% | 40% | 100% |

(It turns out that $\mathbb P(\text{blond hair and blue eyes}) = 0.1 \neq 0.08$, so they are not independent.)

We know that 20% of people have blue eyes. But suppose you already know that someone has blond hair: what then is their probability of have blue eyes *given* that they have blond hair?

Well, the 40% of blond-haired people is made up of the 10% of people who also have blue eyes to go along with their blond hair, and the 30% of people who have brown eyes to go along with their blond hair. So of the 40% of blond-haired people, three times as many have brown eyes, so only one quarter of that 40% have blue eyes. If we use a vertical line $|$ in a probability  to mean "given" (or "assuming that" or "conditional upon"), then we can write this as
\[  \mathbb P(\text{blue eyes} \mid \text{blond hair}) = \frac{\mathbb P(\text{blue eyes and blond hair})}{\mathbb P(\text{blond hair})} = \frac{0.1}{0.4} = \frac14. \]

What we've seen here is called a "conditional probability".

::: {.definition}
Let $A$ and $B$ be events, with $\mathbb P(A) > 0$. Then the **conditional probability of $B$ given $A$** is defined to be
\[  \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} . \]
:::

The condition $\mathbb P(A) > 0$ is to ensure we don't have any "divide by 0" errors. (I normally won't bother saying this explicitly -- any statement about conditional probability will implicitly assume that the event being conditioned on has nonzero probability.)

As with independence, conditional probability can be used in different ways: given any two of $\mathbb P(A)$, $\mathbb P(A \cap B)$, and $\mathbb P(B \mid A)$ you can work out the other one.

Conditional probability ties in with independence in an important way. Suppose $A$ and $B$ are independent, so $\mathbb P(A \cap B) = \mathbb P(A) \, \mathbb P(B)$. Then the conditional probability becomes
\[ \mathbb P(B \mid A) = \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} =  \frac{\mathbb P(A) \, \mathbb P(B)}{\mathbb P(A)} = \mathbb P(B) , \]
so $\mathbb P(B \mid A) = \mathbb P(B)$. In other words, if $A$ and $B$ are independent, then $A$ happening doesn't affect the probability of $B$ happening (and vice versa).

So when we have independence, $\mathbb P(A \cap B) = \mathbb P(A)\,\mathbb P(B)$, and the mathematics is quite easy. But conditional probability tells us how things work when we don't have independence.


## Chain rule  {#chain-rule}


We can rewrite the definition of conditional probability like this:
\[ \mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B \mid A). \]
This can be a useful way to think when $A$ concerns the first stage of an experiment and $B$ the second stage. This says that the probability $A$ happens then $B$ happens is equal to the probability $A$ happens multiplied the probability, given that $A$ has happened, that $B$ then happens too.

We can extend this to more events. For three events, we have
\begin{align*}
\mathbb P(A \cap B \cap C)
  &= \mathbb P(A \cap B) \, \mathbb P(C \mid A \cap B) \\
  &= \mathbb P(A) \, \mathbb P(B \mid A)\, \mathbb P(C \mid A \cap B) ,
\end{align*}
which can be useful when we have three stages of an experiment.

Continuing that process, we get a general rule.

::: {.theorem #thchain name="Chain rule"}
For events $A_1, A_2, \dots, A_n$, we have
\begin{multline*}  \mathbb P(A_1 \cap A_2 \cap \cdots \cap A_n) \\
  = \mathbb P(A_1) \, \mathbb P(A_2 \mid A_1) \, \mathbb P(A_3 \mid A_1 \cap A_2) \cdots \mathbb P(A_n \mid A_1 \cap A_2 \cap \cdots \cap  A_{n-1}) .\end{multline*}
:::

Often questions that can be solved using the classical probability counting methods from Section 3 also be solved in stages using the chain rule. (It's a matter of personal taste which you prefer.)

::: {.example}
*Recall the Lotto problem from Example \@ref(exm:lotto): What is the probability we match 6 balls from 59?*

Let $A_1, A_2, \dots, A_6$ be the events that the first, second, ..., sixth balls out of the machine are on our ticket. Clearly $\mathbb P(A_1) = \frac{6}{59}$, as we have six numbers the ball could match. Then the conditional probability that the second ball matches given that the first ball matched is $\mathbb P(A_2 \mid A_1) = \frac{5}{58}$, because there are 58 balls left in the machine and, given that we got the first number right, there are 5 numbers left on our ticket. Similarly, $\mathbb P(A_3 \mid A_1 \cap A_2) = \frac{4}{57}$, and so on, down to $\mathbb P(A_6 \mid A_1 \cap \cdots\cap A_5) = \frac{1}{54}$.

So, using the chain rule, we get
\begin{align*}
\mathbb P(A_1 \cap A_2 &\cap \cdots \cap A_6) \\
&= \mathbb P(A_1) \, \mathbb P(A_2 \mid A_1) \, \mathbb P(A_3 \mid A_1 \cap A_2) \cdots \mathbb P(A_6 \mid A_1 \cap \cdots \cap A_5) \\
&= \frac{6}{59} \times \frac{5}{58} \times \frac{4}{57} \times \frac{3}{56} \times \frac{2}{55} \times \frac{1}{54} .
\end{align*}

The answer we got before was
\[ \frac{6 \times 5 \times 4 \times 3 \times 2 \times 1}{59 \times 58 \times 57 \times 56 \times 55 \times 54} . \]
It's easy to see that this is the same answer, and the structure of the answers shows how the old method got the answer "all at once", while this new method gets the answer "one stage at a time".
:::



## Law of total probability  {#total-prob}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/zBfFOYkfoss"></iframe>
:::
::::

::: {.example}
*My friend has three dice: a 4-sided dice, a 6-side dice, and a 10-side dice. He picks one of them at random, with each dice equally likely. What is the probability my friend rolls a 5?*

If my friend were to tell which dice he picked, then this question would be very easy! If we write $D_4$, $D_6$ and $D_{10}$ to be the events that he picks the 4-sided, 6-sided, or 10-sided dice, then we know immediately that
\[ \mathbb P(\text{roll 4} \mid D_4) = 0 \qquad \mathbb P(\text{roll 4} \mid D_6) = \tfrac16 \qquad \mathbb P(\text{roll 4} \mid D_{10}) = \tfrac{1}{10} .  \]
What we need is a way to combine the results for different "sub-cases" into an over-all answer.
:::

Luckily, there exists just such a tool for this job! It's called the "law of total probability" (also known as the "partition theorem"). The important point is to make sure that the different sub-cases cover all possibilities, but that only one of them happens at a time.

::: {.definition}
A set of events $A_1, A_2, \dots, A_n$ are said to be a **partition** of the sample space $\Omega$ if 

1. they are disjoint, in that $A_i \cap A_j = \varnothing$ for all $i \neq j$;
1. they cover space, in that $A_1 \cup A_2 \cup \cdots \cup A_n = \Omega$.
:::

::: {.theorem #thlawtotal name="Law of total probability"}
Let $A_1, A_2, \dots, A_n$ be a partition, and $B$ another event. Then
\[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
:::

So the law of total probability tells us we can add up the probabilities $\mathbb P(B \mid A_i)$ for each of the sub-cases provided we weight them by how likely $\mathbb P(A_i)$ by how likely each sub-case is.

::: {.proof}
Since the partition of $A_i$s cover space, we can split up $B$ depending on which part of the partition it is in:
\[  B = (B \cap A_1) \cup (B \cap A_2) \cup \cdots \cup (B \cap A_n) .  \]

*[I meant to draw a picture here, but didn't get round to it -- perhaps you'd like to draw your own?]*

Since the $A_i$ are disjoint, the union on the right is disjoint also.
Therefore we can use Axiom 3 to get
\[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(B \cap A_i) . \]
But using the definition of conditional probability, each "summand" (term inside the sum) is
\[ \mathbb P(B \cap A_i) = \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
The result follows.
:::

Returning to our dice example, $D_4, D_6, D_{10}$ is indeed a partition, since these are the only possibilities and we only choose one dice. So the law of total probability tells us that
\[ \mathbb P(\text{roll 5}) = \mathbb P(D_4) \, \mathbb P(\text{roll 5} \mid D_4) +  \mathbb P(D_6) \, \mathbb P(\text{roll 5} \mid D_6) + \mathbb P(D_{10}) \, \mathbb P(\text{roll 5} \mid D_{10}) . \]

We were told that all the dice were picked with equal probability, so $\mathbb P(D_4) = \mathbb P(D_6) = \mathbb P(D_{10}) = \frac13$, and we calculated the individual conditional probabilities as
\[ \mathbb P(\text{roll 4} \mid D_4) = 0 \qquad \mathbb P(\text{roll 4} \mid D_6) = \tfrac16 \qquad \mathbb P(\text{roll 4} \mid D_{10}) = \tfrac{1}{10} .  \]

Therefore, we have
\[ \mathbb P(\text{roll 5}) = \tfrac13\times 0 +  \tfrac13\times\tfrac16 +  \tfrac13\times\tfrac1{10} = \tfrac{8}{90} = 0.089. \]


## Bayes' theorem  {#bayes}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/uLSewGUPH1g"></iframe>
:::
::::


In this subsection, we will discuss an important result called **Bayes' theorem**. 
Let's first state and prove this result, and do an example, and then afterwards we'll talk about two reasons why Bayes' theorem is so important.

::: {.theorem #thbayes name="Bayes' theorem"}
For events $A$ and $B$ with $\mathbb P(A), \mathbb P(B) > 0$, we have
\[ \mathbb P(A \mid B) = \frac{\mathbb P(A) \,\mathbb P(B \mid A)}{\mathbb P(B)} .  \]
:::

Bayes' theorem is thought to have first appeared in the writings of Rev. [Thomas Bayes](https://mathshistory.st-andrews.ac.uk/Biographies/Bayes/), a British church minister and mathematician, shortly after his death, in the 1760s. However, his work was significantly edited by [Richard Price](https://mathshistory.st-andrews.ac.uk/Biographies/Price/), another minister--mathematician, and many people think that Price deserves a large share of the credit.

::: {.proof}
From the definition of conditional probability, we can write $\mathbb P(A \cap B)$ in two different ways: we can write it as
\[  \mathbb P(A \cap B) = \mathbb P(A) \, \mathbb P(B\mid A) , \]
but we can also write it as
\[  \mathbb P(A \cap B) = \mathbb P(B) \, \mathbb P(A\mid B) . \]
Since these are two different ways of writing the same thing, we can equate them, to get
\[ \mathbb P(A) \, \mathbb P(B\mid A) = \mathbb P(B) \, \mathbb P(A\mid B) . \]
Dividing both sides by $\mathbb P(B)$ gives the result.
:::

::: {.example}
*My friend again secretly picks the 4-sided, 6-sided, or 10-sided dice, each with probability $\frac13$. He rolls that secret dice, and tells me he rolled a 5. What is the probability he picked the 6-sided dice?*

This is asking us to calculate $\mathbb P(D_6 \mid \text{roll 5})$. Bayes' theorem tells us that
\[
  \mathbb P(D_6 \mid \text{roll 5})
  = \frac{\mathbb P(D_6) \, \mathbb P(\text{roll 5} \mid D_6)}{\mathbb P(\text{roll 5})} 
  = \frac{\frac13 \times \frac16}{\frac{8}{90}} 
  = \tfrac{5}{8} ,
\]
since we had calculated $\mathbb P(\text{roll 5}) = \frac{8}{90}$ in the previous subsection.
:::

The first way to think about Bayes' theorem is that it tells us how to relate $\mathbb P(A \mid B)$ and $\mathbb P(B \mid A)$. Remember that $\mathbb P(A \mid B)$ and $\mathbb P(B \mid A)$ are not the same thing! The conditional probability someone is under 40 given they are a Premiership footballer is very high, but the conditional probability someone is a Premiership footballer given they are under 40 is very low.

Bayes' theorem, in this first view, is a useful technical result that helps us switch the order of a conditional probability from $B$ given $A$ to $A$ given $B$: we have
\[ \mathbb P(A \mid B) = \frac{\mathbb P(A)}{\mathbb P(B)} \times \mathbb P(B \mid A) .  \]

In the dice example, the probability $\mathbb P(\text{roll 5} \mid D_6) = \frac16$ was very obvious, but Bayes' theorem allowed us to reverse the conditioning, to find $\mathbb P(D_6 \mid \text{roll 5}) = \frac58$ instead.

The second way to think about Bayes' rule is that it tells us how to update our beliefs as we acquire more evidence. That is, we might start by believing that the probability some event $A$ will occur is $\mathbb P(A)$. But then we find out that $B$ has occurred, so we want to incorporate that knowledge and update our belief of the probability $A$ will occur to $\mathbb P(A \mid B)$, the conditional probability $A$ will occur given this new evidence $B$.

Bayes theorem, in this second view, tells us how to update from $\mathbb P(A)$ to $\mathbb P(A \mid B)$: we have
\[ \mathbb P(A \mid B) = \mathbb P(A) \times \frac{\mathbb P(B \mid A)}{\mathbb P(B)} .  \]

In the dice example, we initially believed there was a $\mathbb P(D_6) = \frac13 = 0.333$ chance our friend had chosen the six-sided dice. But when we heard that our friend had rolled a 5, we updated our belief to now thinking there was now a $\mathbb P(D_6 \mid \text{roll 5}) =\frac58 = 0.625$ chance it was the 6-sided dice.

This second way of thinking about Bayes' theorem is at the heart of **Bayesian statistics**. In Bayesian statistics, we start with a "prior" belief about a model, then, after collecting some data, we update to a "posterior" belief, according to the rules of Bayes' theorem. We will discuss Bayesian statistics much more in Section 10.




Quite often we use Bayes' theorem and the law of total probability together. If we have a partition $A_1, A_2, \dots, A_n$, perhaps representing some possible hypotheses, and we observe an event $B$, then Bayes' theorem tells us how likely each hypothesis is given the observation:
\[ \mathbb P(A_i \mid B) = \frac{\mathbb P(A_i) \,\mathbb P(B \mid A_i)}{\mathbb P(B)} .  \]
But this shared denominator $\mathbb P(B)$ can be expanded using the law of total probability
\[ \mathbb P(B) = \sum_{j=1}^n \mathbb P(A_j) \,\mathbb P(B \mid A_j) . \]
Together, we get the following.

::: {.theorem #bayes-total}
Let $\{A_1, A_2, \dots, A_n\}$ be a partition of a sample space and let $B$ be another event. Then, for all $i=1,2,\dots,n$, we have
\[ \mathbb P(A_i \mid B) = \frac{\mathbb P(A_i) \,\mathbb P(B \mid A_i)}{\sum_{j=1}^n \mathbb P(A_j) \, \mathbb P(B \mid A_j)} .  \]
:::

This is essentially what we did with the dice example, although we split it up into two separate parts rather than using this formula directly.


## Diagnostic testing  {#screening}

*Members of the public are tested for a certain disease. About 2% of the population have the disease. The test is 95% accurate, in the following sense: if you have the disease, there's a 95% chance you correctly get a positive test result, while if you don't have the disease, there's a 95% chance you correctly get a negative test result. Suppose you get a positive test result. What is the probability you have the disease?*

The first thing we have to do is translate the words in the question into probability statements. Let $D$ be the event you have the disease, so $D^\comp$ is the event you don't have the disease, and let $+$ be the event you get a positive result. Then the question tells us that

* $\mathbb P(D) = 0.02$ and $\mathbb P(D^\comp) = 0.98$;
* $\mathbb P({+} \mid D) = 0.95$ and $\mathbb P({+}\mid D^\comp) = 0.05$;
* we want to find $\mathbb P(D \mid {+})$.

Note also that $D$ (you have the disease) and $D^\comp$ (you don't) make up a partition. Then Theorem \@ref(thm:bayes-total) tells us that
\[  \mathbb P(D \mid {+}) = \frac{\mathbb P(D) \,\mathbb P({+} \mid D)}{\mathbb P(D) \,\mathbb P({+} \mid D)+\mathbb P(D^\comp) \,\mathbb P({+} \mid D^\comp)} . \]
Putting in all the numbers we have, we get
\[ \mathbb P(D \mid {+}) = \frac{0.02 \times 0.95}{0.02 \times 0.95 + 0.98 \times 0.05} = 0.28 .\]

So if you get a positive result on this 95%-accurate test, there's still only about a 1 in 4 chance you actually have the disease.

Many people find this result surprising. It sometimes helps to put more concrete numbers on things. Suppose 1000 people get tested. On average, we expect about 20 of them to have the disease, and 980 of to not have the disease. Of the 20 with the disease, on average 19 will correctly test positive, while 1 will test negative. Of the 980 without the disease, an average 931 will correctly test negative, but 49 will wrongly test positive. So of the $19+49 = 68$ people with positive tests, only 19 of them actually have the disease, which is 28%.

The key point is that the disease is rare -- only 2% of people have it. So even though positive test increases the likelihood you have the disease a lot (it's about 14 times more likely), it's not enough to make it a very large probability.

## Summary  {#summary-034 .unnumbered}

::: {.mysummary}
* Two events are independent if $\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb P(B)$.
* The conditional probability of $B$ given $A$ is ${\displaystyle \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)}}$.
* The law of total probability says that if $A_1, A_2, \dots A_n$ is a partition of the sample space, then
\[ \mathbb P(B) = \sum_{i=1}^n \mathbb P(A_i) \, \mathbb P(B \mid A_i) . \]
* Bayes' theorem says that ${\displaystyle \mathbb P(A \mid B) = \frac{\mathbb P(A) \,\mathbb P(B \mid A)}{\mathbb P(B)} }$.
:::
