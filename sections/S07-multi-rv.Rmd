# Multiple random variables  {#S07-multi-rv}

## Joint distributions  {#joint}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/5trzf2sZnLw"></iframe>
:::
::::


In previous sections, we have looked at single discrete random variables in isolation. In the section, we want to look at how multiple discrete random variables can interact.

Consider tossing a fair coin 3 times. Let $X$ be the number of Heads in the first two tosses, and let $Y$ be the number of Heads over all three tosses.

We know that $X \sim \text{Bin}(2, \frac12)$ and $Y \sim \text{Bin}(3, \frac12)$, so we can easily write down their probability mass functions:

| $x$ | $x = 0$ | $x = 1$ | $x = 2$ |
|:-:|:-:|:-:|:-:|
| $p_X(x)$ | $\frac14$ | $\frac12$ | $\frac14$ | 

| $y$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ |
|:-:|:-:|:-:|:-:|:-:|
| $p_Y(y)$ | $\frac18$ | $\frac38$ | $\frac38$ | $\frac18$ |

When we have multiple random variables and we want to emphasise that a PMF refers to only one of them, we often use the phrase **marginal PMF** or **marginal distribution**. So the PMFs above are the marginal distributions of $X$ and $Y$.

However, we might also want to know how $X$ and $Y$ interact. To do this, we will need the **joint PMF**, given by
\[ p_{X,Y}(x,y) = \mathbb P(X = x \text{ and } Y = y) . \]

In our case of the coin tosses, we have

| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $\phantom{p_X(x)}$ |
|:-:|:-:|:-:|:-:|:-:|:-:|
| $x=0$ | $\frac18$ | $\frac18$ | $0$ | $0$ | |
| $x=1$ | $0$ | $\frac14$ | $\frac14$ | $0$ | |
| $x=2$ | $0$ | $0$ | $\frac18$ | $\frac18$ | |
| $\vphantom{p_Y(y)}$ | | | | |

For probabilities of events, we had that if some $A_i$s form a partition, then
\[ \mathbb P(B) = \sum_i \mathbb P(B \cap A_i) . \]
Note that the events $\{X = x\}$, as $x$ varies over the range of $X$, also make up a partition. Therefore, we have
\[ \mathbb P(Y = y) = \sum_x \mathbb P(X = x \text{ and } Y = y) ; \]
or, to phrase this in terms of joint and marginal PMFs,
\[ p_Y(y) = \sum_x p_{X,Y}(x, y) . \]
In other words, to get the marginal distribution of $Y$, we need to sum down the columns in the table of the joint distribution.

| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $\phantom{p_X(x)}$ |
|:-:|:-:|:-:|:-:|:-:|:-:|
| $x=0$ | $\frac18$ | $\frac18$ | $0$ | $0$ | |
| $x=1$ | $0$ | $\frac14$ | $\frac14$ | $0$ | |
| $x=2$ | $0$ | $0$ | $\frac18$ | $\frac18$ | |
| $p_Y(y)$ | $\frac18$ | $\frac38$ | $\frac38$ | $\frac18$ | |

In exactly the same way, we have
\[ p_X(x) = \sum_y p_{X,Y}(x, y) ; \]
 so to get the marginal distribution of $X$, we need to sum across the rows in the table of the joint distribution.
 
| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $p_X(x)$ |
|:-:|:-:|:-:|:-:|:-:|:-:|
| $x=0$ | $\frac18$ | $\frac18$ | $0$ | $0$ | $\frac14$ |
| $x=1$ | $0$ | $\frac14$ | $\frac14$ | $0$ | $\frac12$ |
| $x=2$ | $0$ | $0$ | $\frac18$ | $\frac18$ | $\frac14$ |
| $p_Y(y)$ | $\frac18$ | $\frac38$ | $\frac38$ | $\frac18$ | |

We can check that these marginal PMFs match those we started with. The term "marginal" PMF or distribution is presumably because one ends up writing the values in the "margins" of the table.

Note that the joint PMF conforms to the same rules as a normal PMF:

* it is non-negative: $p_{X,Y}(x,y) \geq 0$;
* it sums to 1: $\displaystyle\sum_{x,y} p_{X,Y}(x,y) = 1$. 

We may want to look at more than two random variables, $\mathbf X = (X_1, X_2, \dots, X_n)$. In this case, the joint PMF is
\[ p_{\mathbf X}(\mathbf x) = p_{X_1, \dots, X_n}(x_1, \dots, x_n) = \mathbb P(X_1 = x_1 \text{ and } \cdots \text{ and } X_n = x_n) .   \]
In the same way, we can find the marginal distribution of one of the variables -- say $X_1$ -- by summing over all the other variables:
\[ p_{X_1}(x_1) = \sum_{x_2, \dots, x_n} p_{X_1, X_2, \dots, X_n}(x_1, x_2, \dots, x_n) . \]


## Independence of random variables {#independence-rv}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/ZxoVHpnI298"></iframe>
:::
::::


We said that two events are independent if $\mathbb P(A \cap B) = \mathbb P(A)\, \mathbb  P(B)$. We now can give a similar definition for what it means two random variables to be independent.

::: {.definition}
We say two discrete random variables are **independent** if, for all $x$ and $y$,
\[ \mathbb P(X = x \text{ and } Y = y) = \mathbb P(X = x) \, \mathbb P(Y = y) . \]
In terms of the joint and marginal PMFs, this is the condition that
\[ p_{X,Y}(x,y) = p_X(x) \, p_Y(y) . \]

More generally, a sequence of random variables $\mathbf X = (X_1, X_2, \dots)$ are independent if
\[ p_{\mathbf X}(\mathbf x) = p_{X_1}(x_1) \times p_{X_2}(x_2) \times \cdots \times p_{X_n}(x_n). \]
:::

Returning to our case of the dice from before, we see that $X$ and $Y$ are not independent, because, for just one counterexample, $p_{X,Y}(0,0) = \frac18$, while $p_X(0) = \frac14$ and $p_Y(0) = \frac18$, so $p_{X,Y}(0,0) \neq p_X(0) \, p_Y(0)$.

An important scenario in probability theory and statistics is that of **independent and identically distributed** (or **IID**) random variables. IID random variables represent an experiment that is repeated many times, with each experiment independent of the others. So all the random variables have the same distribution, and they are all independent of each other. So $\mathbf X = (X_1, X_2, \dots )$ are IID random variables with a common PMF $p_X$, say, if
\[ p_{\mathbf X}(\mathbf x) = p_X(X_1) \times p_X(x_2) \times \cdots . \]

::: {.example}
*Let $X_1, X_2, \dots, X_{20}$ be IID random variables following a Poisson distribution with rate $\lambda = 3$. What is the probability that all 20 of the the $X_i$ are nonzero?*

Because the $X_i$ are identically distribution, the probability that any one of them is nonzero is
\[ \mathbb P(X_1 > 0) = 1 - \mathbb P(X_1 = 0) = 1 - \mathrm{e}^{-3} = 0.950 . \]
Then, because the $X_i$ independent, the probability that they are all nonzero is
\[ \mathbb P(X_1 > 0)^{20} = 0.950^{20} = 0.360. \]
:::


## Conditional distributions  {#cond-rv}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/U0kNB45K81w"></iframe>
:::
::::


For probability with events, we had the conditional probability
\[ \mathbb P(B \mid A) = \frac{\mathbb P(A \cap B)}{\mathbb P(A)} . \]
In the same way, for random variables, we have
\[ \mathbb P(Y = y \mid X = x) = \frac{\mathbb P(X = x \text{ and } Y = y)}{\mathbb P(X = x)} . \]
It makes sense to call the distribution of this the **conditional distribution**.

::: {.definition}
Let $X$ and $Y$ be two random variables with joint PMF $p_{X,Y}$ and marginal PMFs $p_X$ and $p_Y$ respectively. Then the **condition probability mass function of $Y$ given $X$** $p_{Y \mid X}$ is given by
\[ p_{Y \mid X}(y \mid x) = \frac{p_{X,Y}(x,y)}{p_X(x)} .   \]
:::

Let's think again about our coin tossing example. To get the conditional distribution of $Y$ given $X = 1$, say, we have
\[ p_{Y \mid X}(y \mid 1) = \frac{p_{X,Y}(1,y)}{p_X(1)} ;   \]
so we take the $x = 1$ row of the joint distribution table and "renormalise it" by dividing through by the total of the row, so it adds up to 1. That is,
\begin{align*}
  p_{Y \mid X} (0 \mid 1) &= \frac{p_{X,Y}(1, 0)}{p_X(1)} = \frac{0}{\frac12} = 0 , \\
  p_{Y \mid X} (1 \mid 1) &= \frac{p_{X,Y}(1, 1)}{p_X(1)} = \frac{\frac14}{\frac12} = \tfrac12 , \\
  p_{Y \mid X} (2 \mid 1) &= \frac{p_{X,Y}(1, 2)}{p_X(1)} = \frac{\frac14}{\frac12} = \tfrac12 , \\
  p_{Y \mid X} (3 \mid 1) &= \frac{p_{X,Y}(1, 3)}{p_X(1)} = \frac{0}{\frac12} = 0 .
\end{align*}
In just the same way, we could get the conditional distribution of $X$ given $Y = 2$, say, by taking the $y = 2$ column of the joint distribution table, and renormalising so that the column sums to 1, That is,
\begin{align*}
  p_{X \mid Y} (0 \mid 2) &= \frac{p_{X,Y}(0,2)}{p_Y(2)} = \frac{0}{\frac38} = 0 , \\
  p_{X \mid Y} (1 \mid 2) &= \frac{p_{X,Y}(1,2)}{p_Y(2)} = \frac{\frac14}{\frac38} = \tfrac23 , \\
  p_{X \mid Y} (2 \mid 2) &= \frac{p_{X,Y}(2,2)}{p_Y(2)} = \frac{\frac18}{\frac38} = \tfrac13 .
\end{align*}

Results that we used for conditional probability with events also carry over to random variables. For example, from **Bayes' theorem** we know that
\[ \mathbb P(A \mid B) = \frac{ \mathbb P(A) \, \mathbb P(B \mid A)}{\mathbb P(B)} . \]
In the same way, we have
\[ \mathbb P(X = x \mid Y = y) = \frac{ \mathbb P(X = x) \, \mathbb P(Y = y \mid X = x)}{\mathbb P(Y = y)} , \]
which in terms of conditional and marginal PMFs is
\[ p_{X \mid Y}(x \mid y) = \frac{p_X(x) \, p_{Y \mid X}(y \mid x)}{p_Y(y)} . \]
This will be a particularly important formula when we study Bayesian statistics at the end of the module.

We can check Bayes' theorem with $x = 1$ and $y = 2$, for example. 
The right-hand side of Bayes' theorem is
\[ \frac{p_X(1) \, p_{Y \mid X}(2 \mid 1)}{p_Y(2)} = \frac{\frac12 \times \frac12}{\frac38} = \tfrac{23} .   \]
The left-hand side of Bayes' theorem is
\[ p_{X \mid Y}(1 \mid 2) = \tfrac23 , \]
which is equal, as it should be, to the right-hand side.


## Expectation of sums and products {#sum-product}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/BaEbVJZkGGY"></iframe>
:::
::::

When we have multiple random variables, we might be interested in functions of those multiple random variables -- for example their sum or their product. (In fact, it's often possible to find out about the whole distribution of a sum, product, or function of the variables -- see MATH2715 Statistical Methods for more on this -- but will just look at their expectations and, later, variances.)

::: {.theorem #linearity2}
Let $X$ and $Y$ be two random variables with joint probability mass function $p_{X,Y}$. Then

1. $\mathbb Eg(X,Y) = \displaystyle\sum_{x,y} g(x,y) p_{X,Y}(x,y)$.
2. **(Linearity of expectation, 2)** $\mathbb E(X + Y) = \mathbb EX + \mathbb EY$, regardless of whether $X$ and $Y$ are independent or not.
3. If $X$ and $Y$ are independent, then $\mathbb EXY = \mathbb EX \times \mathbb EY$.
:::

If we put the second point here together with the other result of linearity of expectation (Theorem \@ref(thm:linearity1)) then we get the general rule
\[ \mathbb E(aX + bY + c) = a\,\mathbb EX + b \,\mathbb EY + c , \]
and this holds whether or not $X$ and $Y$ are independent.

::: {.proof}
Part 1 is just the law of the unconscious statistician for the random variable $(X,Y)$, and the same proof holds.

For part 2, we have
\begin{align*}
\mathbb E(X + Y) &= \sum_{x,y} (x + y)p_{X,Y}(x,y) \\
  &= \sum_{x,y} x\,p_{X,Y}(x,y) + \sum_{x,y} y\,p_{X,Y}(x,y) \\
  &= \sum_x x \sum_y p_{X,Y}(x,y) + \sum_y y \sum_x p_{X,Y}(x,y)
\end{align*}
But summing a joint PMF over one of the variables gives the marginal PMF; so $\sum_y p_{X,Y}(x,y) = p_X(x)$ and $\sum_x p_{X,Y}(x,y) = p_Y(y)$. So this gives
\begin{align*}
\mathbb E(X + Y) &= \sum_x x\, p_X(x) + \sum_y y\,p_Y(y) \\
&= \mathbb EX + \mathbb EY .
\end{align*}

For part 3, if $X$ and $Y$ are independent, then $p_{X,Y}(x,y) = p_X(x) \, p_Y(y)$. Therefore,
\begin{align*}
\mathbb EXY &= \sum_{x,y} xy p_{X,Y}(x,y) \\
  &= \sum_x \sum_y xy p_X(x) p_Y(y) \\
  &= \sum_x x p_X(x) \sum_y y p_Y(y) \\
  &= \mathbb EX \times \mathbb EY,
\end{align*}
as required.
:::

::: {.example}
*A student is solving five questions on a problem sheet. The time taken for each question to the nearest minute is an identically distributed random variable with expectation $\mathbb EX_i = \mu$. What is the total expected time to complete the problem sheet?*

By linearity of expectation, this is
\[ \mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = \mathbb EX_1 + \mathbb EX_2 + \mathbb EX_3 + \mathbb EX_4 + \mathbb EX_5 = 5\mu . \]

*What if the lengths of time are not independent -- for example, if the student is slower at answering all the questions when she is tired?*

It's still the case that $\mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = 5\mu$, because this result is true whether or not the random variables are independent.
:::



## Covariance {#covariance}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/eXsUrJzcRCE"></iframe>
:::
::::

If we are interested at how two random variables vary together, we need to look at the covariance.

\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}

::: {.definition}
Let $X$ and $Y$ be two random variables with expectations $\mu_X$ and $\mu_Y$ respectively. Then their **covariance** is
\[ \Cov(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) . \]
:::

In the least surprising result of this whole module, we also have a computational formula to go along with this definitional formula.

::: {.theorem}
Let $X$ and $Y$ be two random variables with expectations $\mu_X$ and $\mu_Y$ respectively. Then their covariance can also be calculated as
\[ \Cov(X,Y) = \mathbb EXY - \mu_X\, \mu_Y . \]
:::

::: {.proof}
Exactly as we've done many times before, we have
\begin{align*}
\Cov(X,Y) &= \mathbb E(X - \mu_X)(Y - \mu_Y) \\
&= \mathbb E(XY - X\,\mu_Y - \mu_X\, Y + \mu_X\,\mu_Y) \\
&= \mathbb EXY  - \mu_Y \,\mathbb EX - \mu_X \,\mathbb EY + \mu_X \, \mu_Y \\
&= \mathbb EXY - \mu_X \, \mu_Y - \mu_X \, \mu_Y + \mu_X \, \mu_Y \\
&= \mathbb EXY - \mu_X \, \mu_Y ,
\end{align*}
and we're done.
:::

::: {.example}
We continue with our coin-tossing example. We know that $X \sim \text{Bin}(2, \frac12)$ so $\mu_X = 1$ and  $Y \sim \text{Bin}(3, \frac12)$ so $\mu_Y = 1.5$. We then need $\mathbb EXY$, which is
\begin{align*}
\mathbb EXY &= \sum_{x,y} xy\, p_{X,Y}(x,y) \\
  &= 0\times 0\,p_{X,Y}(0,0) + 0 \times 1 \, p_{X,Y}(0,1) + \cdots + 2\times 3 \,p_{X,Y}(2,3) \\
  &= 0 \times \tfrac18 + 0 \times \tfrac18 + \cdots + 6 \times \tfrac18 \\
  &= 2.
\end{align*}
Hence the covariance is
\[ \Cov (X,Y) = \mathbb EXY - \mu_X\mu_Y = 2 - 1 \times 1.5 = 0.5 .\]
:::


A very important fact is the following.

::: {.theorem}
If $X$ and $Y$ are independent, then $\Cov(X,Y) = 0$.
:::

To use the ["contrapositive"](https://www.varsitytutors.com/hotmath/hotmath_help/topics/converse-inverse-contrapositive), in our example, $\Cov(X,Y) \neq 0$, which means that $X$ and $Y$ are not independent (confirming what we already knew).

::: {.proof}
Recall from Theorem \@ref(thm:linearity2) that if $X$ and $Y$ are independent, we have $\mathbb EXY = \mathbb EX \times \mathbb EY = \mu_X \, \mu_Y$. Then from the computational formula, we have
\[ \Cov(X,Y) = \mathbb EXY - \mu_X\,\mu_Y = \mu_X\,\mu_Y - \mu_X\,\mu_Y = 0, \]
and we are done.
:::

Here are some more important properties of the covariance.

::: {.theorem}
Let $X$, $Y$ and $Z$ be random variables. Then

1. $\Cov(X,Y) = \Cov(Y,X)$;
2. $\Cov(X,X) = \Var(X)$'
3. $\Cov(aX, Y) = a\,\Cov(X,Y)$;
4. $\Cov(X + b, Y) = \Cov(X,Y)$;
5. $\Cov(X + Y, Z) = \Cov(X, Z) + \Cov(Y,Z)$.
:::

::: {.proof}
Part 1 and 2 are immediate from the definition.

Parts 3, 4 and 5 are quite similar. We'll do part 5 here, and you can do parts 3 and 4 on [Problem Sheet 4](#P4).

For part 5, note that $\mathbb E(X + Y) = \mu_X + \mu_Y$ by linearity of expectation. Hence
\begin{align*}
\Cov(X + Y, Z)
  &= \mathbb E (X + Y - \mu_X - \mu_Y)(Z - \mu_Z) \\
  &= \mathbb E \big((X - \mu_X) + (Y - \mu_Y)\big)(Z - \mu_Z) \\
  &= \mathbb E \big((X - \mu_X)(Z - \mu_Z) + (Y - \mu_Y) (Z - \mu_Z) \big) \\
  &= \mathbb E (X - \mu_X)(Z - \mu_Z) + \mathbb E  (Y - \mu_Y) (Z - \mu_Z) \\
  &= \Cov(X,Z) + \Cov(Y,Z) ,
\end{align*}
as required.
:::

We could calculate the covariance in our coin-tossing example a different way, by noting that $Y = X + Z$, where $Z \sim \text{Bern}(\frac12)$ represents the third coin toss and is independent of $X$. Then we have
\begin{multline*}
\Cov(X,Y) = \Cov(X, X + Z) = \Cov(X, X) + \Cov(X, Z) \\
= \Var(X) + 0 = 2 \times \tfrac12 \times \big(1 - \tfrac12\big) = \tfrac12, \end{multline*}
matching our previous calculation. In the above calculation, we used $\Cov(X, Z) = 0$ since $X$ and $Z$ are independent, and we knew the variance of $X$ because $X \sim \text{Bin}(2, \frac12)$.

Now that we know some facts about the covariance, we can calculate the variance of a sum.

::: {.theorem}
Let $X$ and $Y$ be two random variables. Then
\[ \Var(X + Y) = \Var(X) + 2\Cov(X,Y) + \Var(Y) . \]

If $X$ and $Y$ are independent, then
\[ \Var(X + Y) = \Var(X) + \Var(Y) . \]
:::

It's easy to forget the conditions for the following two facts:

* $\mathbb E(X + Y) = \mathbb EX + \mathbb EY$ regardless of whether $X$ and $Y$ are independent or not.
* $\Var(X+Y) = \Var(X) + \Var(Y)$ if $X$ and $Y$ are independent.

::: {.proof}
For the main part of the proof, we start with the definition of variance. By linearity of expectation, we have $\mathbb E(X + Y) = \mu_X + \mu_Y$. So
\begin{align*}
\Var(X + Y) &= \mathbb E\big((X + Y) - (\mu_X + \mu_Y)\big)^2 \\
  &= \mathbb E \big((X - \mu_X) + (Y - \mu_Y) \big)^2 \\
  &= \mathbb E \big( (X - \mu_X)^2 + 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2\big) \\
  &= \mathbb E(X - \mu_X)^2 + 2 \mathbb E(X - \mu_X)(Y - \mu_Y) + \mathbb E (Y - \mu_Y)^2 \\
  &= \Var(X) + 2\Cov(X,Y) + \Var(Y) ,
\end{align*}
where we used the linearity of expectation.

For the second part, recall that is $X$ and $Y$ are independent, then $\Cov(X,Y) = 0$.
:::

It can sometimes be useful to "normalise" the covariance, by dividing through by the individual standard deviations. This gives a measurement of the linear relationship between two random variables.

::: {.definition}
Let $X$ and $Y$ be two random variables. Then the **correlation** between $X$ and $Y$ is
\[ \Corr(X,Y) = \frac{\Cov(X,Y)} {\sqrt{\Var(X)\,\Var(Y)}} . \]
:::

As with the sample correlation $r_{xy}$ from Section 1, the correlation is a number between $-1$ and $+1$, where values near $+1$ mean that large values of $X$ and large values of $Y$ are likely to occur together, while values near $-1$ mean that large values of $X$ and small values of $Y$ are likely to occur together.

Recall that, if $X$ and $Y$ are independent, then $\Cov(X,Y) = 0$. Hence it follows that if $X$ and $Y$ are independent, then $\Corr(X,Y) = 0$ also.

::: {.example}
For the coin-tossing again, we have
\[ \Corr(X,Y) = \frac{\Cov(X,Y)} {\sqrt{\Var(X)\,\Var(Y)}} = \frac{\frac12}{\sqrt{\frac12 \times \frac34}} = \sqrt{\tfrac23} = 0.816 .    \]
:::


## Law of large numbers  {#lln}

:::: {.videowrap}
::: {.videowrapper}
<iframe src="https://www.youtube.com/embed/u4hJv2sDUoc"></iframe>
:::
::::

Let $X_1, X_2, \dots, X_n$ be a sequence of IID random variables. Let us write $\mu = \mathbb EX_1$ for the common expectation and $\sigma^2 = \Var(X_1)$ for the common variance.

At the beginning of the course, we saw the mean of some values was
\[ \bar x = \frac{1}{n} (x_1 + x_2 + \cdots + x_n) = \frac{1}{n} \sum_{i=1}^n x_i ; \]
that is, what we get if we add them up and divide by $n$. In the same way, we could calculate the "mean" of some random variables by adding them up and dividing by $n$; that is:
\[ \overline X_n = \frac{1}{n} (X_1 + X_2 + \cdots + X_n) = \frac{1}{n} \sum_{i=1}^n X_i . \]
(The subscript $n$ on "$\overline X_n$" is just to remind us this is a mean of $n$ random variables.)

Here, each of the $X_i$s is a random variable, so their mean $\overline X_n$ is another random variable as well. So we can ask questions about the random variable $\overline X_n$ just the same as we would ask about any other random variable. For example: What is its expectation and variance?

The expectation of $\bar X_n$ is
\begin{align*}
\mathbb E \overline X_n &= \mathbb E \left( \frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
&=   \frac{1}{n} (\mathbb EX_1 + \mathbb EX_2 + \cdots + \mathbb EX_n)\\
&= \frac{1}{n} (\mu + \mu + \cdots + \mu)\\
&= \frac{1}{n} n \mu \\
&= \mu .
\end{align*}
Here we use linearity of expectation to take the $1/n$ out of the brackets, and then add up the individual expectations.

Since the $X_i$s are independent, the variance of $\overline X_n$ is
\begin{align*}
\Var( \overline X_n) &= \Var \left( \frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
&= \left(\frac{1}{n}\right)^2 \Var(X_1 + X_2 + \cdots + X_n) \\
&=   \frac{1}{n^2} \big(\Var(X_1) + \Var(X_2) + \cdots + \Var (X_n)\big)\\
&= \frac{1}{n^2} (\sigma^2 + \sigma^2+ \cdots + \sigma^2)\\
&= \frac{1}{n^2} n \sigma^2 \\
&= \frac{\sigma^2}{n} .
\end{align*}

In conclusion we have this:

::: {.theorem}
Let $X_1, X_2, \dots, X_n$ be a sequence of IID random variables. Write $\mu = \mathbb EX_1$ for the common expectation and $\sigma^2 = \Var(X_1)$ for the common variance, and
$\overline X_n =\frac{1}{n} \sum_{i=1}^n X_i$ for the mean. Then
\[ \mathbb E \overline X_n = \mu \qquad \Var(\overline X_n) = \frac{\sigma^2}{n} . \]
:::

Now think about what happens to this mean $\overline X_n$ when $n$ gets very large. We see that the expectation $\mathbb E\overline X_n = \mu$ stays the same, but the variance $\Var(\overline X_n) = \sigma^2/n$ gets smaller and smaller as $n$ gets bigger. Thus the range of probably values for $\overline X_n$ well be squeezing tighter and tighter around $\mu$. Given that, it seems as if (and it can be proven that) we have the "law of large numbers".

::: {.theorem #thLLN name="Law of large numbers"}
Let $X_1, X_2, \dots$ be a sequence of IID random variables. Write $\mu = \mathbb EX_1$ for the common expectation and $\overline X_n =\frac{1}{n} \sum_{i=1}^n X_i$ for the mean of the first $n$ random variables. Then 
\[ \overline X_n \to \mu \quad \text{in probability as $n \to \infty$}; \]
by which we mean that, for any $\epsilon > 0$, 
\[ \mathbb P\big(|\overline X_n - \mu| > \epsilon\big) \to 0 \quad \text{as $n\to\infty$.} \]
:::

The precise mathematical definition of the convergence is not important here. What is important is the general principle that the expectation $\mathbb EX = \mu$ represents the "long-run average" of independent experiments.

One special case is if we have repeated experiments the succeed with probability $p$; that is, $X_n \sim \text{Bern}(p)$. Then the law of large numbers says that the long-run proportion of successes is
\[ \frac{1}{n} \sum_{i = 1}^n X_n = \overline X_n \to \mathbb EX_1 = p . \]
So the long-run proportion of times an event happens converges to its probability. This goes back to what we said about "frequentist probability" [right at the beginning of Section 2](#what-is-prob): that one way to understand the probability of an event is as the long-run frequency of its occurrence. 

::: {.mysummary}
* For two random variables, the joint PMF $p_{X,Y}$, marginal PMF $p_X$, and conditional PMF $p_{Y \mid X}$ are
\begin{align*}
  p_{X,Y}(x,y) &= \mathbb P(X =x \text{ and } Y = y) \\
  p_X(x) &= \mathbb P(X = x) = \sum_y p_{X,Y}(x,y) \\
  p_{Y \mid X}(y \mid x) &= \mathbb P(Y = y \mid X = x) = \frac{p_{X,Y}(x,y)}{p_X(x)} 
\end{align*}
* Two random variables are independent if $p_{X,Y}(x,y) = p_X(x) \, p_Y(y)$.
* $\mathbb E(X + Y) = \mathbb EX + \mathbb EY$
* The covariance is $\Cov(X,Y) = \mathbb E(X - \mu_X)(Y - \mu_Y) = \mathbb EXY - \mu_X \,\mu_Y$.
* $\Var(X + Y) = \Var(X) + 2\Cov(X,Y) + \Var(Y)$; or if $X$ and $Y$ are independent, then $\Var(X + Y) = \Var(X) + \Var(Y)$.
* The law of large numbers says that the mean $\overline X_n$ tends to the expectation $\mu$.
:::